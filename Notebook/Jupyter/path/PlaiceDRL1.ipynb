{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyのツノガレイ強化学習の本\n",
    "\n",
    "## 多腕バンディット問題、動的計画法\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- [多腕バンディット問題](#多腕バンディット問題)\n",
    "  - [期待値＝報酬の平均として価値を求める実装](#期待値＝報酬の平均として価値を求める実装)\n",
    "    - [naive implementation](#naive_implementation)\n",
    "    - [incremental implementation](#incremental_implementation)\n",
    "  - [クラス定義](#クラス定義)\n",
    "    - [スロット台](#プレーヤー)\n",
    "    - [プレーヤー](#プレーヤー)\n",
    "  - [プレーヤーがスロットしつつ強化学習](#プレーヤーがスロットしつつ強化学習)\n",
    "    - [1000ステップで強化学習](#1000ステップで強化学習)\n",
    "      - [1000ステップの強化学習を１回実行](#1000ステップの強化学習を１回実行)\n",
    "      - [1000ステップの強化学習を10回実行](#1000ステップの強化学習を10回実行)\n",
    "    - [1000ステップで強化学習を200回繰り返した平均で評価](#1000ステップで強化学習を200回繰り返した平均で評価)\n",
    "      - [探索の確率を10％にして実行](#探索の確立を10％にして実行)\n",
    "      - [探索の確率を1～30％にして実行](#探索の確率を1～30％にして実行)\n",
    "  - [非定常な多腕バンディット問題](#非定常な多腕バンディット問題)\n",
    "    - [コードの修正](#コードの修正)\n",
    "      - [スロット台](#スロット台)\n",
    "      - [プレイヤー](#プレイヤー)\n",
    "    - [コードの実行](#コードの実行)\n",
    "- [動的計画法（方策反復法、価値反復法）](#動的計画法（方策反復法、価値反復法）)\n",
    "  - [連立方程式ソルバをベルマン作用素的に解く](#連立方程式ソルバをベルマン作用素的に解く)\n",
    "    - [普通に実装](#普通に実装)\n",
    "    - [上書き方式の実装](#上書き方式の実装)\n",
    "  - [3*4マスのグリッド・ワールド](#3*4マスのグリッド・ワールド)\n",
    "    - [状態価値関数のランダム初期化＆表示](#状態価値関数のランダム初期化＆表示)\n",
    "    - [方策反復](#方策反復)\n",
    "      - [方策評価、状態価値関数の推定](#方策評価、状態価値関数の推定)\n",
    "      - [方策改善、状態価値関数の変化](#方策改善、状態価値関数の変化)\n",
    "    - [価値反復](#価値反復)\n",
    "\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-4/tree/master/ch01\n",
    "- [強化学習（Reinforcement Learning） - .NET 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88Reinforcement%20Learning%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39e2e7-67b7-464d-b254-68521ef5a3de",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea6589-87d6-459c-b8b8-55179ed99adf",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1a3d-fc30-4a7f-b180-a221b11e7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install dezerogym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4d655-73aa-4cef-9def-0de5a4ef83d1",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854310-e2f8-48c9-986c-e05d242744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a909-b830-4776-890f-b6cef29a456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836fb9c-95b7-4cce-9651-d948fcc6fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共通関数\n",
    "greedy法、貪欲法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d4aee-8de2-4762-b4a4-1575d8caeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(d):\n",
    "    \"\"\"d (dict)\"\"\"\n",
    "    max_value = max(d.values())\n",
    "    max_key = -1\n",
    "    for key, value in d.items():\n",
    "        if value == max_value:\n",
    "            max_key = key\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fdd44-9e4d-48a2-a5a7-cc78e056e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(V, env, gamma):\n",
    "    pi = {}\n",
    "\n",
    "    for state in env.states():\n",
    "        action_values = {}\n",
    "\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]\n",
    "            action_values[action] = value\n",
    "\n",
    "        max_action = argmax(action_values)\n",
    "        action_probs = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "        action_probs[max_action] = 1.0\n",
    "        pi[state] = action_probs\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5acd28-b5e2-4714-904b-a3b1d891fda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 多腕バンディット問題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ce253-4d2c-4706-870d-551f036c9a25",
   "metadata": {},
   "source": [
    "### 期待値＝報酬の平均として価値を求める実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4dd4fb-cf66-4029-8dd1-199862a9578e",
   "metadata": {},
   "source": [
    "#### naive_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72d557-68c7-4680-9ecf-1f9939639993",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rewards = []\n",
    "\n",
    "for n in range(1, 11):\n",
    "    reward = np.random.rand()\n",
    "    rewards.append(reward)\n",
    "    Q = sum(rewards) / n\n",
    "    print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b1b95-90e1-4c6b-9cd4-5ba69a8f0bca",
   "metadata": {},
   "source": [
    "#### incremental_implementation\n",
    "インクリメンタルな実装は計算量が少なくて済む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb143e4-a657-48f9-9d82-ef0b49cefded",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "Q = 0\n",
    "\n",
    "for n in range(1, 11):\n",
    "    reward = np.random.rand()\n",
    "    Q = Q + (reward - Q) / n\n",
    "    print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8f56c-8433-4a55-b11b-f9c20288310e",
   "metadata": {},
   "source": [
    "### クラス定義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46256c1a-f81b-4fb3-82dc-c616d172184c",
   "metadata": {},
   "source": [
    "#### スロット台"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5640c-f707-41fb-9588-b9d2183605f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def __init__(self, arms=10):\n",
    "        self.rates = np.random.rand(arms)\n",
    "\n",
    "    def play(self, arm):\n",
    "        rate = self.rates[arm]\n",
    "        if rate > np.random.rand():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829565fc-a7db-41ae-85a5-d0c4a4132c2b",
   "metadata": {},
   "source": [
    "#### プレーヤー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f081e9-5a49-4078-8e18-f35af61512d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, epsilon, action_size=10):\n",
    "        self.epsilon = epsilon           # ε貪欲法\n",
    "        self.Qs = np.zeros(action_size)  # 価値\n",
    "        self.ns = np.zeros(action_size)  # 台毎のカウンタ\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        self.ns[action] += 1 # 台毎のカウンタをインクリメント\n",
    "        self.Qs[action] = self.Qs[action] + (1 / self.ns[action]) * (reward - self.Qs[action])\n",
    "        \n",
    "    def get_action(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, len(self.Qs))\n",
    "        return np.argmax(self.Qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f371dc-4a70-4925-911a-208b79112412",
   "metadata": {},
   "source": [
    "### プレーヤーがスロットしつつ強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdb58e-f41b-46a9-8fe8-eb13c4b36783",
   "metadata": {},
   "source": [
    "#### 1000ステップで強化学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e11831-7f21-469a-b0cd-90f000732b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute1():\n",
    "    steps = 1000\n",
    "    epsilon = 0.1\n",
    "\n",
    "    # 台の初期化（10台）\n",
    "    bandit = Bandit()\n",
    "    agent = Agent(epsilon)\n",
    "    total_reward = 0\n",
    "    total_rewards = []\n",
    "    rates = []\n",
    "\n",
    "    for step in range(steps):\n",
    "        # 台の選択\n",
    "        action = agent.get_action()\n",
    "        # 台のプレイ\n",
    "        reward = bandit.play(action)\n",
    "        # 台の価値の更新\n",
    "        agent.update(action, reward)\n",
    "        # 報酬の総額＝収益\n",
    "        total_reward += reward\n",
    "\n",
    "        # グラフ描画の履歴\n",
    "        total_rewards.append(total_reward)\n",
    "        rates.append(total_reward / (step + 1))\n",
    "\n",
    "    # グラフ描画の履歴を戻す\n",
    "    return total_rewards, rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2681e9-a694-4d3a-904d-7bc97a13786b",
   "metadata": {},
   "source": [
    "##### 1000ステップの強化学習を１回実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c161bd8-736d-4e7f-a29f-0785800cf4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards, rates = execute1()\n",
    "\n",
    "plt.ylabel('Total reward')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(total_rewards)\n",
    "plt.show()\n",
    "\n",
    "plt.ylabel('Rates')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebaa166-bd27-47ae-985a-f4e492d4dd62",
   "metadata": {},
   "source": [
    "##### 1000ステップの強化学習を10回実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc9bde-3060-4420-a729-f079b6c0e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Rates')\n",
    "plt.xlabel('Steps')\n",
    "\n",
    "for cnt in range(10):\n",
    "    total_rewards, rates = execute1()\n",
    "    color = plt.cm.jet(cnt / 10)\n",
    "    plt.plot(rates, color=color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82290217-8934-47f2-8bf0-28b234fc936d",
   "metadata": {},
   "source": [
    "#### 1000ステップの強化学習を200回繰り返した平均で評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e6eda-b1d0-4e95-9c0f-c0768b9cf5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute2(epsilon):\n",
    "    runs = 200\n",
    "    steps = 1000\n",
    "    all_rates = np.zeros((runs, steps))  # (2000, 1000)\n",
    "\n",
    "    # 強化学習を200回繰り返した平均で評価\n",
    "    for run in range(runs):\n",
    "        bandit = Bandit()\n",
    "        agent = Agent(epsilon)\n",
    "        total_reward = 0\n",
    "        rates = []\n",
    "\n",
    "        # 1000ステップ\n",
    "        for step in range(steps):\n",
    "            action = agent.get_action()\n",
    "            reward = bandit.play(action)\n",
    "            agent.update(action, reward)\n",
    "            total_reward += reward\n",
    "            rates.append(total_reward / (step + 1))\n",
    "\n",
    "        all_rates[run] = rates\n",
    "\n",
    "    return np.average(all_rates, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b9580-50f6-429c-b5e7-d844d356db48",
   "metadata": {},
   "source": [
    "##### 探索の確立を10％にして実行\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e98ab-58b9-404b-bde2-9649743c307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "avg_rates = execute2(epsilon)\n",
    "plt.ylabel('Rates')\n",
    "plt.xlabel('Steps')\n",
    "plt.plot(avg_rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdca867-12a3-4e4c-ba3d-31b366046186",
   "metadata": {},
   "source": [
    "##### 探索の確率を1～30％にして実行\n",
    "epsilon = 0.01, 0.1, 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6782e45-f2fc-47e9-8381-0c2abdf7ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rates1 = execute2(0.01)\n",
    "avg_rates2 = execute2(0.1)\n",
    "avg_rates3 = execute2(0.3)\n",
    "\n",
    "plt.plot(avg_rates1, color='blue', label='0.01')\n",
    "plt.plot(avg_rates2, color='red', label='0.1')\n",
    "plt.plot(avg_rates3, color='green', label='0.3')\n",
    "\n",
    "plt.ylabel('Rates')\n",
    "plt.xlabel('Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a5a8b-7f93-4556-bf18-d01a97dbd95a",
   "metadata": {},
   "source": [
    "### 非定常な多腕バンディット問題\n",
    "非定常問題では、スロット台の勝率が時間とともに変化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1c1f9-ec6b-42d0-9fb6-8f5151bc2aaf",
   "metadata": {},
   "source": [
    "#### コードの修正"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d6f65-35e0-4c1e-9408-077dc61c459e",
   "metadata": {},
   "source": [
    "##### スロット台\n",
    "プレイの度に、勝率を変化させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1249dd4-afcc-46af-99ce-b136020c17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStatBandit:\n",
    "    def __init__(self, arms=10):\n",
    "        self.arms = arms\n",
    "        self.rates = np.random.rand(arms)\n",
    "\n",
    "    def play(self, arm):\n",
    "        rate = self.rates[arm]\n",
    "        self.rates += 0.1 * np.random.randn(self.arms)  # Add noise\n",
    "        if rate > np.random.rand():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2d746-26d9-49ef-9229-d2b9a20f747e",
   "metadata": {},
   "source": [
    "##### プレイヤー\n",
    "価値の更新のアルゴリズムを変更する。\n",
    "- 元のアルゴリズムは試行回数が増えるほど、最新の結果の影響度は低くなっていた。\n",
    "- 変更後のアルゴリズムは、最新の結果の影響度は試行回数の影響を受けないように変化させている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28c654-6085-442c-9886-87274f27a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAgent:\n",
    "    def __init__(self, epsilon, alpha, actions=10):\n",
    "        self.epsilon = epsilon      # ε貪欲法\n",
    "        self.Qs = np.zeros(actions) # 価値\n",
    "        \n",
    "        # 学習率を 1/n から 0 ≦ α ≦ 1 に変更。\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        self.Qs[action] = self.Qs[action] + self.alpha  * (reward - self.Qs[action])\n",
    "\n",
    "    def get_action(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, len(self.Qs))\n",
    "        return np.argmax(self.Qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06990a-623b-4e90-a21c-c422d953b0cb",
   "metadata": {},
   "source": [
    "#### コードの実行\n",
    "非定常問題に合わせて価値の更新のアルゴリズムを変更した方が、勝率が伸びる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a8883-d336-4d07-82b0-075fa274aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 200\n",
    "steps = 1000\n",
    "epsilon = 0.1\n",
    "alpha = 0.8\n",
    "agent_types = ['sample average', 'alpha const update']\n",
    "results = {}\n",
    "\n",
    "for agent_type in agent_types:\n",
    "    all_rates = np.zeros((runs, steps))  # (200, 1000)\n",
    "\n",
    "    # 強化学習を200回繰り返した平均で評価\n",
    "    for run in range(runs):\n",
    "        if agent_type == 'sample average':\n",
    "            agent = Agent(epsilon)\n",
    "        else:\n",
    "            agent = AlphaAgent(epsilon, alpha)\n",
    "\n",
    "        # 台の初期化（10台）\n",
    "        bandit = NonStatBandit()\n",
    "        total_reward = 0\n",
    "        rates = []\n",
    "        \n",
    "        # 1000ステップ\n",
    "        for step in range(steps):\n",
    "            action = agent.get_action()\n",
    "            reward = bandit.play(action)\n",
    "            agent.update(action, reward)\n",
    "            total_reward += reward\n",
    "            rates.append(total_reward / (step + 1))\n",
    "\n",
    "        all_rates[run] = rates\n",
    "\n",
    "    avg_rates = np.average(all_rates, axis=0)\n",
    "    results[agent_type] = avg_rates\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.ylabel('Average Rates')\n",
    "plt.xlabel('Steps')\n",
    "for key, avg_rates in results.items():\n",
    "    plt.plot(avg_rates, label=key)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a9df6-0c97-4921-8ced-d6eaa5539a9a",
   "metadata": {},
   "source": [
    "## 動的計画法（方策反復法、価値反復法）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d7fb5-bf0d-492a-96ed-9d22e1614f64",
   "metadata": {},
   "source": [
    "### 連立方程式ソルバをベルマン作用素的に解く"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3702c1-688e-4a4f-b826-284266bcb34a",
   "metadata": {},
   "source": [
    "#### 普通に実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cbab8a-7ffa-49e8-b74d-80fc574a7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {'L1': 0.0, 'L2': 0.0}\n",
    "new_V = V.copy()\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "    new_V['L1'] = 0.5 * (-1 + 0.9 * V['L1']) + 0.5 * (1 + 0.9 * V['L2'])\n",
    "    new_V['L2'] = 0.5 * (0 + 0.9 * V['L1']) + 0.5 * (-1 + 0.9 * V['L2'])\n",
    "\n",
    "    delta = abs(new_V['L1'] - V['L1'])\n",
    "    delta = max(delta, abs(new_V['L2'] - V['L2']))\n",
    "    V = new_V.copy()\n",
    "\n",
    "    cnt += 1\n",
    "    if delta < 0.0001:\n",
    "        print(V)\n",
    "        print(cnt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1fa00-fc02-47eb-b3b2-6ac29bb4d46c",
   "metadata": {},
   "source": [
    "#### 上書き方式の実装\n",
    "コッチのほうが少々早く収束する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc3a940-5a96-4c5d-9e33-721251091fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = {'L1': 0.0, 'L2': 0.0}\n",
    "\n",
    "cnt = 0\n",
    "while True:\n",
    "    t = 0.5 * (-1 + 0.9 * V['L1']) + 0.5 * (1 + 0.9 * V['L2'])\n",
    "    delta = abs(t - V['L1'])\n",
    "    V['L1'] = t\n",
    "\n",
    "    t = 0.5 * (0 + 0.9 * V['L1']) + 0.5 * (-1 + 0.9 * V['L2'])\n",
    "    delta = max(delta, abs(t - V['L2']))\n",
    "    V['L2'] = t\n",
    "\n",
    "    cnt += 1\n",
    "    if delta < 0.0001:\n",
    "        print(V)\n",
    "        print(cnt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a97b4e-d1dd-4fad-87a7-79eb06bca3a9",
   "metadata": {},
   "source": [
    "### 3*4マスのグリッド・ワールド"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d45e3-6adf-425f-bf1e-bb0ab615a49e",
   "metadata": {},
   "source": [
    "#### 状態価値関数のランダム初期化＆表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21977b5-dd57-40e9-a235-966b041cf3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dezerogym.gridworld import GridWorld\n",
    "\n",
    "env = GridWorld()\n",
    "V = {}\n",
    "for state in env.states():\n",
    "    V[state] = np.random.randn()\n",
    "env.render_v(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8956c65-c307-4d64-9f5c-e68f1078e4c8",
   "metadata": {},
   "source": [
    "#### 方策反復\n",
    "方策評価（ベルマン作用素で状態価値関数を推定）、方策改善を繰り返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2bde1d-327f-48a2-935c-d4e93119a87b",
   "metadata": {},
   "source": [
    "##### 方策評価、状態価値関数の推定\n",
    "爆弾の周辺が赤く、リンゴの周辺が緑になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3cd7e-5b11-4633-9732-94ff97367b70",
   "metadata": {},
   "source": [
    "###### 定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274451f-767f-4d3e-a19d-436b78e5adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from dezerogym.gridworld import GridWorld\n",
    "\n",
    "# 方策評価の1ステップ\n",
    "def eval_onestep(pi, V, env, gamma=0.9):\n",
    "    \n",
    "    # 全状態を処理\n",
    "    for state in env.states():\n",
    "        \n",
    "        # ゴールの\n",
    "        if state == env.goal_state:\n",
    "            V[state] = 0 # 状態価値は0（その前に報酬がある）\n",
    "            continue\n",
    "\n",
    "        # 固定の確率的方策を使用する\n",
    "        action_probs = pi[state]\n",
    "        \n",
    "        # 以下の実装が ≒ 方策評価\n",
    "        # ベルマン作用素で状態価値関数の推定\n",
    "        new_V = 0\n",
    "        for action, action_prob in action_probs.items():\n",
    "            next_state = env.next_state(state, action)\n",
    "            reward = env.reward(state, action, next_state)\n",
    "            # 報酬や状態遷移確率は=1なので表記されていない。\n",
    "            new_V += action_prob * (reward + gamma * V[next_state])\n",
    "            \n",
    "        # 新しい状態価値関数\n",
    "        V[state] = new_V\n",
    "        \n",
    "    return V # 新しい状態価値関数\n",
    "\n",
    "# 方策評価\n",
    "def policy_eval(pi, V, env, gamma, threshold=0.001):\n",
    "    \n",
    "    # 収束するまで実行\n",
    "    while True:\n",
    "        old_V = V.copy()\n",
    "        \n",
    "        # 全状態の評価\n",
    "        V = eval_onestep(pi, V, env, gamma)\n",
    "\n",
    "        # 収束したか確認\n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b54cd-70e4-4169-ab17-b4a3a53cf985",
   "metadata": {},
   "source": [
    "###### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266cf11-780c-486c-afb1-5105a433b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "\n",
    "# 割引率を指定\n",
    "gamma = 0.9\n",
    "\n",
    "# 確率的方策を指定\n",
    "pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})\n",
    "\n",
    "# 状態価値関数を初期化\n",
    "V = defaultdict(lambda: 0)\n",
    "\n",
    "# 方策反復の方策評価（状態価値関数を更新）\n",
    "V = policy_eval(pi, V, env, gamma)\n",
    "\n",
    "# 状態価値関数のヒートマップを作図\n",
    "env.render_v(V, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e24b8-fc5e-4388-bf75-f78edc24daa2",
   "metadata": {},
   "source": [
    "##### 方策改善、状態価値関数の変化\n",
    "方策反復で方策改善を行った後の方策 → 状態価値関数の変化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3689d-c230-4b96-ad87-0269b3355734",
   "metadata": {},
   "source": [
    "###### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02317cd-a5bb-4aee-ab72-7ce80c37854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方策反復\n",
    "def policy_iter(env, gamma, threshold=0.001, is_render=True):\n",
    "    \n",
    "    # 方策\n",
    "    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})\n",
    "    # 状態価値関数\n",
    "    V = defaultdict(lambda: 0)\n",
    "\n",
    "    # 方策反復\n",
    "    while True:\n",
    "        \n",
    "        # 方策評価\n",
    "        V = policy_eval(pi, V, env, gamma, threshold)\n",
    "        # 方策改善\n",
    "        new_pi = greedy_policy(V, env, gamma)\n",
    "\n",
    "        # 状態価値関数を描画\n",
    "        if is_render:\n",
    "            env.render_v(V, pi)\n",
    "\n",
    "        # 方策が収束するまで継続\n",
    "        if new_pi == pi:\n",
    "            break\n",
    "        pi = new_pi\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3a5a8-64b4-4a02-95f1-5721a70b105a",
   "metadata": {},
   "source": [
    "###### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6747a4ce-22f5-4aff-bd6e-73fa513d0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "\n",
    "# 割引率を指定\n",
    "gamma = 0.9\n",
    "\n",
    "# 方策反復\n",
    "pi = policy_iter(env, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c91728-d5a3-4239-8e0d-5b53d5befdd9",
   "metadata": {},
   "source": [
    "#### 価値反復\n",
    "ベルマン最適作用素で最適価値関数を求め最適方策を決める。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c76921-3928-48d5-bce9-520e43e9e582",
   "metadata": {},
   "source": [
    "###### 定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b88410-9d88-48b9-9a9b-53d000574189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方策評価の1ステップ\n",
    "def value_iter_onestep(V, env, gamma):\n",
    "    \n",
    "    # 全状態を処理\n",
    "    for state in env.states():\n",
    "        \n",
    "        # ゴールの\n",
    "        if state == env.goal_state:\n",
    "            V[state] = 0 # 状態価値は0（その前に報酬がある）\n",
    "            continue\n",
    "\n",
    "        # 以下の実装が ≒ 方策評価\n",
    "        # ベルマン最適作用素で状態価値関数の推定\n",
    "        action_values = []\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            reward = env.reward(state, action, next_state)\n",
    "            # 報酬や状態遷移確率は=1なので表記されていない。\n",
    "            value = reward + gamma * V[next_state]\n",
    "            action_values.append(value)\n",
    "\n",
    "        # 新しい状態価値関数\n",
    "        V[state] = max(action_values)\n",
    "    \n",
    "    return V # 新しい状態価値関数\n",
    "\n",
    "# 価値反復（＝方策評価の反復で方策反復はしない）\n",
    "def value_iter(V, env, gamma, threshold=0.001, is_render=True):\n",
    "    \n",
    "    # 状態価値関数\n",
    "    V = defaultdict(lambda: 0)\n",
    "\n",
    "    # 収束するまで実行\n",
    "    while True:\n",
    "        \n",
    "        # 状態価値関数を描画\n",
    "        if is_render:\n",
    "            env.render_v(V)\n",
    "\n",
    "        old_V = V.copy()\n",
    "        \n",
    "        # 全状態の評価\n",
    "        V = value_iter_onestep(V, env, gamma)\n",
    "\n",
    "        # 収束したか確認\n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    \n",
    "    # 最適方策を描画\n",
    "    pi = greedy_policy(V, env, gamma)\n",
    "    env.render_v(V, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1369344-a3f8-480a-a43b-8a520d9a46fa",
   "metadata": {},
   "source": [
    "###### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e7124-3db5-40a8-a9de-fa2061c93b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "\n",
    "# 割引率を指定\n",
    "gamma = 0.9\n",
    "\n",
    "# 価値反復\n",
    "value_iter(V, env, gamma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
