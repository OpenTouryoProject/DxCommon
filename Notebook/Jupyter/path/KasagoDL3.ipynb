{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f411ca43-7579-4ada-9c84-f8a4e38561d5",
   "metadata": {},
   "source": [
    "# o'reillyのカサゴ深層学習の本\n",
    "\n",
    "## MINST学習（誤差逆伝播）\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [計算グラフ](#計算グラフ)\n",
    "- [ニューラルネットワーク](#ニューラルネットワーク)\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/notebooks/\n",
    "- [深層学習（deep learning） - 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88deep%20learning%EF%BC%89)  \n",
    "&gt; [ニューラルネットワーク](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)  \n",
    "&gt; [ニューラルネットワーク（学習）](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%88%E5%AD%A6%E7%BF%92%EF%BC%89)  \n",
    "&gt; [深層学習の誤差逆伝播法](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be3053-bd3d-427b-89be-aa3a09c6134e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 深層学習の誤差逆伝播法\n",
    "https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820eb846-c5bb-4629-8914-c48b56b3d674",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 計算グラフ\n",
    "計算グラフで誤差逆伝播法を視覚的に理解するために使用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad540d-0d8e-4f38-accd-1f92ea12bef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 計算グラフのレイヤの定義\n",
    "基本的なレイヤの定義を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306bb74-928e-4e25-b610-19d1f039bc69",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 加算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc1fa0-5df9-430c-86c3-72fedb44e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加算レイヤ\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 加算\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # １を乗算\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df101571-de2c-4bea-97d3-9d9f33d3268a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 乗算レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e86d8-4ad8-4d7d-8936-6c561b7daa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乗算レイヤ\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 乗算\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 倍率を乗算\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52752d-3e88-45f1-a23e-8b4b8ed61bdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 計算グラフの逆伝播\n",
    "以下の計算グラフの逆伝播で得られる値は、\n",
    "- 連鎖率で微分を乗算していって得られた値で、\n",
    "- 其々の値が１増えた時、その他の値に変更がない場合、最終結果に影響を与える大きさ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d811b-4a08-4337-8981-cec34955b16d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 計算グラフの例１\n",
    "以下の計算グラフの逆伝播で其々の値が１増えた時、  \n",
    "その他の値に変更がない場合、最終結果に影響を与える大きさを計算。\n",
    "<img src=\"../work/computational-graphs1.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5eccb1-c94d-4001-be1a-b9605d09c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1 # 初期値は１（= dL/dL）\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"- forward -\")\n",
    "print(\"price:\", int(price))\n",
    "\n",
    "print(\"- backward -\")\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f19f6-1dd5-4dce-9447-63547cc08cbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 計算グラフの例２\n",
    "以下の計算グラフの逆伝播で其々の値が１増えた時、  \n",
    "その他の値に変更がない場合、最終結果に影響を与える大きさを計算。\n",
    "<img src=\"../work/computational-graphs2.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbd3bb-1ae1-4494-be25-a527f6765139",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    "\n",
    "print(\"- forward -\")\n",
    "print(\"price:\", int(price))\n",
    "\n",
    "print(\"- backward -\")\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95887a52-d48f-43b1-9a0c-8fd8775d2e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7367b-dab8-44a4-ba7d-2c074dab2e9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ニューラルネットワークのレイヤの定義\n",
    "ココではニューラルネットワーク用のレイヤの定義を行う。\n",
    "- ここでのポイントは、\n",
    "  - ニューラルネットワークを計算グラフと捉え逆伝播を計算すると、\n",
    "  - (数値) 微分をせずに勾配 → 重みを計算できる点。\n",
    "- backwardの出力は、\n",
    "  - forwardの入力の重み変数に対する勾配になる。\n",
    "  - 特に、forwardでは、最後に誤差関数を用いるため、  \n",
    "  誤差を０にするような重みを求めるための勾配になる。\n",
    "- 従って、レイヤのクラスは以下のメソッドを持つ。\n",
    "  - \\_\\_init\\_\\_(self, ...\n",
    "  - def forward(self, ...\n",
    "  - def backward(self, ...\n",
    "- 各レイヤに実装されている処理は[コチラ](KasagoDL2.ipynb)を参考にすると良い。\n",
    "- Import\n",
    "  - [common/layers.py](Kasago/common/layers.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980326c-c4ca-4da5-af8a-ec313011b1cc",
   "metadata": {},
   "source": [
    "##### 中間層・出力層のレイヤ\n",
    "- Affine  \n",
    "中間層・出力層の重みとバイアスを処理するレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71f029-f86a-4021-9010-40349746c9bd",
   "metadata": {},
   "source": [
    "##### 中間層の活性化関数のレイヤ\n",
    "- Sigmoid  \n",
    "[中間層の活性化関数のシグモイド関数](KasagoDL1.ipynb)\n",
    "- Relu  \n",
    "[中間層の活性化関数のRelu関数](KasagoDL1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31de83-4354-4af7-bc7e-1283c724c5fd",
   "metadata": {},
   "source": [
    "##### 出力層のソフトマックス関数のレイヤ\n",
    "- SoftmaxWithLoss  \n",
    "[出力層の活性化関数のソフトマックス関数](KasagoDL1.ipynb)と  \n",
    "[交差エントロピー誤差の損失関数](KasagoDL2.ipynb)の両方が実装されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1ad8f-0f25-4e74-9507-3f44c508fb0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ニューラルネットワークの逆伝播\n",
    "誤差逆伝播法で実装された２層ニューラルネットワーク。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce488c1-c437-4ca1-87af-51d14b257254",
   "metadata": {},
   "source": [
    "##### 定義\n",
    "- [common/layers.py](Kasago/common/layers.py)\n",
    "- [common/gradient.py](Kasago/common/gradient.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16d899-f461-4a1d-9f88-aca91036f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from kasago.common.layers import *\n",
    "from kasago.common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ２層ニューラルネットワーク・クラス\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # パラメタ（重みとバイアス）の初期化\n",
    "        self.params = {}\n",
    "        \n",
    "        # ランダムな重み\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        \n",
    "        # バイアス（０\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        # 出力レイヤ\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # 推論\n",
    "        \n",
    "        # レイヤを順次取り出しながらforward実行\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        # 活性化関数（Relu）はlayers中に含まれる。\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        # 勾配計算用関数\n",
    "        \n",
    "        # 推論（Affine.forward\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # 損失関数（SoftmaxWithLoss.forward\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        # 正解率計算用関数\n",
    "        \n",
    "        # 推論（分類\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # y, tを揃える\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        # 正解率の計算\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    # 学習（確認用）\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        # 数値微分による勾配計算により、\n",
    "        # predictとLearnを行う。\n",
    "        \n",
    "        # 勾配計算用関数\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # 勾配計算\n",
    "        # ヤヤコシイが\n",
    "        #  - ココのnumerical_gradientはself.numerical_gradientじゃない。\n",
    "        #  - なお、self.predictは、self.lossの中で実行されている。\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 学習（高速版）\n",
    "    # x:入力データ, t:教師データ\n",
    "    def gradient(self, x, t):\n",
    "        # 誤差逆伝播法による勾配計算により、\n",
    "        # predictとLearnを行う。\n",
    "        \n",
    "        # 勾配の計算\n",
    "        \n",
    "        # forward（predict\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward（Learn\n",
    "        dout = 1\n",
    "        \n",
    "        # 出力レイヤの逆順再生\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        # 中間レイヤの逆順再生\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # パラメタ（重みとバイアス）の更新\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ade8b-e836-4f3e-8637-1269ce683499",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d364a-a67c-4628-9005-c571bfd831e2",
   "metadata": {},
   "source": [
    "###### 数値微分と誤差逆伝播法の差\n",
    "- 数値微分と誤差逆伝播法で算出した勾配の差を確認\n",
    "- かなり小さい値になっていればOK（e^-10 前後）\n",
    "- Import\n",
    "  - [dataset/mnist.py](Kasago/dataset/mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23256dae-c694-448d-b4cd-71e8b730997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# ２層ニューラルネット\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# バッチサイズ３\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# 数値微分と誤差逆伝播法で勾配算出\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 算出した勾配の差を確認（コレを勾配確認と呼ぶ）\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e74e10-352c-4f80-94f0-f21966c449d0",
   "metadata": {},
   "source": [
    "###### 誤差逆伝播法でMISNTの画像認識\n",
    "- Import\n",
    "  - [dataset/mnist.py](Kasago/dataset/mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203be77e-0d4e-4ee8-b54d-b8cd01b7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# ２層ニューラルネット\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# ハイパーパラメタ\n",
    "\n",
    "# 繰り返し回数を適宜設定する\n",
    "iters_num = 10000\n",
    "# 学習率\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 訓練データサイズ\n",
    "train_size = x_train.shape[0]\n",
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "# サブセット数（イテレーション数）\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 結果を格納する変数\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(iters_num): # 繰り返し回数\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 学習\n",
    "    \n",
    "    # 数値微分は、都度、推論が必要なので遅過ぎて使えない。\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # 誤差逆伝播法の実用可能な高速版\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメタ（重みとバイアス）の更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 損失の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:# エポック毎\n",
    "        \n",
    "        # 正解率の計算\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        # 正解率の記録\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n",
    "        \n",
    "print(\"train and test have been completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
