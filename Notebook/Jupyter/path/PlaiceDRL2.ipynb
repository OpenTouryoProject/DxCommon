{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyã®ãƒ„ãƒã‚¬ãƒ¬ã‚¤å¼·åŒ–å­¦ç¿’ã®æœ¬\n",
    "\n",
    "## GPIã®æ‰‹æ³•ï¼šãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã€TDæ³•\n",
    "\n",
    "## [ç›®æ¬¡](TableOfContents.ipynb)\n",
    "- [ç’°å¢ƒæº–å‚™](#ç’°å¢ƒæº–å‚™)\n",
    "  - [ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)\n",
    "  - [ã‚¤ãƒ³ãƒãƒ¼ãƒˆ](#ã‚¤ãƒ³ãƒãƒ¼ãƒˆ)\n",
    "  - [å…±é€šé–¢æ•°](#å…±é€šé–¢æ•°)\n",
    "- [ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•](#ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•)\n",
    "  - [ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã§æœŸå¾…å€¤ã‚’è¨ˆç®—](#ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã§æœŸå¾…å€¤ã‚’è¨ˆç®—)\n",
    "  - [ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•](#ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•)\n",
    "    - [çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•A](#çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•A)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…A](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…A)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒA](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒA)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•B](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•B)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…B](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…B)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒB](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒB)\n",
    "- [æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•](#æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•)\n",
    "  - [é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](#é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)\n",
    "  - [æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•](#æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•)\n",
    "    - [çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•C](#çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•C)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•D](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•D)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…D](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…D)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒD](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒD)\n",
    "- [TDæ³•ï¼ˆæ™‚é–“çš„å·®åˆ†å­¦ç¿’æ³•ï¼‰](#TDæ³•ï¼ˆæ™‚é–“çš„å·®åˆ†å­¦ç¿’æ³•ï¼‰)\n",
    "  - [...](#...)\n",
    "  - [TDæ³•ã®æ–¹ç­–åå¾©æ³•](#TDæ³•ã®æ–¹ç­–åå¾©æ³•)\n",
    "    - [çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•E](#çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•E)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…E](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…E)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒE](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒE)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(Qå­¦ç¿’)F](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(Qå­¦ç¿’)F)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…F](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…F)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒF](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒF)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•Qå­¦ç¿’)G](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•Qå­¦ç¿’)G)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…G](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…G)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒG](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒG)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(SARSA)H](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(SARSA)H)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…H](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…H)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒH](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒH)\n",
    "    - [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•ã®SARSA)I](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•ã®SARSA)I)\n",
    "      - [ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…I](#ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…I)\n",
    "      - [ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒI](#ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒI)\n",
    "      \n",
    "## å‚è€ƒ\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-4/tree/master/ch01\n",
    "- [å¼·åŒ–å­¦ç¿’ï¼ˆReinforcement Learningï¼‰ - .NET é–‹ç™ºåŸºç›¤éƒ¨ä¼š Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88Reinforcement%20Learning%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39e2e7-67b7-464d-b254-68521ef5a3de",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea6589-87d6-459c-b8b8-55179ed99adf",
   "metadata": {},
   "source": [
    "### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1a3d-fc30-4a7f-b180-a221b11e7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install dezerogym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4d655-73aa-4cef-9def-0de5a4ef83d1",
   "metadata": {},
   "source": [
    "### ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854310-e2f8-48c9-986c-e05d242744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "from dezerogym.gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a909-b830-4776-890f-b6cef29a456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836fb9c-95b7-4cce-9651-d948fcc6fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### å…±é€šé–¢æ•°\n",
    "greedyæ³•ã€è²ªæ¬²æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf5812-b3f4-42f7-b382-45b41487c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs):\n",
    "    idxes = [i for i, x in enumerate(xs) if x == max(xs)]\n",
    "    if len(idxes) == 1:\n",
    "        return idxes[0]\n",
    "    elif len(idxes) == 0:\n",
    "        return np.random.choice(len(xs))\n",
    "    selected = np.random.choice(idxes)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d009e1f-a79e-42a0-af11-de63636e0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_probs(Q, state, epsilon=0, action_size=4):\n",
    "    qs = [Q[(state, action)] for action in range(action_size)]\n",
    "    max_action = argmax(qs)  # OR np.argmax(qs)\n",
    "    base_prob = epsilon / action_size\n",
    "    action_probs = {action: base_prob for action in range(action_size)}  #{0: Îµ/4, 1: Îµ/4, 2: Îµ/4, 3: Îµ/4}\n",
    "    action_probs[max_action] += (1 - epsilon)\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5acd28-b5e2-4714-904b-a3b1d891fda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbbd7c-35d8-4fec-8fd1-9807e70bea37",
   "metadata": {},
   "source": [
    "### ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã§æœŸå¾…å€¤ã‚’è¨ˆç®—\n",
    "ã‚µã‚¤ã‚³ãƒ­ã‚’äºŒå›æŒ¯ã£ãŸã¨ãã®åˆè¨ˆå€¤ã®æœŸå¾…å€¤\n",
    "- ã‚µãƒ³ãƒ—ãƒ«å›æ•°ã‚’å¢—ã‚„ã›ã°7ã«åæŸã—ã¦ã„ãã€‚\n",
    "- ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨åŒã˜ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ãªå®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba367222-12bb-4d07-b34c-5e2e76dd5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dices=2):\n",
    "    x = 0\n",
    "    for _ in range(dices):\n",
    "        x += np.random.choice([1, 2, 3, 4, 5, 6])\n",
    "    return x\n",
    "\n",
    "trial = 1000\n",
    "V, n = 0, 0\n",
    "\n",
    "for _ in range(trial):\n",
    "    s = sample()\n",
    "    n += 1\n",
    "    V += (s - V) / n\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766c5fe-7e6c-45b0-abe1-6b6238dfe1f4",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•\n",
    "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•ã§ã¯ã€çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‹è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®ã©ã¡ã‚‰ã‹ä¸€æ–¹ã‚’ä½¿ç”¨ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9ca6f-5ffd-49a3-b2be-7a89d886b00c",
   "metadata": {},
   "source": [
    "#### çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f71c7-9d3c-49a5-ac11-32b947cc7f42",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9266c3d-5616-4315-a9ee-5ecc2567717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # æ–¹ç­–\n",
    "        self.V = defaultdict(lambda: 0) # çŠ¶æ…‹ä¾¡å€¤é–¢æ•°\n",
    "        self.cnts = defaultdict(lambda: 0) # ã‚«ã‚¦ãƒ³ã‚¿\n",
    "        self.memory = []\n",
    "\n",
    "    # æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "    def eval(self):\n",
    "        G = 0\n",
    "        \n",
    "        # é€†ã‹ã‚‰è¨ˆç®—ã—ã¦ã„ãã€‚\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            \n",
    "            # æ–¹ç­–è©•ä¾¡\n",
    "            \n",
    "            # ã‚³ã‚³ãŒã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ãªåç›Šã®è¨ˆç®—\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # ã‚³ã‚³ã¯åŠ¹ç‡ã®è‰¯ã„çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã®æ›´æ–°ã®å®Ÿè£…\n",
    "            self.cnts[state] += 1 # ã‚«ã‚¦ãƒ³ã‚¿\n",
    "            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ã®çŠ¶æ…‹æ¯ã®åç›Šã¯ã€å±¥æ­´ã®æœ€æ–°ã®è¨ˆç®—çµæœã‚’åˆ©ç”¨ã—ã¦è¨ˆç®—ã—ã¦ã„ãã€‚\n",
    "            self.V[state] = self.V[state] + (1 / self.cnts[state]) * (G - self.V[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09884a-2d68-429c-a249-9e616205db1d",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c7a70-b29d-4b9f-b264-96c66854d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "env = GridWorld()\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "agent = RandomAgent()\n",
    "\n",
    "# 1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # ç’°å¢ƒã¨å±¥æ­´ã®åˆæœŸåŒ–\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        #å±¥æ­´ã‚’æ ¼ç´ã—\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            agent.eval() # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "            break\n",
    "            \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_v(agent.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fbd21-29dc-4741-8eac-0ab11fea3f58",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62cdef-c60f-46c8-b930-704adfedabd1",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…B\n",
    "ä»¥ä¸‹ã®ç‚¹ã‚’ä¿®æ­£ã™ã‚‹ã€‚\n",
    "- ä¾¡å€¤é–¢æ•°ã¯ã€çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã§ã¯ãªãæ–¹ç­–ä¾¡å€¤é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã€‚  \n",
    "ï¼ˆå®Ÿè£…ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï½“ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï½“,ï½ã‚’ä½¿ã£ã¦é›†è¨ˆã™ã‚‹æ§˜ã«å¤‰æ›´ï¼‰\n",
    "  - çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ï¼šğ‘‰ğ‘›(ğ‘ )ï¼(G(1)+G(2)+â€¦+G(ğ‘›))/n =ğ‘‰ğ‘›-1(ğ‘ )+1/ğ‘›{G(ğ‘›)-ğ‘‰ğ‘›-1(ğ‘ )} \n",
    "  - æ–¹ç­–ä¾¡å€¤é–¢æ•°ï¼šQğ‘›(ğ‘ ,ğ‘)ï¼(G(1)+G(2)+â€¦+G(ğ‘›))/n =Qğ‘›-1(ğ‘ ,ğ‘)+1/ğ‘›{G(ğ‘›)-ğ‘‰ğ‘›-1(ğ‘ ,ğ‘)} \n",
    "- è¡Œå‹•é¸æŠã®æˆ¦ç•¥\n",
    "  - è²ªæ¬²æ³•ã‚’æ¡ç”¨ï¼šæœ€é©è§£ã‚’100ï¼…ã®ç¢ºç‡ã§å®Ÿè¡Œã™ã‚‹ã€‚\n",
    "  - Îµ-è²ªæ¬²æ³•ã‚’æ¡ç”¨ï¼šæœ€é©è§£ã‚’ï¼‘-Îµ+Îµ/4ã€ã‚½ãƒ¬ä»¥å¤–ã‚’Îµ/4ã®ç¢ºç‡ã«æ›´æ–°ã—å®Ÿè¡Œã€‚\n",
    "- Îµ-è²ªæ¬²æ³•ã§ã¯ã€éå®šå¸¸ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã®æ§˜ã«å­¦ç¿’ç‡ã‚’1/ğ‘›ã‹ã‚‰Î±ã«å¤‰æ›´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c7e9d-c1e2-455c-b338-612630fd0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class McAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # æ–¹ç­–\n",
    "        self.Q = defaultdict(lambda: 0) # è¡Œå‹•ä¾¡å€¤é–¢æ•°\n",
    "        self.memory = []\n",
    "\n",
    "    # æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å±¥æ­´ã‚’è¿½åŠ \n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "    def update(self):\n",
    "        G = 0\n",
    "        \n",
    "        # é€†ã‹ã‚‰è¨ˆç®—ã—ã¦ã„ãã€‚\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            key = (state, action)\n",
    "            \n",
    "            # æ–¹ç­–è©•ä¾¡\n",
    "            \n",
    "            # ã‚³ã‚³ãŒã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ãªåç›Šã®è¨ˆç®—\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # ã‚³ã‚³ã¯åŠ¹ç‡ã®è‰¯ã„è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®æ›´æ–°ã®å®Ÿè£…\n",
    "            # è¡Œå‹•ä¾¡å€¤é–¢æ•°ã¯ã€å½“è©²çŠ¶æ…‹æ¯ã€ä¸Šãƒ»ä¸‹ãƒ»å·¦ãƒ»å³ã®Actionæ¯ã«ä¾¡å€¤ãŒã‚ã‚‹ã€‚\n",
    "            # ç¢ºç‡åˆ†å¸ƒãŒå¤‰åŒ–ã™ã‚‹ãŸã‚æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’ç”¨ã„ã‚‹ã€‚\n",
    "            self.Q[key] = self.Q[key] + self.alpha * (G - self.Q[key])\n",
    "            \n",
    "            # æ–¹ç­–æ”¹å–„ï¼ˆæ–¹ç­–ã¯ã€çŠ¶æ…‹æ¯ã€ä¸Šãƒ»ä¸‹ãƒ»å·¦ãƒ»å³ã®ç¢ºç‡ï¼‰ã«Îµ-è²ªæ¬²æ³•ï¼ˆÎµ-greedyæ³•ï¼‰ã‚’æ¡ç”¨ã€‚\n",
    "            self.pi[state] = greedy_probs(self.Q, state, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054e5b1-ec66-4151-8165-a475720ca829",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3af88c-aa66-438b-8365-d5b6bf23b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "env = GridWorld()\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "agent = McAgent()\n",
    "\n",
    "# 10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # ç’°å¢ƒã¨å±¥æ­´ã®åˆæœŸåŒ–\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # å±¥æ­´ã‚’æ ¼ç´ã—\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            agent.update() # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "            break\n",
    "        \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e6ab4-29e4-4b90-aaa6-52e66105edc7",
   "metadata": {},
   "source": [
    "## æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23ec87-08fd-4810-8527-86aaed28575a",
   "metadata": {},
   "source": [
    "### é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7819620-0326-4167-89b4-eccb12f163fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "pi = np.array([0.1, 0.1, 0.8])\n",
    "\n",
    "# =========== Expectation ==================\n",
    "e = np.sum(x * pi)\n",
    "print('E_pi[x]', e)\n",
    "\n",
    "# =========== Monte Carlo ==================\n",
    "n = 100\n",
    "samples = []\n",
    "for _ in range(n):\n",
    "    s = np.random.choice(x, p=pi)\n",
    "    samples.append(s)\n",
    "print('MC: {:.2f} (var: {:.2f})'.format(np.mean(samples), np.var(samples)))\n",
    "\n",
    "# =========== Importance Sampling ===========\n",
    "b = np.array([0.2, 0.2, 0.6])  #b = np.array([1/3, 1/3, 1/3])\n",
    "samples = []\n",
    "for _ in range(n):\n",
    "    idx = np.arange(len(b))  # [0, 1, 2]\n",
    "    i = np.random.choice(idx, p=b)\n",
    "    s = x[i]\n",
    "    rho = pi[i] / b[i]\n",
    "    samples.append(rho * s)\n",
    "print('IS: {:.2f} (var: {:.2f})'.format(np.mean(samples), np.var(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c28f8-ebdc-442c-9271-0c87da4210ca",
   "metadata": {},
   "source": [
    "### æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•\n",
    "- ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼šè¡Œå‹•ã®é¸æŠã¯ã€æ¢ç´¢ã¨é©ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚ŠãŸã„ãŒã€è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®è©•ä¾¡ã«ã¯é©ç”¨ã®ã¿ã‚’è¡Œã„ãŸã„ã€‚\n",
    "- åˆ¥ã®ã‚µã‚¤ãƒˆã§ã¯ã€å‰åŠã‚’è²ªæ¬²æ³•ï¼ˆgreedyæ³•ï¼‰ã€å¾ŒåŠã®å­¦ç¿’åˆ©ç”¨ã‚’Îµ-è²ªæ¬²æ³•ï¼ˆÎµ-greedyæ³•ï¼‰ã¨è¨€ã†æ–¹æ³•ã‚‚ææ¡ˆã•ã‚Œã¦ã„ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f15100-1dee-4101-980b-1628921cff04",
   "metadata": {},
   "source": [
    "#### çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•C\n",
    "- ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã¯ã€çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ãƒ»è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®ã„ãšã‚Œã‹ã‚’åˆ©ç”¨ãªã®ã§\n",
    "- æ–¹ç­–ã‚ªãƒ•ã®ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã®æ–¹ç­–åå¾©æ³•ã«ã¯çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ç‰ˆã¯ç„¡ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3156498-7ad0-4f75-854e-bed2011f5eeb",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc12924-91cd-4b43-b27a-fffea2c93654",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90da7a-a324-46e5-ae1e-f14be31066c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class McOffPolicyAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.2\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # ä¾¡å€¤æ¨å®šç”¨ã®æ–¹ç­–\n",
    "        self.b = defaultdict(lambda: random_actions) # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = []\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å±¥æ­´ã‚’è¿½åŠ \n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "    def update(self):\n",
    "        G = 0\n",
    "        rho = 1\n",
    "\n",
    "        # é€†ã‹ã‚‰è¨ˆç®—ã—ã¦ã„ãã€‚\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            key = (state, action)\n",
    "\n",
    "            # æ–¹ç­–è©•ä¾¡\n",
    "            # ã‚³ã‚³ãŒã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ãªåç›Šã®è¨ˆç®—\n",
    "            # rhoãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã®ãŒãƒã‚¤ãƒ³ãƒˆ\n",
    "            G = reward + rho * self.gamma * G\n",
    "            \n",
    "            # ã‚³ã‚³ã¯åŠ¹ç‡ã®è‰¯ã„è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®æ›´æ–°ã®å®Ÿè£…\n",
    "            # å½“è©²çŠ¶æ…‹æ¯ã€ä¸Šãƒ»ä¸‹ãƒ»å·¦ãƒ»å³ã®Actionæ¯ã«ä¾¡å€¤ãŒã‚ã‚‹ã€‚\n",
    "            self.Q[key] = self.Q[key] + self.alpha * (G - self.Q[key])\n",
    "            \n",
    "            # é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Ïï¼ˆé‡ã¿ï¼‰ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "            rho *= self.pi[state][action] / self.b[state][action]\n",
    "            \n",
    "            # 2ã¤ã®æ–¹ç­–ã‚’å…¶ã€…ã€åˆ¥ã€…ã«æ›´æ–°ã™ã‚‹\n",
    "            self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # greedyæ³•\n",
    "            self.b[state] = greedy_probs(self.Q, state, self.epsilon) # Îµ-greedyæ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b32d21-2da6-4d7b-bcb6-3694bd7849f8",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒD\n",
    "ã‚³ã‚³ã®å®Ÿè£…ã¯ã»ã¼å¤‰æ›´ãªã—ï¼ˆMcAgent â†’ McOffPolicyAgentã®å¤‰æ›´ãƒ€ã‚±ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d860af-f181-42e0-8c1a-4b2ca3ecc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = McOffPolicyAgent()\n",
    "\n",
    "# 10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # ç’°å¢ƒã¨å±¥æ­´ã®åˆæœŸåŒ–\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # å±¥æ­´ã‚’æ ¼ç´ã—\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            agent.update() # æ–¹ç­–åå¾©ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯ï¼‰\n",
    "            break\n",
    "\n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00273ec-a047-4702-a526-3b6f434e5f6f",
   "metadata": {},
   "source": [
    "## TDæ³•ï¼ˆæ™‚é–“çš„å·®åˆ†å­¦ç¿’æ³•ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0459b-eb54-4fad-94ce-ae44cef14095",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c35af-7b62-4e77-97f9-15a0311cf2bc",
   "metadata": {},
   "source": [
    "### TDæ³•ã®æ–¹ç­–åå¾©æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba340-f2ba-4994-a5a0-5d93b6a0e2d9",
   "metadata": {},
   "source": [
    "#### çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•E\n",
    "[çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•A](#çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•A)ã¨æ¯”è¼ƒã™ã‚‹ã¨è‰¯ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc30e0-3857-4c49-96f4-e754ec92c926",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7d64b-1652-4011-af80-2f4313217130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TdAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.01\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # æ–¹ç­–\n",
    "        self.V = defaultdict(lambda: 0) # çŠ¶æ…‹ä¾¡å€¤é–¢æ•°\n",
    "\n",
    "    # æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "    def eval(self, state, reward, next_state, done):\n",
    "        \n",
    "        # ã‚´ãƒ¼ãƒ«ã®çŠ¶æ…‹ä¾¡å€¤ã¯0ï¼ˆãã®å‰ã«å ±é…¬ãŒã‚ã‚‹ï¼‰\n",
    "        # next_Vã¯ã€done=trueãªã‚‰0ã€done=tureã§ãªã„ãªã‚‰self.V[next_state]\n",
    "        next_V = 0 if done else self.V[next_state]\n",
    "        \n",
    "        # æ–¹ç­–è©•ä¾¡\n",
    "        # ã‚³ã‚³ãŒã€= TDï¼ˆæ™‚é–“å·®åˆ†å­¦ç¿’ï¼‰æ³•ã®å®Ÿè£…\n",
    "        #target = reward + self.gamma * next_V\n",
    "        #self.V[state] += self.alpha * (target - self.V[state])\n",
    "        self.V[state] = self.V[state] + self.alpha * ((reward + self.gamma * next_V) - self.V[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e56b4-5b67-4fe2-b3ee-f575ef931d80",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc0e18-8b52-4c71-ab8f-e372b1fe8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "env = GridWorld()\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "agent = TdAgent()\n",
    "\n",
    "# 1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # ç’°å¢ƒã®åˆæœŸåŒ–\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "        agent.eval(state, reward, next_state, done)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_v(agent.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3143d87-cf2c-4af5-aa89-bb9ca7c6f3c5",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(Qå­¦ç¿’)F\n",
    "- [è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•B](#è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•B)ã¨æ¯”è¼ƒã™ã‚‹ã¨è‰¯ã„ãŒã€ã‹ãªã‚Šå®Ÿè£…ãŒé•ã†ã®ã§è§£ã‚Šé›£ã„ã€‚\n",
    "- ä»¥é™ã®é †ç•ªã¯ã€Qã€Qï¼ˆæ–¹ç­–ã‚ªãƒ•ï¼‰ã€SARSAã€SARSAï¼ˆæ–¹ç­–ã‚ªãƒ•ï¼‰ã«å¤‰æ›´ã€‚\n",
    "- æ­´å²çš„ã«Q â†’ SARSAã¨ç™»å ´ã€Qã¯æ–¹ç­–ã‚ªãƒ•ãŒæ¨™æº–ã€SARSAã¯æ–¹ç­–ã‚ªãƒ³ãŒæ¨™æº–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade416b-2fd2-4358-8f21-48686af26651",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841e616-384b-49aa-a2a0-7c136a3a2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "\n",
    "    # Îµ-greedyã§è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            qs = [self.Q[state, a] for a in range(self.action_size)]\n",
    "            return np.argmax(qs)\n",
    "    \n",
    "    # æ–¹ç­–æ”¹å–„ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # next_q_maxã‚’è¨ˆç®—\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "\n",
    "        # ã‚³ã‚³ãŒQå­¦ç¿’\n",
    "        #target = reward + self.gamma * next_q_max\n",
    "        #self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q_max) - self.Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bf817-4e8e-4082-86c2-998f38119547",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993a662-d95e-4b8f-b264-e961323187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# 1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # ç’°å¢ƒã®åˆæœŸåŒ–\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665e8b6-3aa8-467a-b162-aa477fd6685c",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•Qå­¦ç¿’)G\n",
    "- Qã¯æ–¹ç­–ã‚ªãƒ•ãŒæ¨™æº–ã§é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å¿…è¦ã¨ã—ãªã„ã€‚\n",
    "- ç†ç”±ã¯è¡Œå‹•ä¾¡å€¤é–¢æ•°ã®æ¨å®šã‚’æ–¹ç­–ã«ä¾å­˜ã—ãªã„greedyã§è¡Œã†ã®ã§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3ac72-a0df-44a7-8298-1b24f443c775",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e3e54-7041-4125-a78a-ab0f4cd1577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        #self.pi = defaultdict(lambda: random_actions) # ä¾¡å€¤æ¨å®šç”¨ã®æ–¹ç­–\n",
    "        self.b = defaultdict(lambda: random_actions) # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # æ–¹ç­–æ”¹å–„ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # next_q_maxã‚’è¨ˆç®—ï¼ˆgreedyãªã®ã§piãŒä¸è¦ã«ï¼‰\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "\n",
    "        # ã‚³ã‚³ãŒQå­¦ç¿’\n",
    "        #target = reward + self.gamma * next_q_max\n",
    "        #self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q_max) - self.Q[state, action])\n",
    "\n",
    "        # 2ã¤ã®æ–¹ç­–ã‚’å…¶ã€…ã€åˆ¥ã€…ã«æ›´æ–°ã™ã‚‹\n",
    "        #self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # ä¾¡å€¤æ¨å®šç”¨ã®æ–¹ç­–ã¯greedy\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon) # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–ã¯Îµ-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2f387-4dfc-41fe-a38c-f21fba33d4dc",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1dc5e-73e7-4ba9-9f5f-5de7c3ed7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# 10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # ç’°å¢ƒã®åˆæœŸåŒ–\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442f960-d218-43d1-b92d-99ac0aeac2d0",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(SARSA)H\n",
    "SARSAã¯æ–¹ç­–ã‚ªãƒ³ãŒæ¨™æº–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77917e-16a8-449d-b3f9-7e3b9224d2dc",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cb524-bcf6-4c10-9b3f-6d94e3aa61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # æ–¹ç­–\n",
    "        self.Q = defaultdict(lambda: 0) # è¡Œå‹•ä¾¡å€¤é–¢æ•°\n",
    "        self.memory = deque(maxlen=2)\n",
    "\n",
    "    # æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ  \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "    def update(self, state, action, reward, done):\n",
    "        \n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—ã®å±¥æ­´ã‚’è¿½åŠ \n",
    "        self.memory.append((state, action, reward, done))\n",
    "        \n",
    "        # SARASãªã®ã§2ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦\n",
    "        if len(self.memory) < 2:\n",
    "            return\n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _ = self.memory[1]\n",
    "        \n",
    "        # next_q\n",
    "        next_q = 0 if done else self.Q[next_state, next_action]\n",
    "        \n",
    "        # ã‚³ã‚³ãŒSARAS\n",
    "        #target = reward + self.gamma * next_q\n",
    "        #self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q) - self.Q[state, action])\n",
    "        \n",
    "        self.pi[state] = greedy_probs(self.Q, state, self.epsilon) # ç›´greedyã§ã‚‚è‰¯ã„ã®ã§ã¯ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa15a6-3bb8-42ce-aa24-5a5f34f4ace8",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df82d-47cb-4eb8-b70a-03a60e29a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = SarsaAgent()\n",
    "\n",
    "# 10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # ç’°å¢ƒã¨å±¥æ­´ã®åˆæœŸåŒ–\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "        agent.update(state, action, reward, done)\n",
    "\n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            agent.update(next_state, None, None, None) # æ–¹ç­–åå¾©ï¼ˆæœ€å¾Œï¼‰\n",
    "            break\n",
    "        \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d5695-2733-4c65-9705-2dfdb4aa5d76",
   "metadata": {},
   "source": [
    "#### è¡Œå‹•ä¾¡å€¤é–¢æ•°ã‚’ä½¿ã£ãŸæ–¹ç­–åå¾©æ³•(æ–¹ç­–ã‚ªãƒ•ã®SARSA)I\n",
    "æ–¹ç­–ã‚ªãƒ•ã®SARSAã§ã¯é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ã«ãªã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fd848-f202-4b5d-9bda-3c8dbd66c975",
   "metadata": {},
   "source": [
    "##### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45b638-a371-4fb7-8aaf-9d0a2c28ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaOffPolicyAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # ä¾¡å€¤æ¨å®šç”¨ã®æ–¹ç­–\n",
    "        self.b = defaultdict(lambda: random_actions) # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = deque(maxlen=2)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–ã‹ã‚‰è¡Œå‹•ã‚’é¸æŠ\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "    def update(self, state, action, reward, done):\n",
    "        \n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—ã®å±¥æ­´ã‚’è¿½åŠ \n",
    "        self.memory.append((state, action, reward, done))\n",
    "        \n",
    "        # SARASãªã®ã§2ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦\n",
    "        if len(self.memory) < 2:\n",
    "            return\n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _ = self.memory[1]\n",
    "\n",
    "        # next_q, rho\n",
    "        if done:\n",
    "            next_q = 0\n",
    "            rho = 1\n",
    "        else:\n",
    "            # next_q\n",
    "            next_q = self.Q[next_state, next_action]\n",
    "            # é‡ç‚¹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Ïï¼ˆé‡ã¿ï¼‰ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "            rho = self.pi[next_state][next_action] / self.b[next_state][next_action]\n",
    "        \n",
    "        # ã‚³ã‚³ãŒSARASï¼ˆæ–¹ç­–ã‚ªãƒ•ï¼‰\n",
    "        #target = rho * (reward + self.gamma * next_q)\n",
    "        #self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((rho * (reward + self.gamma * next_q)) - self.Q[state, action])\n",
    "        \n",
    "        # 2ã¤ã®æ–¹ç­–ã‚’å…¶ã€…ã€åˆ¥ã€…ã«æ›´æ–°ã™ã‚‹\n",
    "        self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # ä¾¡å€¤æ¨å®šç”¨ã®æ–¹ç­–ã¯greedy\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon) # ãƒ‡ãƒ¼ã‚¿åé›†ç”¨ã®æ–¹ç­–ã¯Îµ-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425dc12-e2e4-4ff2-bdc4-64f457233040",
   "metadata": {},
   "source": [
    "##### ã‚°ãƒªãƒƒãƒ‰ãƒ»ãƒ¯ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡ŒI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa76f-40b1-40f2-be83-0c84f6e5079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = SarsaOffPolicyAgent()\n",
    "\n",
    "# 10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # ç’°å¢ƒã¨å±¥æ­´ã®åˆæœŸåŒ–\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureã¾ã§ãŒ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\n",
    "    while True:\n",
    "        \n",
    "        # Actionã§Step\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # æ–¹ç­–åå¾©ï¼ˆã‚¹ãƒ†ãƒƒãƒ—æ¯ï¼‰\n",
    "        agent.update(state, action, reward, done)\n",
    "\n",
    "        # done=tureã¾ã§ã€\n",
    "        if done:\n",
    "            agent.update(next_state, None, None, None) # æ–¹ç­–åå¾©ï¼ˆæœ€å¾Œï¼‰\n",
    "            break\n",
    "            \n",
    "        # æ¬¡ã®çŠ¶æ…‹\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffe13b-688c-475f-a452-a22f9da83ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
