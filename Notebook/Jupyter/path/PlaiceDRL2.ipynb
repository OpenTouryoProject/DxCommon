{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyのツノガレイ強化学習の本\n",
    "\n",
    "## GPIの手法：モンテカルロ法、TD法\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- [モンテカルロ法](#モンテカルロ法)\n",
    "  - [サンプルモデルで期待値を計算](#サンプルモデルで期待値を計算)\n",
    "  - [モンテカルロ法の方策反復法](#モンテカルロ法の方策反復法)\n",
    "    - [状態価値関数を使った方策反復法A](#状態価値関数を使った方策反復法A)\n",
    "      - [エージェント実装A](#エージェント実装A)\n",
    "      - [グリッド・ワールド上でエージェントを実行A](#グリッド・ワールド上でエージェントを実行A)\n",
    "    - [行動価値関数を使った方策反復法B](#行動価値関数を使った方策反復法B)\n",
    "      - [エージェントの実装B](#エージェントの実装B)\n",
    "      - [グリッド・ワールド上でエージェントを実行B](#グリッド・ワールド上でエージェントを実行B)\n",
    "- [方策オフのモンテカルロ法](#方策オフのモンテカルロ法)\n",
    "  - [重点サンプリング](#重点サンプリング)\n",
    "  - [方策オフのモンテカルロ法の方策反復法](#方策オフのモンテカルロ法の方策反復法)\n",
    "    - [状態価値関数を使った方策反復法C](#状態価値関数を使った方策反復法C)\n",
    "    - [行動価値関数を使った方策反復法D](#行動価値関数を使った方策反復法D)\n",
    "      - [エージェント実装D](#エージェント実装D)\n",
    "      - [グリッド・ワールド上でエージェントを実行D](#グリッド・ワールド上でエージェントを実行D)\n",
    "- [TD法（時間的差分学習法）](#TD法（時間的差分学習法）)\n",
    "  - [...](#...)\n",
    "  - [TD法の方策反復法](#TD法の方策反復法)\n",
    "    - [状態価値関数を使った方策反復法E](#状態価値関数を使った方策反復法E)\n",
    "      - [エージェント実装E](#エージェント実装E)\n",
    "      - [グリッド・ワールド上でエージェントを実行E](#グリッド・ワールド上でエージェントを実行E)\n",
    "    - [行動価値関数を使った方策反復法(Q学習)F](#行動価値関数を使った方策反復法(Q学習)F)\n",
    "      - [エージェント実装F](#エージェント実装F)\n",
    "      - [グリッド・ワールド上でエージェントを実行F](#グリッド・ワールド上でエージェントを実行F)\n",
    "    - [行動価値関数を使った方策反復法(方策オフQ学習)G](#行動価値関数を使った方策反復法(方策オフQ学習)G)\n",
    "      - [エージェント実装G](#エージェント実装G)\n",
    "      - [グリッド・ワールド上でエージェントを実行G](#グリッド・ワールド上でエージェントを実行G)\n",
    "    - [行動価値関数を使った方策反復法(SARSA)H](#行動価値関数を使った方策反復法(SARSA)H)\n",
    "      - [エージェント実装H](#エージェント実装H)\n",
    "      - [グリッド・ワールド上でエージェントを実行H](#グリッド・ワールド上でエージェントを実行H)\n",
    "    - [行動価値関数を使った方策反復法(方策オフのSARSA)I](#行動価値関数を使った方策反復法(方策オフのSARSA)I)\n",
    "      - [エージェント実装I](#エージェント実装I)\n",
    "      - [グリッド・ワールド上でエージェントを実行I](#グリッド・ワールド上でエージェントを実行I)\n",
    "      \n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-4/tree/master/ch01\n",
    "- [強化学習（Reinforcement Learning） - .NET 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88Reinforcement%20Learning%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39e2e7-67b7-464d-b254-68521ef5a3de",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea6589-87d6-459c-b8b8-55179ed99adf",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1a3d-fc30-4a7f-b180-a221b11e7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install dezerogym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4d655-73aa-4cef-9def-0de5a4ef83d1",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854310-e2f8-48c9-986c-e05d242744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import deque\n",
    "from dezerogym.gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a909-b830-4776-890f-b6cef29a456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836fb9c-95b7-4cce-9651-d948fcc6fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共通関数\n",
    "greedy法、貪欲法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf5812-b3f4-42f7-b382-45b41487c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(xs):\n",
    "    idxes = [i for i, x in enumerate(xs) if x == max(xs)]\n",
    "    if len(idxes) == 1:\n",
    "        return idxes[0]\n",
    "    elif len(idxes) == 0:\n",
    "        return np.random.choice(len(xs))\n",
    "    selected = np.random.choice(idxes)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d009e1f-a79e-42a0-af11-de63636e0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_probs(Q, state, epsilon=0, action_size=4):\n",
    "    qs = [Q[(state, action)] for action in range(action_size)]\n",
    "    max_action = argmax(qs)  # OR np.argmax(qs)\n",
    "    base_prob = epsilon / action_size\n",
    "    action_probs = {action: base_prob for action in range(action_size)}  #{0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}\n",
    "    action_probs[max_action] += (1 - epsilon)\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5acd28-b5e2-4714-904b-a3b1d891fda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## モンテカルロ法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbbd7c-35d8-4fec-8fd1-9807e70bea37",
   "metadata": {},
   "source": [
    "### サンプルモデルで期待値を計算\n",
    "サイコロを二回振ったときの合計値の期待値\n",
    "- サンプル回数を増やせば7に収束していく。\n",
    "- バンディット問題と同じインクリメンタルな実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba367222-12bb-4d07-b34c-5e2e76dd5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dices=2):\n",
    "    x = 0\n",
    "    for _ in range(dices):\n",
    "        x += np.random.choice([1, 2, 3, 4, 5, 6])\n",
    "    return x\n",
    "\n",
    "trial = 1000\n",
    "V, n = 0, 0\n",
    "\n",
    "for _ in range(trial):\n",
    "    s = sample()\n",
    "    n += 1\n",
    "    V += (s - V) / n\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5766c5fe-7e6c-45b0-abe1-6b6238dfe1f4",
   "metadata": {},
   "source": [
    "### モンテカルロ法の方策反復法\n",
    "モンテカルロ法の方策反復法では、状態価値関数か行動価値関数のどちらか一方を使用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9ca6f-5ffd-49a3-b2be-7a89d886b00c",
   "metadata": {},
   "source": [
    "#### 状態価値関数を使った方策反復法A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f71c7-9d3c-49a5-ac11-32b947cc7f42",
   "metadata": {},
   "source": [
    "##### エージェント実装A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9266c3d-5616-4315-a9ee-5ecc2567717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 方策\n",
    "        self.V = defaultdict(lambda: 0) # 状態価値関数\n",
    "        self.cnts = defaultdict(lambda: 0) # カウンタ\n",
    "        self.memory = []\n",
    "\n",
    "    # 方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # 方策反復（エピソード毎）\n",
    "    def eval(self):\n",
    "        G = 0\n",
    "        \n",
    "        # 逆から計算していく。\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            \n",
    "            # 方策評価\n",
    "            \n",
    "            # ココがインクリメンタルな収益の計算\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # ココは効率の良い状態価値関数の更新の実装\n",
    "            self.cnts[state] += 1 # カウンタ\n",
    "            # エピソード毎の状態毎の収益は、履歴の最新の計算結果を利用して計算していく。\n",
    "            self.V[state] = self.V[state] + (1 / self.cnts[state]) * (G - self.V[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b09884a-2d68-429c-a249-9e616205db1d",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c7a70-b29d-4b9f-b264-96c66854d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "# エージェントのインスタンスを作成\n",
    "agent = RandomAgent()\n",
    "\n",
    "# 1000エピソード\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境と履歴の初期化\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        #履歴を格納し\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            agent.eval() # 方策反復（エピソード毎）\n",
    "            break\n",
    "            \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_v(agent.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59fbd21-29dc-4741-8eac-0ab11fea3f58",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62cdef-c60f-46c8-b930-704adfedabd1",
   "metadata": {},
   "source": [
    "##### エージェントの実装B\n",
    "以下の点を修正する。\n",
    "- 価値関数は、状態価値関数ではなく方策価値関数を使用する。  \n",
    "（実装はインデックスｓからインデックスｓ,ａを使って集計する様に変更）\n",
    "  - 状態価値関数：𝑉𝑛(𝑠)＝(G(1)+G(2)+…+G(𝑛))/n =𝑉𝑛-1(𝑠)+1/𝑛{G(𝑛)-𝑉𝑛-1(𝑠)} \n",
    "  - 方策価値関数：Q𝑛(𝑠,𝑎)＝(G(1)+G(2)+…+G(𝑛))/n =Q𝑛-1(𝑠,𝑎)+1/𝑛{G(𝑛)-𝑉𝑛-1(𝑠,𝑎)} \n",
    "- 行動選択の戦略\n",
    "  - 貪欲法を採用：最適解を100％の確率で実行する。\n",
    "  - ε-貪欲法を採用：最適解を１-ε+ε/4、ソレ以外をε/4の確率に更新し実行。\n",
    "- ε-貪欲法では、非定常バンディット問題の様に学習率を1/𝑛からαに変更。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c7e9d-c1e2-455c-b338-612630fd0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class McAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 方策\n",
    "        self.Q = defaultdict(lambda: 0) # 行動価値関数\n",
    "        self.memory = []\n",
    "\n",
    "    # 方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # サンプリングして履歴を追加\n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # 方策反復（エピソード毎）\n",
    "    def update(self):\n",
    "        G = 0\n",
    "        \n",
    "        # 逆から計算していく。\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            key = (state, action)\n",
    "            \n",
    "            # 方策評価\n",
    "            \n",
    "            # ココがインクリメンタルな収益の計算\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # ココは効率の良い行動価値関数の更新の実装\n",
    "            # 行動価値関数は、当該状態毎、上・下・左・右のAction毎に価値がある。\n",
    "            # 確率分布が変化するため指数移動平均を用いる。\n",
    "            self.Q[key] = self.Q[key] + self.alpha * (G - self.Q[key])\n",
    "            \n",
    "            # 方策改善（方策は、状態毎、上・下・左・右の確率）にε-貪欲法（ε-greedy法）を採用。\n",
    "            self.pi[state] = greedy_probs(self.Q, state, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054e5b1-ec66-4151-8165-a475720ca829",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3af88c-aa66-438b-8365-d5b6bf23b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "# エージェントのインスタンスを作成\n",
    "agent = McAgent()\n",
    "\n",
    "# 10000エピソード\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境と履歴の初期化\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 履歴を格納し\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            agent.update() # 方策反復（エピソード毎）\n",
    "            break\n",
    "        \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e6ab4-29e4-4b90-aaa6-52e66105edc7",
   "metadata": {},
   "source": [
    "## 方策オフのモンテカルロ法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23ec87-08fd-4810-8527-86aaed28575a",
   "metadata": {},
   "source": [
    "### 重点サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7819620-0326-4167-89b4-eccb12f163fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "pi = np.array([0.1, 0.1, 0.8])\n",
    "\n",
    "# =========== Expectation ==================\n",
    "e = np.sum(x * pi)\n",
    "print('E_pi[x]', e)\n",
    "\n",
    "# =========== Monte Carlo ==================\n",
    "n = 100\n",
    "samples = []\n",
    "for _ in range(n):\n",
    "    s = np.random.choice(x, p=pi)\n",
    "    samples.append(s)\n",
    "print('MC: {:.2f} (var: {:.2f})'.format(np.mean(samples), np.var(samples)))\n",
    "\n",
    "# =========== Importance Sampling ===========\n",
    "b = np.array([0.2, 0.2, 0.6])  #b = np.array([1/3, 1/3, 1/3])\n",
    "samples = []\n",
    "for _ in range(n):\n",
    "    idx = np.arange(len(b))  # [0, 1, 2]\n",
    "    i = np.random.choice(idx, p=b)\n",
    "    s = x[i]\n",
    "    rho = pi[i] / b[i]\n",
    "    samples.append(rho * s)\n",
    "print('IS: {:.2f} (var: {:.2f})'.format(np.mean(samples), np.var(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c28f8-ebdc-442c-9271-0c87da4210ca",
   "metadata": {},
   "source": [
    "### 方策オフのモンテカルロ法の方策反復法\n",
    "- モチベーション：行動の選択は、探索と適用のバランスを取りたいが、行動価値関数の評価には適用のみを行いたい。\n",
    "- 別のサイトでは、前半を貪欲法（greedy法）、後半の学習利用をε-貪欲法（ε-greedy法）と言う方法も提案されていた。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f15100-1dee-4101-980b-1628921cff04",
   "metadata": {},
   "source": [
    "#### 状態価値関数を使った方策反復法C\n",
    "- モンテカルロ法は、状態価値関数・行動価値関数のいずれかを利用なので\n",
    "- 方策オフのモンテカルロ法の方策反復法には状態価値関数版は無い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3156498-7ad0-4f75-854e-bed2011f5eeb",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc12924-91cd-4b43-b27a-fffea2c93654",
   "metadata": {},
   "source": [
    "##### エージェント実装D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90da7a-a324-46e5-ae1e-f14be31066c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class McOffPolicyAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.2\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 価値推定用の方策\n",
    "        self.b = defaultdict(lambda: random_actions) # データ収集用の方策\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = []\n",
    "\n",
    "    # データ収集用の方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # サンプリングして履歴を追加\n",
    "    def add(self, state, action, reward):\n",
    "        data = (state, action, reward)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # 方策反復（エピソード毎）\n",
    "    def update(self):\n",
    "        G = 0\n",
    "        rho = 1\n",
    "\n",
    "        # 逆から計算していく。\n",
    "        for data in reversed(self.memory):\n",
    "            state, action, reward = data\n",
    "            key = (state, action)\n",
    "\n",
    "            # 方策評価\n",
    "            # ココがインクリメンタルな収益の計算\n",
    "            # rhoが追加されているのがポイント\n",
    "            G = reward + rho * self.gamma * G\n",
    "            \n",
    "            # ココは効率の良い行動価値関数の更新の実装\n",
    "            # 当該状態毎、上・下・左・右のAction毎に価値がある。\n",
    "            self.Q[key] = self.Q[key] + self.alpha * (G - self.Q[key])\n",
    "            \n",
    "            # 重点サンプリングのρ（重み）が計算されている。\n",
    "            rho *= self.pi[state][action] / self.b[state][action]\n",
    "            \n",
    "            # 2つの方策を其々、別々に更新する\n",
    "            self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # greedy法\n",
    "            self.b[state] = greedy_probs(self.Q, state, self.epsilon) # ε-greedy法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b32d21-2da6-4d7b-bcb6-3694bd7849f8",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行D\n",
    "ココの実装はほぼ変更なし（McAgent → McOffPolicyAgentの変更ダケ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d860af-f181-42e0-8c1a-4b2ca3ecc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = McOffPolicyAgent()\n",
    "\n",
    "# 10000エピソード\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境と履歴の初期化\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 履歴を格納し\n",
    "        agent.add(state, action, reward)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            agent.update() # 方策反復（エピソード毎）\n",
    "            break\n",
    "\n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00273ec-a047-4702-a526-3b6f434e5f6f",
   "metadata": {},
   "source": [
    "## TD法（時間的差分学習法）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0459b-eb54-4fad-94ce-ae44cef14095",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c35af-7b62-4e77-97f9-15a0311cf2bc",
   "metadata": {},
   "source": [
    "### TD法の方策反復法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbba340-f2ba-4994-a5a0-5d93b6a0e2d9",
   "metadata": {},
   "source": [
    "#### 状態価値関数を使った方策反復法E\n",
    "[状態価値関数を使った方策反復法A](#状態価値関数を使った方策反復法A)と比較すると良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc30e0-3857-4c49-96f4-e754ec92c926",
   "metadata": {},
   "source": [
    "##### エージェント実装E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7d64b-1652-4011-af80-2f4313217130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TdAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.01\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 方策\n",
    "        self.V = defaultdict(lambda: 0) # 状態価値関数\n",
    "\n",
    "    # 方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # 方策反復（ステップ毎）\n",
    "    def eval(self, state, reward, next_state, done):\n",
    "        \n",
    "        # ゴールの状態価値は0（その前に報酬がある）\n",
    "        # next_Vは、done=trueなら0、done=tureでないならself.V[next_state]\n",
    "        next_V = 0 if done else self.V[next_state]\n",
    "        \n",
    "        # 方策評価\n",
    "        # ココが、= TD（時間差分学習）法の実装\n",
    "        #target = reward + self.gamma * next_V\n",
    "        #self.V[state] += self.alpha * (target - self.V[state])\n",
    "        self.V[state] = self.V[state] + self.alpha * ((reward + self.gamma * next_V) - self.V[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e56b4-5b67-4fe2-b3ee-f575ef931d80",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc0e18-8b52-4c71-ab8f-e372b1fe8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# グリッドワールドのインスタンスを作成\n",
    "env = GridWorld()\n",
    "# エージェントのインスタンスを作成\n",
    "agent = TdAgent()\n",
    "\n",
    "# 1000エピソード\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # 環境の初期化\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 方策反復（ステップ毎）\n",
    "        agent.eval(state, reward, next_state, done)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_v(agent.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3143d87-cf2c-4af5-aa89-bb9ca7c6f3c5",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法(Q学習)F\n",
    "- [行動価値関数を使った方策反復法B](#行動価値関数を使った方策反復法B)と比較すると良いが、かなり実装が違うので解り難い。\n",
    "- 以降の順番は、Q、Q（方策オフ）、SARSA、SARSA（方策オフ）に変更。\n",
    "- 歴史的にQ → SARSAと登場、Qは方策オフが標準、SARSAは方策オンが標準"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade416b-2fd2-4358-8f21-48686af26651",
   "metadata": {},
   "source": [
    "##### エージェント実装F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841e616-384b-49aa-a2a0-7c136a3a2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "\n",
    "    # ε-greedyで行動を選択\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            qs = [self.Q[state, a] for a in range(self.action_size)]\n",
    "            return np.argmax(qs)\n",
    "    \n",
    "    # 方策改善（ステップ毎）\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # next_q_maxを計算\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "\n",
    "        # ココがQ学習\n",
    "        #target = reward + self.gamma * next_q_max\n",
    "        #self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q_max) - self.Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bf817-4e8e-4082-86c2-998f38119547",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993a662-d95e-4b8f-b264-e961323187e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# 1000エピソード\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # 環境の初期化\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # 方策反復（ステップ毎）\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665e8b6-3aa8-467a-b162-aa477fd6685c",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法(方策オフQ学習)G\n",
    "- Qは方策オフが標準で重点サンプリングを必要としない。\n",
    "- 理由は行動価値関数の推定を方策に依存しないgreedyで行うので。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3ac72-a0df-44a7-8298-1b24f443c775",
   "metadata": {},
   "source": [
    "##### エージェント実装G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e3e54-7041-4125-a78a-ab0f4cd1577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        #self.pi = defaultdict(lambda: random_actions) # 価値推定用の方策\n",
    "        self.b = defaultdict(lambda: random_actions) # データ収集用の方策\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "\n",
    "    # データ収集用の方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    # 方策改善（ステップ毎）\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # next_q_maxを計算（greedyなのでpiが不要に）\n",
    "        if done:\n",
    "            next_q_max = 0\n",
    "        else:\n",
    "            next_qs = [self.Q[next_state, a] for a in range(self.action_size)]\n",
    "            next_q_max = max(next_qs)\n",
    "\n",
    "        # ココがQ学習\n",
    "        #target = reward + self.gamma * next_q_max\n",
    "        #self.Q[state, action] += self.alpha * (target - self.Q[state, action])\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q_max) - self.Q[state, action])\n",
    "\n",
    "        # 2つの方策を其々、別々に更新する\n",
    "        #self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # 価値推定用の方策はgreedy\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon) # データ収集用の方策はε-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2f387-4dfc-41fe-a38c-f21fba33d4dc",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1dc5e-73e7-4ba9-9f5f-5de7c3ed7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# 10000エピソード\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # 環境の初期化\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 方策反復（ステップ毎）\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442f960-d218-43d1-b92d-99ac0aeac2d0",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法(SARSA)H\n",
    "SARSAは方策オンが標準"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f77917e-16a8-449d-b3f9-7e3b9224d2dc",
   "metadata": {},
   "source": [
    "##### エージェント実装H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cb524-bcf6-4c10-9b3f-6d94e3aa61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 方策\n",
    "        self.Q = defaultdict(lambda: 0) # 行動価値関数\n",
    "        self.memory = deque(maxlen=2)\n",
    "\n",
    "    # 方策から行動を選択  \n",
    "    def get_action(self, state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # 方策反復（ステップ毎）\n",
    "    def update(self, state, action, reward, done):\n",
    "        \n",
    "        # ステップの履歴を追加\n",
    "        self.memory.append((state, action, reward, done))\n",
    "        \n",
    "        # SARASなので2ステップ必要\n",
    "        if len(self.memory) < 2:\n",
    "            return\n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _ = self.memory[1]\n",
    "        \n",
    "        # next_q\n",
    "        next_q = 0 if done else self.Q[next_state, next_action]\n",
    "        \n",
    "        # ココがSARAS\n",
    "        #target = reward + self.gamma * next_q\n",
    "        #self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((reward + self.gamma * next_q) - self.Q[state, action])\n",
    "        \n",
    "        self.pi[state] = greedy_probs(self.Q, state, self.epsilon) # 直greedyでも良いのでは？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa15a6-3bb8-42ce-aa24-5a5f34f4ace8",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5df82d-47cb-4eb8-b70a-03a60e29a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = SarsaAgent()\n",
    "\n",
    "# 10000エピソード\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境と履歴の初期化\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 方策反復（ステップ毎）\n",
    "        agent.update(state, action, reward, done)\n",
    "\n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            agent.update(next_state, None, None, None) # 方策反復（最後）\n",
    "            break\n",
    "        \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d5695-2733-4c65-9705-2dfdb4aa5d76",
   "metadata": {},
   "source": [
    "#### 行動価値関数を使った方策反復法(方策オフのSARSA)I\n",
    "方策オフのSARSAでは重点サンプリングが必要になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fd848-f202-4b5d-9bda-3c8dbd66c975",
   "metadata": {},
   "source": [
    "##### エージェント実装I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45b638-a371-4fb7-8aaf-9d0a2c28ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaOffPolicyAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.alpha = 0.8\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        random_actions = {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n",
    "        self.pi = defaultdict(lambda: random_actions) # 価値推定用の方策\n",
    "        self.b = defaultdict(lambda: random_actions) # データ収集用の方策\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "        self.memory = deque(maxlen=2)\n",
    "\n",
    "    # データ収集用の方策から行動を選択\n",
    "    def get_action(self, state):\n",
    "        action_probs = self.b[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions, p=probs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    # 方策反復（ステップ毎）\n",
    "    def update(self, state, action, reward, done):\n",
    "        \n",
    "        # ステップの履歴を追加\n",
    "        self.memory.append((state, action, reward, done))\n",
    "        \n",
    "        # SARASなので2ステップ必要\n",
    "        if len(self.memory) < 2:\n",
    "            return\n",
    "        state, action, reward, done = self.memory[0]\n",
    "        next_state, next_action, _, _ = self.memory[1]\n",
    "\n",
    "        # next_q, rho\n",
    "        if done:\n",
    "            next_q = 0\n",
    "            rho = 1\n",
    "        else:\n",
    "            # next_q\n",
    "            next_q = self.Q[next_state, next_action]\n",
    "            # 重点サンプリングのρ（重み）が計算されている。\n",
    "            rho = self.pi[next_state][next_action] / self.b[next_state][next_action]\n",
    "        \n",
    "        # ココがSARAS（方策オフ）\n",
    "        #target = rho * (reward + self.gamma * next_q)\n",
    "        #self.Q[state, action] += (target - self.Q[state, action]) * self.alpha\n",
    "        self.Q[state, action] = self.Q[state, action] + self.alpha * ((rho * (reward + self.gamma * next_q)) - self.Q[state, action])\n",
    "        \n",
    "        # 2つの方策を其々、別々に更新する\n",
    "        self.pi[state] = greedy_probs(self.Q, state, epsilon=0) # 価値推定用の方策はgreedy\n",
    "        self.b[state] = greedy_probs(self.Q, state, self.epsilon) # データ収集用の方策はε-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425dc12-e2e4-4ff2-bdc4-64f457233040",
   "metadata": {},
   "source": [
    "##### グリッド・ワールド上でエージェントを実行I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa76f-40b1-40f2-be83-0c84f6e5079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = SarsaOffPolicyAgent()\n",
    "\n",
    "# 10000エピソード\n",
    "episodes = 10000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境と履歴の初期化\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while True:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # 方策反復（ステップ毎）\n",
    "        agent.update(state, action, reward, done)\n",
    "\n",
    "        # done=tureまで、\n",
    "        if done:\n",
    "            agent.update(next_state, None, None, None) # 方策反復（最後）\n",
    "            break\n",
    "            \n",
    "        # 次の状態\n",
    "        state = next_state\n",
    "\n",
    "env.render_q(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffe13b-688c-475f-a452-a22f9da83ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
