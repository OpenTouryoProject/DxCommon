{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87b4d10-5704-4796-8d16-5797523647c9",
   "metadata": {},
   "source": [
    "# o'reillyのネゴザメ言語モデルの本\n",
    "RNN Search（Attention）\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "- [RNN Search（Attention）](#RNN_Search（Attention）)\n",
    "  - [Attention](#Attention)\n",
    "    - [Weight Sumレイヤ](#Weight_Sumレイヤ)\n",
    "    - [Attention Weightレイヤ](#Attention_Weightレイヤ)\n",
    "    - [Attentionレイヤ](#Attentionレイヤ)\n",
    "    - [Time Attentionレイヤ](#Time_Attentionレイヤ)\n",
    "  - [RNN Search（Attention）](#RNN_Search（Attention）)\n",
    "    - [Encoder](#Encoder)\n",
    "    - [Decoder](#Decoder)\n",
    "    - [Attention付きseq2seq](#Attention付きseq2seq)\n",
    "    \n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-2/tree/master/ch08\n",
    "- [RNN Encoder-Decoder（Sequence-to-Sequence） - 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?RNN%20Encoder-Decoder%EF%BC%88Sequence-to-Sequence%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f149dd-6504-4850-99b9-626887faba7d",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d975a-b9ce-4eff-a3f5-ae65d56655a9",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89147361-440c-47a1-b02b-942f39590bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377a319-9b97-4c21-9d8f-8664c955b33e",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814784dc-0f79-448a-9608-e9952549ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b638958-9d39-4dd1-9d46-f2f97d5849fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f3ff5d-d8f9-4f26-b9d4-da73bcbdd70e",
   "metadata": {},
   "source": [
    "## RNN Search（Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4650a-e1b4-4da5-b2af-68fddd0a6b63",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcbfb0-9e91-46e7-b19b-9ff83a7b20f5",
   "metadata": {},
   "source": [
    "#### Weight_Sumレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cea648-d503-4f64-8e82-4d4a721bc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとパラメータの形状に関する値を指定\n",
    "N = 3 # バッチサイズ(入力する文章数)\n",
    "T = 4 # Encoderの時系列サイズ(入力する単語数)\n",
    "H = 5 # 隠れ状態のサイズ(LSTMレイヤの中間層のニューロン数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26207079-00ef-4304-a097-edfbd26ad250",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3db49-bad8-4b51-980e-f3f8aad888cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WeightSum:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self):\n",
    "        # 他のレイヤと対応させるための空のリストを作成\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        \n",
    "        # 中間変数の受け皿を初期化\n",
    "        self.cache = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, hs, a):\n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        # Encoderの隠れ状態と同じ形状に複製\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        \n",
    "        # コンテキスト(重み付き和)を計算\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        # 逆伝播の計算用に変数を保存\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dc):\n",
    "        # 変数を取得\n",
    "        hs, ar = self.cache\n",
    "        \n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        # Sumノードの逆伝播を計算\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        \n",
    "        # 乗算ノードの逆伝播を計算\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar # Encoderの隠れ状態の勾配\n",
    "        \n",
    "        # Repeatノードの逆伝播を計算\n",
    "        da = np.sum(dar, axis=2) # Attentionの重みの勾配\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe8480-17cf-4ec3-8fd9-04ec2927658b",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f49be1-c866-4ffa-9841-6e33841ed485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Sumレイヤのインスタンスを作成\n",
    "weightsum_layer = _WeightSum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e16670-1df0-4c54-af16-960422603b8c",
   "metadata": {},
   "source": [
    "##### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd257bcb-2832-4eac-b94f-cd000658b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)Encoderの隠れ状態を作成\n",
    "hs = np.random.randn(N, T, H)\n",
    "print(np.round(hs, 2))\n",
    "print(hs.shape)\n",
    "\n",
    "# (簡易的に)Attentionの重みを作成\n",
    "a = np.random.rand(N, T)\n",
    "a /= np.sum(a, axis=1, keepdims=True) # 正規化\n",
    "print(np.round(a, 2))\n",
    "print(np.sum(a, axis=1))\n",
    "print(a.shape)\n",
    "\n",
    "# 順伝播を計算\n",
    "c = weightsum_layer.forward(hs, a)\n",
    "print(np.round(c, 2))\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0cdb45-ed3a-4635-92ea-c9d0b6ee9817",
   "metadata": {},
   "source": [
    "##### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b581a59-a69e-4c38-884e-6ecf4b88cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)コンテキストの勾配を作成\n",
    "dc = np.ones((N, H))\n",
    "print(dc.shape)\n",
    "\n",
    "# 逆伝播を計算\n",
    "dhs, da = weightsum_layer.backward(dc)\n",
    "print(np.round(dhs, 2))\n",
    "print(dhs.shape)\n",
    "print(np.round(da, 2))\n",
    "print(da.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb53ec7-2fd9-440d-a8be-ffa4f98e3744",
   "metadata": {},
   "source": [
    "#### Attention_Weightレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e885e1c-5b84-4661-a463-01204e413425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装済みクラスを読み込み\n",
    "from nekozame.common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80020a35-2ad1-446d-82a9-7690d1b5298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとパラメータの形状に関する値を指定\n",
    "N = 3 # バッチサイズ(入力する文章数)\n",
    "T = 4 # Encoderの時系列サイズ(入力する単語数)\n",
    "H = 5 # 隠れ状態のサイズ(LSTMレイヤの中間層のニューロン数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf607ccc-30f6-410e-8eb0-bc474e3fe374",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33c9f4-60c7-453a-9270-d9e9df3fef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Weightレイヤの実装\n",
    "class _AttentionWeight:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self):\n",
    "        # 他のレイヤと対応させるための空のリストを作成\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        \n",
    "        # Softmaxレイヤのインスタンスを作成\n",
    "        self.softmax = Softmax()\n",
    "        \n",
    "        # 中間変数の受け皿を初期化\n",
    "        self.cache = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, hs, h):\n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        # Encoderの隠れ状態同じ形状に複製\n",
    "        hr = h.reshape((N, 1, H)).repeat(T, axis=1)\n",
    "        \n",
    "        # スコア(内積)を計算\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        \n",
    "        # Attentionの重みに変換(正規化)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        # 逆伝播の計算用に変数を保存\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, da):\n",
    "        # 変数を取得\n",
    "        hs, hr = self.cache\n",
    "        \n",
    "        # 形状に関する値を取得\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        # Softmaxレイヤの逆伝播(スコアの勾配)を計算\n",
    "        ds = self.softmax.backward(da)\n",
    "        \n",
    "        # Sumノードの逆伝播を計算\n",
    "        dt = ds.reshape((N, T, 1)).repeat(H, axis=2)\n",
    "        \n",
    "        # 乗算ノードの逆伝播を計算\n",
    "        dhs = dt * hr # EncoderのT個の隠れ状態の勾配\n",
    "        dhr = dt * hs\n",
    "        \n",
    "        # Repeatノードの逆伝播を計算\n",
    "        dh = np.sum(dhr, axis=1) # Decoderのt番目の隠れ状態の勾配\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88415b-3287-46b4-9976-9a242bbb514e",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c1e69-e1f3-4c9b-818e-3aa73538e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンスを作成\n",
    "attention_weight_layer = _AttentionWeight()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a135b7-f853-4890-8a69-501090141452",
   "metadata": {},
   "source": [
    "##### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4727662-adc8-4e40-87e4-46019533b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)EncoderのT個の隠れ状態を作成\n",
    "hs = np.random.randn(N, T, H)\n",
    "print(hs.shape)\n",
    "\n",
    "# (簡易的に)Decoderの隠れ状態を作成\n",
    "h = np.random.randn(N, H)\n",
    "print(h.shape)\n",
    "\n",
    "# 順伝播を計算\n",
    "a = attention_weight_layer.forward(hs, h)\n",
    "print(np.round(a, 2))\n",
    "print(np.sum(a, axis=1))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774dc3fb-beda-4848-84e0-45d6bcd575ce",
   "metadata": {},
   "source": [
    "##### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc70215-724e-4d9e-8931-62eee635e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)逆伝播の入力を作成\n",
    "da = np.random.randn(N, T)\n",
    "print(da.shape)\n",
    "\n",
    "# 逆伝播を計算\n",
    "dhs, dh = attention_weight_layer.backward(da)\n",
    "print(np.round(dhs, 3))\n",
    "print(dhs.shape)\n",
    "print(np.round(dh, 3))\n",
    "print(dh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d2cc4-4221-48aa-94a1-5bdf7decbb45",
   "metadata": {},
   "source": [
    "#### Attentionレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bd986-496c-4b39-a97c-c1e5a393f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとパラメータの形状に関する値を指定\n",
    "N = 3 # バッチサイズ(入力する文章数)\n",
    "T = 4 # Encoderの時系列サイズ(入力する単語数)\n",
    "H = 5 # 隠れ状態のサイズ(LSTMレイヤの中間層のニューロン数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412eab1-44b0-4747-ab4f-6203580e0a26",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab76de-16da-4b67-b870-cdf12e79dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionレイヤの実装\n",
    "class _Attention:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self):\n",
    "        # 他のレイヤと対応させるための空のリストを作成\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        \n",
    "        # レイヤのインスタンスを作成\n",
    "        self.attention_weight_layer = _AttentionWeight()\n",
    "        self.weight_sum_layer = _WeightSum()\n",
    "        \n",
    "        # Attentionの重みを初期化\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, hs, h):\n",
    "        # Attentionの重みを計算\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        \n",
    "        # EncoderのT個の隠れ状態の重み付け和を計算\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        \n",
    "        # Attentionの重みを保存\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dout):\n",
    "        # 各レイヤの逆伝播を計算\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        \n",
    "        # EncoderのT個の隠れ状態の勾配を計算\n",
    "        dhs = dhs0 + dhs1\n",
    "        \n",
    "        # EncoderのT個の隠れ状態の勾配とDecoderのt番目の隠れ状態の勾配を出力\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab7d6b-b927-4be0-be85-a820ce182388",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949e480-2ed9-4e2e-994e-394eea507ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンスを作成\n",
    "attention_layer = _Attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313760ad-22ee-4ee4-9c2c-6d0bb1b279f8",
   "metadata": {},
   "source": [
    "##### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883bba4-9f4f-459e-9a4a-fc2a936c49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)EncoderのT個の隠れ状態を作成\n",
    "hs = np.random.randn(N, T, H)\n",
    "print(np.round(hs, 2))\n",
    "print(hs.shape)\n",
    "\n",
    "# (簡易的に)Decoderのt番目の隠れ状態を作成\n",
    "h = np.random.randn(N, H)\n",
    "print(np.round(h, 2))\n",
    "print(h.shape)\n",
    "\n",
    "# 順伝播を計算\n",
    "c = attention_layer.forward(hs, h)\n",
    "print(np.round(c, 2))\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad6c00-d22c-47ab-961d-402825fe5487",
   "metadata": {},
   "source": [
    "##### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8e869-e7e4-48d8-b873-d36550001e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)逆伝播の入力を作成\n",
    "dc = np.ones((N, H))\n",
    "print(dc.shape)\n",
    "\n",
    "# 逆伝播を計算\n",
    "dhs, dh = attention_layer.backward(dc)\n",
    "print(np.round(dhs, 2))\n",
    "print(dhs.shape)\n",
    "print(np.round(dh, 2))\n",
    "print(dh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089aad7-3432-474a-be8b-15eb33d23b77",
   "metadata": {},
   "source": [
    "#### Time_Attentionレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01135ca5-bac3-4f7d-81ab-e977cf0db5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数の形状に関する値を指定\n",
    "N = 3 # バッチサイズ(入力する文章数)\n",
    "T_enc = 4 # Encoderの時系列サイズ(入力する単語数)\n",
    "T_dec = 7 # Decoderの時系列サイズ(入力する単語数)\n",
    "H = 5 # 隠れ状態のサイズ(LSTMレイヤの中間層のニューロン数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905313bc-3481-4b9a-a138-54f7ed3d6ea7",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab2e42-6b4c-46ea-9c50-7c024c222580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Attentionレイヤの実装\n",
    "class _TimeAttention:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self):\n",
    "        # 他のレイヤに対応するための空のリストを作成\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        \n",
    "        # Attentionの重みの受け皿を初期化\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H = hs_dec.shape\n",
    "        \n",
    "        # T個のコンテキストの受け皿を初期化\n",
    "        out = np.empty_like(hs_dec)\n",
    "        \n",
    "        # 受け皿を初期化\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        # Attentionレイヤごとに処理\n",
    "        for t in range(T):\n",
    "            # t番目のAttentionレイヤを作成\n",
    "            layer = _Attention()\n",
    "            \n",
    "            # t番目のコンテキスト(隠れ状態の重み付き和)を計算\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            \n",
    "            # t番目のレイヤとAttentionの重みを格納\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dout):\n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H = dout.shape\n",
    "        \n",
    "        # EncoderとDecoderの隠れ状態の勾配を初期化\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        # Attentionレイヤごとに処理\n",
    "        for t in range(T):\n",
    "            # t番目のAttentionレイヤを取得\n",
    "            layer = self.layers[t]\n",
    "            \n",
    "            # EncoderとDecoderの隠れ状態の勾配を計算\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            \n",
    "            # EncoderのT個の隠れ状態の勾配を加算\n",
    "            dhs_enc += dhs\n",
    "            \n",
    "            # Decoderのt番目の隠れ状態の勾配を格納\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a8fe69-30ef-4ef9-9640-f85a985a4ef7",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae310eba-58a1-4823-93ea-8a81a61b9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンスを作成\n",
    "time_attention_layer = _TimeAttention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3dc77-e087-498a-812b-e681d12079e4",
   "metadata": {},
   "source": [
    "###### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ea7ca-aed9-412b-aee9-f6fa02100f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)EncoderのT個の隠れ状態を作成\n",
    "hs_enc = np.random.randn(N, T_enc, H)\n",
    "print(hs_enc.shape)\n",
    "\n",
    "# (簡易的に)Decoderの隠れ状態を作成\n",
    "hs_enc = np.random.randn(N, T_dec, H)\n",
    "print(hs_enc.shape)\n",
    "\n",
    "# 順伝播\n",
    "cs = time_attention_layer.forward(hs_enc, hs_enc)\n",
    "print(np.round(cs, 2))\n",
    "print(cs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97823e09-6708-4c4d-b253-d150148a06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionの重みを取得\n",
    "attention_weights = time_attention_layer.attention_weights\n",
    "print(np.round(attention_weights[0], 2)) # 0番目のAttentionの重み\n",
    "print(np.sum(attention_weights[0], axis=1))\n",
    "print(np.array(attention_weights[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73edd7e0-d54e-4c55-a4f5-31a71f62f6ba",
   "metadata": {},
   "source": [
    "###### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a448e9-065f-4217-8604-4fd037426fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)逆伝播の入力を作成\n",
    "dcs = np.random.randn(N, T_dec, H)\n",
    "print(dcs.shape)\n",
    "\n",
    "# 逆伝播を計算\n",
    "dhs_enc, dhs_dec = time_attention_layer.backward(dcs)\n",
    "print(dhs_enc.shape)\n",
    "print(dhs_dec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f5c5a-7696-414b-8903-5ac08cd9409e",
   "metadata": {},
   "source": [
    "### RNN_Search（Attention）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a66cf2-dd71-4dd6-8c56-b55f417c357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとパラメータの形状に関する値を指定\n",
    "N = 3 # バッチサイズ(入力する文章数)\n",
    "V = 10 # 単語の種類数\n",
    "D = 5 # 単語ベクトルの次元数(Embedレイヤの中間層のニューロン数)\n",
    "H = 7 # 隠れ状態のサイズ(LSTMレイヤの中間層のニューロン数)\n",
    "T_enc = 6 # Encoderの時系列サイズ(入力する単語数)\n",
    "T_dec = 4 # Decoderの時系列サイズ(入力・予測する単語数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc9c392-fe1c-48e1-9012-e9d785dd7299",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9a118-79c6-4c2e-8c9c-4823617c2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装済みのレイヤを読み込み\n",
    "from nekozame.common.time_layers import TimeEmbedding\n",
    "from nekozame.common.time_layers import TimeLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0be91-3921-4e8a-bcb3-df64ef2b5d74",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78869b-ca2c-4d30-bcbe-3e696a4cc031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention用のEncoderの実装\n",
    "class _AttentionEncoder:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # パラメータを初期化\n",
    "        embed_W = (np.random.randn(V, D) * 0.01).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        # レイヤのインスタンスを作成\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = self.embed.params + self.lstm.params # パラメータ\n",
    "        self.grads = self.embed.grads + self.lstm.grads    # 勾配\n",
    "        \n",
    "        # LSTMレイヤの中間変数を初期化\n",
    "        self.hs = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs):\n",
    "        # 各レイヤの順伝播を計算\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dhs):\n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceef816-6707-4bfc-a7e7-a88c89df2f0f",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8118c4-394b-4315-86c0-b6f46181ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoderのインスタンスを作成\n",
    "encoder = _AttentionEncoder(V, D, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d768e-f3b6-4c55-84dd-ab3870f9ea7b",
   "metadata": {},
   "source": [
    "###### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bf881-b068-4756-b489-2b203bf62ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)Encoderの入力データを作成\n",
    "xs  = np.random.randint(low=0, high=V, size=(N, T_enc))\n",
    "print(xs)\n",
    "print(xs.shape)\n",
    "\n",
    "# Encoderの隠れ状態を計算\n",
    "hs_enc = encoder.forward(xs)\n",
    "print(np.round(hs_enc, 3))\n",
    "print(hs_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89984f-f97a-4578-ae74-38302585a648",
   "metadata": {},
   "source": [
    "###### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a097c9-5ab0-4299-83dc-9d14c3f13bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)Encoderの隠れ状態の勾配を作成\n",
    "dhs = np.random.randn(N, T_enc, H)\n",
    "print(dhs.shape)\n",
    "\n",
    "# 逆伝播を計算\n",
    "dout = encoder.backward(dhs)\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687aef85-3dd8-43f1-9e1d-a503047d5e3d",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8f8d1-1d72-462f-a089-83b3a5b5aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装済みのレイヤを読み込み\n",
    "from nekozame.common.time_layers import TimeAffine\n",
    "from nekozame.common.time_layers import TimeEmbedding\n",
    "from nekozame.common.time_layers import TimeLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c30bdc-b3d5-428d-bba4-3e77433405ab",
   "metadata": {},
   "source": [
    "##### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab775-4cdc-4561-aa50-902ae96c3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention付きDecoderの実装\n",
    "class _AttentionDecoder:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # パラメータを初期化\n",
    "        embed_W = (np.random.randn(V, D) * 0.01).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (np.random.randn(2 * H, V) / np.sqrt(2 * H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤのインスタンスを作成\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = _TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        # レイヤをリストに格納\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs, enc_hs):\n",
    "        # EncoderのT-1番目の隠れ状態を0番目のLSTMレイヤに入力\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 各レイヤの順伝播を計算\n",
    "        out = self.embed.forward(xs)               # 単語ベクトル\n",
    "        dec_hs = self.lstm.forward(out)            # 隠れ状態\n",
    "        c = self.attention.forward(enc_hs, dec_hs) # コンテキスト\n",
    "        out = np.concatenate((c, dec_hs), axis=2)  # コンテキストと隠れ状態を結合\n",
    "        score = self.affine.forward(out)           # スコア\n",
    "        return score\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dscore):\n",
    "        # Time Affineレイヤの逆伝播を計算\n",
    "        dout = self.affine.backward(dscore)\n",
    "        \n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "        \n",
    "        # コンテキストの勾配と隠れ状態の勾配に分割\n",
    "        dc, ddec_hs0 = dout[:, :, :H], dout[:, :, H:]\n",
    "        \n",
    "        # Time Attentionレイヤの逆伝播を計算\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        \n",
    "        # 分岐したDecoderの隠れ状態を合算\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        \n",
    "        # Time LSTMレイヤの逆伝播を計算\n",
    "        dout = self.lstm.backward(ddec_hs) # 単語ベクトルの勾配\n",
    "        \n",
    "        # 分岐したEncoderのT-1番目の隠れ状態の勾配を合算\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        \n",
    "        # Time Embedレイヤの逆伝播を計算\n",
    "        self.embed.backward(dout) # 出力はNone\n",
    "        \n",
    "        return denc_hs\n",
    "    \n",
    "    # 文章生成メソッド\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        # 文字IDの受け皿を初期化\n",
    "        sampled = []\n",
    "        \n",
    "        # 区切り文字のIDを設定\n",
    "        sample_id = start_id\n",
    "        \n",
    "        # Encoderの最後の隠れ状態をDecoderの最初のLSTMレイヤに入力\n",
    "        h = enc_hs[:, -1, :]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 文章を生成\n",
    "        for _ in range(sample_size):\n",
    "            # 入力用に2次元配列に変換\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            \n",
    "            # スコアを計算\n",
    "            out = self.embed.forward(x)                # 単語ベクトル\n",
    "            dec_hs = self.lstm.forward(out)            # 隠れ状態\n",
    "            c = self.attention.forward(enc_hs, dec_hs) # コンテキスト\n",
    "            out = np.concatenate((c, dec_hs), axis=2)  # コンテキストと隠れ状態を結合\n",
    "            score = self.affine.forward(out)           # スコア\n",
    "            \n",
    "            # スコアが最大の単語IDを取得\n",
    "            sample_id = np.argmax(score.flatten()) # 入力単語を更新\n",
    "            sampled.append(int(sample_id)) # サンプリングした単語を保存\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20d6ba-1292-48a7-809e-02b373800433",
   "metadata": {},
   "source": [
    "##### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8f912-95b1-4424-a130-cb4a746e7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoderのインスタンスを作成\n",
    "decoder = _AttentionDecoder(V, D, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b4e85-9815-4bce-bd10-d2002a2b9d29",
   "metadata": {},
   "source": [
    "###### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6048d-0504-49e5-b74e-8562a16bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)Decoderの入力データを作成\n",
    "xs = np.random.randint(low=0, high=V, size=(N, T_dec))\n",
    "print(xs)\n",
    "print(xs.shape)\n",
    "\n",
    "# (簡易的に)Encoderの隠れ状態を作成\n",
    "hs_enc = np.random.randn(N, T_enc, H)\n",
    "print(hs_enc.shape)\n",
    "\n",
    "# スコアを計算\n",
    "score = decoder.forward(xs, hs_enc)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bc86d-8ba5-485f-a7cb-5292e1849d22",
   "metadata": {},
   "source": [
    "###### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f547106-242a-4dbe-92ad-d3a6371d653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)スコアの勾配を作成\n",
    "dscore = np.random.randn(N, T_dec, V)\n",
    "print(dscore.shape)\n",
    "\n",
    "# Encoderの隠れ状態の勾配を計算\n",
    "dhs_enc = decoder.backward(dscore)\n",
    "print(dhs_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79b448-31ca-4c56-bd4e-5c88c2ad4679",
   "metadata": {},
   "source": [
    "### Attention付きseq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7150c7-4d6d-405f-ad86-36bd27f8ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装済みのレイヤを読み込み\n",
    "from nekozame.common.time_layers import TimeSoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66f986-f914-4d44-afe1-cf7a921d157f",
   "metadata": {},
   "source": [
    "#### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a1a9c-c936-42af-9f87-c0cae2fd92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention付きseq2seqの実装\n",
    "class _AttentionSeq2seq:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # 各レイヤのインスタンスを作成\n",
    "        self.encoder = _AttentionEncoder(V, D, H)\n",
    "        self.decoder = _AttentionDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = self.encoder.params + self.decoder.params # パラメータ\n",
    "        self.grads = self.encoder.grads + self.decoder.grads    # 勾配\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs, ts):\n",
    "        # Decoder用の入植データを作成\n",
    "        decoder_xs = ts[:, :-1] # 入力データ:(最後を除く)\n",
    "        decoder_ts = ts[:, 1:]  # 教師データ:(最初を除く)\n",
    "        \n",
    "        # 各レイヤの順伝播を計算\n",
    "        hs = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, hs)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dout=1):\n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        dout = self.softmax.backward(dout) # スコアの勾配\n",
    "        dhs = self.decoder.backward(dout) # Encoderの隠れ状態の勾配\n",
    "        dout = self.encoder.backward(dhs) # 出力はNone\n",
    "        return dout\n",
    "    \n",
    "    # 文章生成メソッド\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        # 問題文をエンコード\n",
    "        hs = self.encoder.forward(xs)\n",
    "        \n",
    "        # 解答を生成\n",
    "        sampled = self.decoder.generate(hs, start_id, sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28d519-d9f7-4677-b43f-ec2c055643c8",
   "metadata": {},
   "source": [
    "#### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a5a6c-9220-4208-a236-72dd8766c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seqのインスタンスを作成\n",
    "model = _AttentionSeq2seq(V, D, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a9749-4afd-40ed-bfa0-9aed48a55846",
   "metadata": {},
   "source": [
    "##### 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc342de-8cc8-42c2-91d9-aff3b831e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (簡易的に)入力データを作成\n",
    "xs = np.random.randint(low=0, high=V, size=(N, T_enc))\n",
    "print(xs)\n",
    "print(xs.shape)\n",
    "\n",
    "# (簡易的に)教師データを作成\n",
    "ts = np.random.randint(low=0, high=V, size=(N, T_dec + 1))\n",
    "print(ts)\n",
    "print(ts.shape)\n",
    "\n",
    "# 順伝播を計算\n",
    "loss = model.forward(xs, ts)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec625282-ff1c-4a29-ad32-6b4bd2c3660a",
   "metadata": {},
   "source": [
    "##### 逆伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337ff0b-1849-406c-95f3-61432b6d8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逆伝播を計算\n",
    "dout = model.backward(dout=1)\n",
    "print(dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20921f-617d-4e8f-975b-b9a0d71f3e5b",
   "metadata": {},
   "source": [
    "#### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed30aab-6c93-40e8-b06c-ec2863df912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実装済みのクラスを読み込み\n",
    "from nekozame.common.optimizer import Adam\n",
    "from nekozame.common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18097249-6280-423c-a59c-ac8965c03fa3",
   "metadata": {},
   "source": [
    "##### データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19790ce9-4c5f-4593-9908-30b4e3b5e3e9",
   "metadata": {},
   "source": [
    "###### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9277758-893b-4f5f-94c6-159e4262ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセット読み込み用のモジュール\n",
    "from nekozame.dataset import sequence\n",
    "\n",
    "#　データセットの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66016c4b-0438-42b9-a00d-7d92f98ebf16",
   "metadata": {},
   "source": [
    "###### ディクショナリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646bac8f-7d79-4255-b8aa-3e2015a7640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字と文字IDの変換用ディクショナリ変数の読み込み\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "print(len(char_to_id))\n",
    "\n",
    "# 文字と文字IDの変換用ディクショナリ変数を確認\n",
    "print(char_to_id)\n",
    "print(id_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875f967-a809-40f1-b235-e9d9a3253acc",
   "metadata": {},
   "source": [
    "###### レコードの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b89f0a-5a1e-4265-942e-43d68167bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表示するデータ番号を指定\n",
    "n = 0\n",
    "\n",
    "# 文字IDのリストを表示\n",
    "print(x_train[n])\n",
    "print(t_train[n])\n",
    "\n",
    "# テキストに変換して表示\n",
    "print(''.join([id_to_char[c_id] for c_id in x_train[n]]))\n",
    "print(''.join([id_to_char[c_id] for c_id in t_train[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff909d4-1e30-4c0e-82c5-60424d53d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表示するデータ番号を指定\n",
    "n = 5\n",
    "\n",
    "# 文字IDのリストを表示\n",
    "print(x_train[n])\n",
    "print(t_train[n])\n",
    "\n",
    "# テキストに変換して表示\n",
    "print(''.join([id_to_char[c_id] for c_id in x_train[n]]))\n",
    "print(''.join([id_to_char[c_id] for c_id in t_train[n]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4e263-0fc7-486c-a205-9f818ecc05c1",
   "metadata": {},
   "source": [
    "###### 入力文を反転"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7648f-aa35-454c-9093-a2a6d2cd5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_x_train = x_train[:, ::-1]\n",
    "reverse_x_test = x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140342c-01e4-4e8c-9791-ebf779f75899",
   "metadata": {},
   "source": [
    "##### 比較対象の再定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5d5a9-9725-4864-86b5-46cc8c7e986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.time_layers import TimeEmbedding\n",
    "from nekozame.common.time_layers import TimeLSTM\n",
    "from nekozame.common.time_layers import TimeAffine\n",
    "from nekozame.common.time_layers import TimeSoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36fef0-1dfb-4496-91fb-377f95aae493",
   "metadata": {},
   "source": [
    "###### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53c891-bfda-4f7e-95ca-419f5940d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンコーダーの実装\n",
    "class Encoder:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # パラメータを初期化\n",
    "        embed_W = (np.random.randn(V, D) * 0.01).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        # レイヤのインスタンスを作成\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = self.embed.params + self.lstm.params # パラメータ\n",
    "        self.grads = self.embed.grads + self.lstm.grads    # 勾配\n",
    "        \n",
    "        # LSTMレイヤの中間変数を初期化\n",
    "        self.hs = None\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs):\n",
    "        # 各レイヤの順伝播を計算\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        \n",
    "        # 逆伝播用に隠れ状態を保存\n",
    "        self.hs = hs\n",
    "        \n",
    "        # T-1番目の隠れ状態をDecoderに出力\n",
    "        return hs[:, -1, :]\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dh):\n",
    "        # 隠れ状態を初期化\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh # Decoderから入力\n",
    "        \n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b277871-9c16-4cc1-8a9c-05c1d99e88f3",
   "metadata": {},
   "source": [
    "###### Dncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125a063-7702-4d70-a10a-9470b4784b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デコーダーの定義\n",
    "class Decoder:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # パラメータを初期化\n",
    "        embed_W = (np.random.randn(V, D) * 0.01).astype('f')\n",
    "        lstm_Wx = (np.random.randn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (np.random.randn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤを生成\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs, h):\n",
    "        # Encoderの隠れ状態を入力\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 各レイヤの順伝播を計算\n",
    "        # Time Embedレイヤの順伝播を計算\n",
    "        out = self.embed.forward(xs)\n",
    "        # Time LSTMレイヤの順伝播を計算\n",
    "        out = self.lstm.forward(out)\n",
    "        # Time Affineレイヤの順伝播を計算\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dscore):\n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        # Time Affineレイヤの逆伝播を計算\n",
    "        dout = self.affine.backward(dscore)\n",
    "        # Time LSTMレイヤの逆伝播を計算\n",
    "        dout = self.lstm.backward(dout)\n",
    "        # Time Embedレイヤの逆伝播を計算\n",
    "        dout = self.embed.backward(dout)\n",
    "        \n",
    "        # EncoderのT-1番目の隠れ状態の勾配をEncoderに出力\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "    \n",
    "    # 文章生成メソッド\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        # 文字IDの受け皿を初期化\n",
    "        sampled = []\n",
    "        \n",
    "        # 区切り文字のIDを設定\n",
    "        sample_id = start_id\n",
    "        \n",
    "        # エンコードされた足し算の式を入力\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 解答を生成\n",
    "        for _ in range(sample_size):\n",
    "            # 入力用に2次元配列に変換\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            \n",
    "            # スコアを計算\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            # スコアが最大の文字IDを取得\n",
    "            sample_id = np.argmax(score.flatten()) # 入力データを更新\n",
    "            sampled.append(sample_id) # サンプルを保存\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9d8e0-f6ba-4811-b871-e7eee14f08d8",
   "metadata": {},
   "source": [
    "###### PeekyDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7162e9a-30cf-4088-8c5c-ba79cd5f3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peeky版デコーダーの定義\n",
    "class PeekyDecoder:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # パラメータを初期化\n",
    "        embed_W = (np.random.randn(V, D) * 0.01).astype('f')\n",
    "        #lstm_Wx = (np.random.randn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wx = (np.random.randn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (np.random.randn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        #affine_W = (np.random.randn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_W = (np.random.randn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤを生成\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = [] # パラメータ\n",
    "        self.grads = []  # 勾配\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # 中間変数の受け皿\n",
    "        self.cache = None\n",
    "        # -------------------\n",
    "        \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs, h):\n",
    "        # --- 追加 ----------\n",
    "        # 変数の形状に関する値を取得\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "        # -------------------\n",
    "        \n",
    "        # Encoderの隠れ状態を入力\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 各レイヤの順伝播を計算\n",
    "        # Time Embedレイヤの順伝播を計算\n",
    "        out = self.embed.forward(xs)\n",
    "        # --- 追加 ----------\n",
    "        # Encoderの隠れ状態を複製\n",
    "        hs = np.repeat(h, T, axis=0).reshape((N, T, H))\n",
    "        # Encoderの隠れ状態(複製)と単語ベクトルを結合\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        # -------------------\n",
    "        # Time LSTMレイヤの順伝播を計算\n",
    "        out = self.lstm.forward(out)\n",
    "        # --- 追加 ----------\n",
    "        # Encoderの隠れ状態(複製)とDecoderの隠れ状態を結合\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "        # -------------------\n",
    "        # Time Affineレイヤの順伝播を計算\n",
    "        score = self.affine.forward(out)\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # 逆伝播用に変数の形状に関する値を保存\n",
    "        self.cache = H\n",
    "        # -------------------\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dscore):\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # 変数の形状に関する値を取得\n",
    "        H = self.cache\n",
    "        # -------------------\n",
    "        \n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        # Time Affineレイヤの逆伝播を計算\n",
    "        dout = self.affine.backward(dscore)\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # 勾配を分割\n",
    "        dhs0 = dout[:, :, :H] # Encoderの隠れ状態(複製)の勾配\n",
    "        dout = dout[:, :, H:] # Decoderの隠れ状態の勾配\n",
    "        # -------------------\n",
    "        \n",
    "        # Time LSTMレイヤの逆伝播を計算\n",
    "        dout = self.lstm.backward(dout)\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # 勾配を分割\n",
    "        dhs1 = dout[:, :, :H]   # Encoderの隠れ状態(複製)の勾配\n",
    "        dembed = dout[:, :, H:] # 単語ベクトルの勾配\n",
    "        # -------------------\n",
    "        \n",
    "        # Time Embedレイヤの逆伝播を計算\n",
    "        dout = self.embed.backward(dembed) # 返り値はNone\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        # Encoderの隠れ状態の勾配を計算\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = np.sum(dhs, axis=1)\n",
    "        # -------------------\n",
    "        \n",
    "        # EncoderのT-1番目の隠れ状態の勾配を合算してをEncoderに出力\n",
    "        dh += self.lstm.dh\n",
    "        return dh\n",
    "    \n",
    "    # 文章生成メソッド\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        # 文字IDの受け皿を初期化\n",
    "        sampled = []\n",
    "        \n",
    "        # 区切り文字のIDを設定\n",
    "        sample_id = start_id\n",
    "        \n",
    "        # エンコードされた足し算の式を入力\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        # 解答を生成\n",
    "        \n",
    "        # --- 追加 ----------\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape((1, 1, H))\n",
    "        # -------------------\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            # 入力用に2次元配列に変換\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            \n",
    "            # スコアを計算\n",
    "            out = self.embed.forward(x)\n",
    "            # --- 追加 ----------\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            # -------------------\n",
    "            out = self.lstm.forward(out)\n",
    "            # --- 追加 ----------\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            # -------------------\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            # スコアが最大の文字IDを取得\n",
    "            sample_id = np.argmax(score.flatten()) # 入力データを更新\n",
    "            sampled.append(sample_id) # サンプルを保存\n",
    "        \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c5f57-bb1f-4a76-8261-26144419bf9e",
   "metadata": {},
   "source": [
    "###### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15972fdc-916b-40b1-bce4-7228663d725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seqの実装\n",
    "class Seq2seq:\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # 各レイヤのインスタンスを作成\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = self.encoder.params + self.decoder.params # パラメータ\n",
    "        self.grads = self.encoder.grads + self.decoder.grads    # 勾配\n",
    "    \n",
    "    # 順伝播メソッド\n",
    "    def forward(self, xs, ts):\n",
    "        # Decoder用のデータを作成\n",
    "        decoder_xs = ts[:, :-1] # 入力データ:(最後を除く)\n",
    "        decoder_ts = ts[:, 1:]  # 教師データ:(最初を除く)\n",
    "        \n",
    "        # 各レイヤの順伝播を計算\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "    \n",
    "    # 逆伝播メソッド\n",
    "    def backward(self, dout=1):\n",
    "        # 各レイヤの逆伝播を逆順に計算\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "    \n",
    "    # 文章生成メソッド\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        # 足し算の式をエンコード\n",
    "        h = self.encoder.forward(xs)\n",
    "        \n",
    "        # 解答を生成\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1465ce2-fee0-4dcc-ae4a-41262e7b99d9",
   "metadata": {},
   "source": [
    "###### PeekySeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f7c40-7a2b-4019-ad9d-60f016746228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seqの実装\n",
    "class PeekySeq2seq(Seq2seq):\n",
    "    # 初期化メソッド\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        # 変数の形状に関する値を取得\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # 各レイヤのインスタンスを作成\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        # パラメータと勾配をリストに格納\n",
    "        self.params = self.encoder.params + self.decoder.params # パラメータ\n",
    "        self.grads = self.encoder.grads + self.decoder.grads    # 勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc7aeb-8f92-4207-aa95-7df259259e12",
   "metadata": {},
   "source": [
    "##### 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7d351-a9a5-47e4-8568-06058fcb6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字の種類数(EmbedレイヤとAffineレイヤのニューロン数)を取得\n",
    "vocab_size = len(char_to_id)\n",
    "\n",
    "# 単語ベクトルのサイズ(Embedレイヤのニューロン数)を指定\n",
    "wordvec_size = 16\n",
    "\n",
    "# 隠れ状態のサイズ(LSTMレイヤのニューロン数)を指定\n",
    "hidden_size = 256\n",
    "\n",
    "# バッチサイズを指定\n",
    "batch_size = 128\n",
    "\n",
    "# エポック当たりの試行回数を指定\n",
    "max_epoch = 10\n",
    "\n",
    "# 勾配の閾値を指定\n",
    "max_grad = 5.0\n",
    "\n",
    "# Attention seq2seqのインスタンスを作成\n",
    "attention_model = _AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "attention_optimizer = Adam()\n",
    "attention_trainer = Trainer(attention_model, attention_optimizer)\n",
    "\n",
    "# Peeky seq2seqのインスタンスを作成\n",
    "peeky_model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "peeky_optimizer = Adam()\n",
    "peeky_trainer = Trainer(peeky_model, peeky_optimizer)\n",
    "\n",
    "# Attention seq2seqのインスタンスを作成\n",
    "base_model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "base_optimizer = Adam()\n",
    "base_trainer = Trainer(base_model, base_optimizer)\n",
    "\n",
    "# 正解率の記録用のリストを初期化\n",
    "attention_acc_list = []\n",
    "peeky_acc_list = []\n",
    "base_acc_list = []\n",
    "\n",
    "# 繰り返し試行\n",
    "for epoch in range(max_epoch):\n",
    "    # 学習\n",
    "    print('----- Attention seq2seq -----')\n",
    "    attention_trainer.fit(reverse_x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    print('----- Peeky seq2seq -----')\n",
    "    peeky_trainer.fit(reverse_x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    print('----- seq2seq -----')\n",
    "    base_trainer.fit(reverse_x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    # 正解数を初期化\n",
    "    attention_correct_num = 0\n",
    "    peeky_correct_num = 0\n",
    "    base_correct_num = 0\n",
    "    \n",
    "    # 精度を測定\n",
    "    for n in range(len(reverse_x_test)):\n",
    "        # データを取得\n",
    "        question = reverse_x_test[[n]]  # Encoderの入力データ(足し算の式)\n",
    "        start_id = t_test[n, 0] # Decoderの入力データの最初の文字(区切り文字)\n",
    "        correct = t_test[n, 1:] # 教師データ(足し算の答)\n",
    "        \n",
    "        # 解答を生成\n",
    "        attention_guess = attention_model.generate(question, start_id, len(correct))\n",
    "        peeky_guess = peeky_model.generate(question, start_id, len(correct))\n",
    "        base_guess = base_model.generate(question, start_id, len(correct))\n",
    "        \n",
    "        # 正解数をカウント\n",
    "        if attention_guess == list(correct): # 解答と正答が一致したら\n",
    "            attention_correct_num += 1\n",
    "        if peeky_guess == list(correct): # 解答と正答が一致したら\n",
    "            peeky_correct_num += 1\n",
    "        if base_guess == list(correct): # 解答と正答が一致したら\n",
    "            base_correct_num += 1\n",
    "    \n",
    "    # 正解率を計算\n",
    "    attention_acc = float(attention_correct_num) / len(reverse_x_test)\n",
    "    attention_acc_list.append(attention_acc)\n",
    "    peeky_acc = float(peeky_correct_num) / len(reverse_x_test)\n",
    "    peeky_acc_list.append(peeky_acc)\n",
    "    base_acc = float(base_correct_num) / len(reverse_x_test)\n",
    "    base_acc_list.append(base_acc)\n",
    "    \n",
    "    # 途中経過を表示\n",
    "    print('----- results -----')\n",
    "    print('val attention acc:' + str(attention_acc * 100))\n",
    "    print('val peeky acc:' + str(peeky_acc * 100))\n",
    "    print('val base acc:' + str(base_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187bf00-9085-4e49-9abf-26b741addb3e",
   "metadata": {},
   "source": [
    "###### 学習結果を表示する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daec636-9221-4e4c-a4fa-b2bbdd39ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作図\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(1 + np.arange(len(base_acc_list)), base_acc_list, \n",
    "         marker='v', label='baseline') # seq2seqの結果\n",
    "plt.plot(1 + np.arange(len(peeky_acc_list)), peeky_acc_list, \n",
    "         marker='D', label='peeky') # Peeky seq2seqの結果\n",
    "plt.plot(1 + np.arange(len(attention_acc_list)), attention_acc_list, \n",
    "         marker='o', label='attention') # Attention seq2seqの結果\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('seq2seq', fontsize=20)\n",
    "#plt.ylim(0, 1) # y軸の表示範囲\n",
    "plt.grid() # グリッド線"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c37345-e832-4423-9618-26a6e7a307fd",
   "metadata": {},
   "source": [
    "###### 学習結果をロードする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60501e61-3d43-421c-9e97-ecda1d2992e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存と復元\n",
    "import pickle\n",
    "\n",
    "# 保存されるオブジェクト\n",
    "#obj = [base_acc_list, peeky_acc_list, attention_acc_list]\n",
    "\n",
    "# シリアライズ相当\n",
    "#with open('../work/NekozameRNN4_acc_list.pickle','wb') as f:\n",
    "#   pickle.dump(obj, f)\n",
    "\n",
    "# デシリアライズ相当\n",
    "with open('../work/NekozameRNN4_acc_list.pickle','rb') as f:\n",
    "   loaded_obj = pickle.load(f)\n",
    "\n",
    "base_acc_list, peeky_acc_list, attention_acc_list = loaded_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
