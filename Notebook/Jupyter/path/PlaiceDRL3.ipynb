{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyのツノガレイ強化学習の本\n",
    "\n",
    "## 深層学習で強化学習\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- [DeZero](#DeZero)\n",
    "  - [matmul](#matmul)\n",
    "  - [微分計算](#微分計算)\n",
    "  - [回帰問題](#回帰問題)\n",
    "    - [線形モデルの回帰](#線形モデルの回帰)\n",
    "    - [非線形モデルの回帰](#非線形モデルの回帰)\n",
    "- [DQN未満](#DQN未満)\n",
    "  - [エージェントの実装1](#エージェントの実装1)\n",
    "  - [エージェントの実行1](#エージェントの実行1)\n",
    "    - [DQN未満学習](#DQN未満学習)\n",
    "    - [学習履歴の表示](#学習履歴の表示)\n",
    "    - [行動価値関数の可視化](#行動価値関数の可視化)\n",
    "- [DQN](#DQN)\n",
    "  - [倒立振子問題](#倒立振子問題)\n",
    "  - [経験再生（リプレイ・バッファ）](#経験再生（リプレイ・バッファ）)\n",
    "    - [クラス定義](#クラス定義)\n",
    "    - [クラス試用](#クラス試用)\n",
    "      - [記録](#記録)\n",
    "      - [再生](#再生)\n",
    "  - [DQNの実装](#DQNの実装)\n",
    "    - [エージェントの実装2](#エージェントの実装2)\n",
    "    - [エージェントの実行2](#エージェントの実行2)\n",
    "    - [実行結果の描画](#実行結果の描画)\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-4/tree/master/ch01\n",
    "- [強化学習（Reinforcement Learning） - .NET 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88Reinforcement%20Learning%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39e2e7-67b7-464d-b254-68521ef5a3de",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea6589-87d6-459c-b8b8-55179ed99adf",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1a3d-fc30-4a7f-b180-a221b11e7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install tabulate\n",
    "!pip install matplotlib\n",
    "!pip install dezero\n",
    "!pip install dezerogym\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4d655-73aa-4cef-9def-0de5a4ef83d1",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854310-e2f8-48c9-986c-e05d242744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 廃止となったエイリアスを書く\n",
    "np.object = object\n",
    "np.bool = bool\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.typeDict = {k: v for k, v in np.sctypeDict.items() if isinstance(v, type)}\n",
    "\n",
    "from dezero import Variable\n",
    "from dezero import Model\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "from dezerogym.gridworld import GridWorld\n",
    "import gym # OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a909-b830-4776-890f-b6cef29a456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836fb9c-95b7-4cce-9651-d948fcc6fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共通関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289f2aa-e5a5-4ac3-bb94-d4a785d3d9ef",
   "metadata": {},
   "source": [
    "#### 状態のone_hot化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9750ba-2d70-4fa1-b5fa-baec995a2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(state):\n",
    "    HEIGHT, WIDTH = 3, 4 # 3*4 グリッドワールド\n",
    "    vec = np.zeros(HEIGHT * WIDTH, dtype=np.float32)\n",
    "    y, x = state\n",
    "    idx = WIDTH * y + x\n",
    "    vec[idx] = 1.0\n",
    "    return vec[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a554ad9-7892-4b93-a26f-534e68e85d5e",
   "metadata": {},
   "source": [
    "#### 行動価値関数のDNN版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c090bc6-ac3d-4f57-b281-8a2fddf348fc",
   "metadata": {},
   "source": [
    "##### QNet1\n",
    "入力x → 隠れ層100 → 出力層4（アクション数と同じ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305a2d5-626a-4365-be1a-17b9185fc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet1(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(100)  # hidden_size\n",
    "        self.l2 = L.Linear(4)  # action_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f3e00-6c5a-4a38-864d-d6c4d98b1801",
   "metadata": {},
   "source": [
    "##### QNet2\n",
    "入力x → 隠れ層 128 128 → 出力層4（アクション数と同じ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa561d9-4065-46bc-8340-7aa492e900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet2(Model):\n",
    "    def __init__(self, action_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(128)\n",
    "        self.l2 = L.Linear(128)\n",
    "        self.l3 = L.Linear(action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4715839-9efb-441d-9976-ab08cfd7ecba",
   "metadata": {},
   "source": [
    "#### OpenAI Gym 表示領域定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed56cd-3ad5-43ce-9123-608620af1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_area (state):\n",
    "    # ラベル\n",
    "    state_text = f'cart position={state[0]:5.2f}, '\n",
    "    state_text += f'cart velocity={state[1]:6.3f}\\n'\n",
    "    state_text += f'pole angle   ={state[2]:5.2f}, '\n",
    "    state_text += f'pole velocity={state[3]:6.3f}'\n",
    "\n",
    "    # 領域定義\n",
    "    fig = plt.figure(figsize=(9, 7), facecolor='white')\n",
    "    plt.suptitle('Cart Pole', fontsize=20)\n",
    "    plt.xticks(ticks=[])\n",
    "    plt.yticks(ticks=[])\n",
    "    plt.title(state_text, loc='left')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81287d9b-764a-4346-80ed-3d8883c80495",
   "metadata": {},
   "source": [
    "### DeZero\n",
    "ゼロから作るDeep Learning 3 ―フレームワーク編で開発されたDeep Learningフレームワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f0a713-6968-4f9d-9b0a-bcb0afc27112",
   "metadata": {},
   "source": [
    "#### matmul\n",
    "ベクトル内積、行列ドット積ができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951e3b3-f237-4644-b519-dab448c97a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "a, b = Variable(a), Variable(b)  # Optional\n",
    "c = F.matmul(a, b)\n",
    "print(c)\n",
    "\n",
    "# Matrix product\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6], [7, 8]])\n",
    "c = F.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38fa91-0709-4979-aebe-a7505c705efb",
   "metadata": {},
   "source": [
    "#### 微分計算\n",
    "rosenbrock関数を微分して勾配降下していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fa0e7-ef29-4bba-aa1e-552270a11ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rosenbrock関数の重みはx0, x1\n",
    "def rosenbrock(x0, x1):\n",
    "    y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2\n",
    "    return y\n",
    "\n",
    "x0 = Variable(np.array(0.0))\n",
    "x1 = Variable(np.array(2.0))\n",
    "\n",
    "lr = 0.001\n",
    "iters = 10000\n",
    "\n",
    "for i in range(iters):\n",
    "    \n",
    "    # 順伝播\n",
    "    y = rosenbrock(x0, x1)\n",
    "\n",
    "    # 勾配クリア\n",
    "    x0.cleargrad()\n",
    "    x1.cleargrad()\n",
    "    \n",
    "    # 逆伝播で勾配計算\n",
    "    y.backward() # y = rosenbrockの微分\n",
    "\n",
    "    # 最適化\n",
    "    x0.data -= lr * x0.grad.data\n",
    "    x1.data -= lr * x1.grad.data\n",
    "\n",
    "print(x0, x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a2cd87-657c-40d3-a7e8-e6aacaa9adbd",
   "metadata": {},
   "source": [
    "#### 回帰問題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf2e9e-1767-4cbb-aaf2-380019be6b27",
   "metadata": {},
   "source": [
    "##### 線形モデルの回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5458c1c-df00-4c75-9453-a15202d08a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トイ・データセット\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 5 + 2 * x + np.random.rand(100, 1)\n",
    "x, y = Variable(x), Variable(y)  # 省略可能\n",
    "\n",
    "W = Variable(np.zeros((1, 1)))\n",
    "b = Variable(np.zeros(1))\n",
    "\n",
    "def predict(x):\n",
    "    y = F.matmul(x, W) + b\n",
    "    return y\n",
    "\n",
    "def mean_squared_error(x0, x1):\n",
    "    diff = x0 - x1\n",
    "    return F.sum(diff ** 2) / len(diff)\n",
    "\n",
    "lr = 0.1\n",
    "iters = 100\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = predict(x)\n",
    "    loss = mean_squared_error(y, y_pred)\n",
    "\n",
    "    W.cleargrad()\n",
    "    b.cleargrad()\n",
    "    loss.backward() # loss = 損失関数()の微分\n",
    "\n",
    "    # 重みの更新\n",
    "    W.data -= lr * W.grad.data\n",
    "    b.data -= lr * b.grad.data\n",
    "\n",
    "    # 損失の表示\n",
    "    if i % 10 == 0:\n",
    "        print(loss.data)\n",
    "\n",
    "print('====')\n",
    "print('W =', W.data)\n",
    "print('b =', b.data)\n",
    "\n",
    "# 回帰直線のPlot\n",
    "plt.scatter(x.data, y.data, s=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "t = np.arange(0, 1, .01)[:, np.newaxis]\n",
    "y_pred = predict(t)\n",
    "plt.plot(t, y_pred.data, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3455de-ebbe-45be-8a2f-9df166bfc016",
   "metadata": {},
   "source": [
    "##### 非線形モデルの回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13b227-3f60-4d48-866c-ff7f327228bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100, 1)\n",
    "\n",
    "lr = 0.2\n",
    "iters = 10000\n",
    "\n",
    "class TwoLayerNet(Model):\n",
    "    def __init__(self, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(hidden_size)\n",
    "        self.l2 = L.Linear(out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.sigmoid(self.l1(x))\n",
    "        y = self.l2(y)\n",
    "        return y\n",
    "\n",
    "model = TwoLayerNet(10, 1)\n",
    "optimizer = optimizers.SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "for i in range(iters):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward() # loss = 損失関数()の微分\n",
    "\n",
    "    # 重みの更新\n",
    "    optimizer.update()\n",
    "    \n",
    "    # 損失の表示\n",
    "    if i % 1000 == 0:\n",
    "        print(loss.data)\n",
    "\n",
    "# 回帰曲線のPlot\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "t = np.arange(0, 1, .01)[:, np.newaxis]\n",
    "y_pred = model(t)\n",
    "plt.plot(t, y_pred.data, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f16336-caac-4c2c-b7e5-0c7e5c4a40be",
   "metadata": {},
   "source": [
    "### DQN未満\n",
    "Q-Net（行動価値関数のDNN版）を適用したダケのQ学習は、まだDQN未満"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78a5d0-72f7-415a-ad96-c0d628dc1864",
   "metadata": {},
   "source": [
    "#### エージェントの実装1\n",
    "ココでは（方策オンの）Q学習をベースに実装しているが、コレは、  \n",
    "後に、オンライン学習からミニバッチ学習に変更するために、  \n",
    "経験再生を実装する際、方策オフ型である必要があるため。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c50b44-fa8b-4c6d-8b3f-20b753b4703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9\n",
    "        self.lr = 0.01\n",
    "        self.epsilon = 0.1\n",
    "        self.action_size = 4\n",
    "\n",
    "        # 行動価値関数をNN化したQNet1を利用\n",
    "        self.qnet = QNet1()\n",
    "        self.optimizer = optimizers.SGD(self.lr)\n",
    "        self.optimizer.setup(self.qnet)\n",
    "\n",
    "    # ε-greedyで行動を選択\n",
    "    def get_action(self, state_vec):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            qs = self.qnet(state_vec)\n",
    "            return qs.data.argmax()\n",
    "\n",
    "    # 方策改善（ステップ毎）\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # next_q_maxを計算\n",
    "        if done:\n",
    "            next_q_max = np.zeros(1)  # [0.]\n",
    "        else:\n",
    "            next_qs = self.qnet(next_state)\n",
    "            next_q_max = next_qs.max(axis=1)\n",
    "            next_q_max.unchain() # 勾配の計算から除外\n",
    "\n",
    "        # ココがQ学習\n",
    "        target = reward + self.gamma * next_q_max\n",
    "        \n",
    "        # ベルマン的先読みで精度を上げるのではなく、\n",
    "        # 正解ラベルとして方策NNを学習させて行動価値関数の精度を上げつつ、方策更新。\n",
    "        \n",
    "        # ・推定（順伝播）\n",
    "        qs = self.qnet(state)\n",
    "        q = qs[:, action]\n",
    "        \n",
    "        # ・学習（逆伝播）\n",
    "        loss = F.mean_squared_error(target, q) # 損失関数で損失計算\n",
    "        self.qnet.cleargrads() # 勾配を初期化\n",
    "        loss.backward() # loss = 損失関数()の微分（誤差逆伝播）\n",
    "        self.optimizer.update() # オンライン学習でパラメタ更新\n",
    "\n",
    "        return loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2590ff5-55a6-4109-a05d-2a724d8cb7e8",
   "metadata": {},
   "source": [
    "#### エージェントの実行1\n",
    "グリッド・ワールド上で"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d81815-d95b-4839-9130-45e217604390",
   "metadata": {},
   "source": [
    "##### DQN未満学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c24f97-5b15-4ce2-bfb2-0a00aa574b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "agent = QLearningAgent()\n",
    "\n",
    "# 1000エピソード\n",
    "episodes = 1000\n",
    "loss_history = []\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境の初期化\n",
    "    state = env.reset()\n",
    "    # 状態のone_hot化\n",
    "    state = one_hot(state)\n",
    "    # その他の初期化\n",
    "    total_loss, cnt = 0, 0\n",
    "    done = False\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        cnt += 1\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = one_hot(next_state) # 状態のone_hot化\n",
    "\n",
    "        # 都度学習 ≒ オンライン学習\n",
    "        loss = agent.update(state, action, reward, next_state, done)\n",
    "        total_loss += loss # 履歴用\n",
    "        state = next_state # 次処理用\n",
    "\n",
    "    # 履歴用\n",
    "    average_loss = total_loss / cnt\n",
    "    loss_history.append(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e646a-96ff-4c05-a130-b72bef46e49a",
   "metadata": {},
   "source": [
    "##### 学習履歴の表示\n",
    "深層学習の損失の履歴を表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772a54e-feac-4ccd-8f15-e195d2f20c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('episode')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(range(len(loss_history)), loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7390ea2-3393-4b52-93d1-bc6a8e3139a9",
   "metadata": {},
   "source": [
    "##### 行動価値関数の可視化\n",
    "行動価値関数をNN化したQNetの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b23ac4-9e04-40b7-a3cc-3f181f9bb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "Q = {}\n",
    "for state in env.states():\n",
    "    for action in env.action_space:\n",
    "        q = agent.qnet(one_hot(state))[:, action]\n",
    "        Q[state, action] = float(q.data)\n",
    "env.render_q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa1ad7-4b39-43b1-b812-0335bbd5445d",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4de18-f181-4d92-a431-556044d982bf",
   "metadata": {},
   "source": [
    "#### 倒立振子問題\n",
    "- アニメーション表示できないので...\n",
    "- PoleAngle が±0.23前後になると終了する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e094a44-6153-4ab1-a32e-ba6fef64d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境のインスタンスを作成\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# 状態を初期化\n",
    "state, info = env.reset()\n",
    "\n",
    "fig = define_area(state)\n",
    "list=[]\n",
    "row = 10\n",
    "col = 10\n",
    "num = 0\n",
    "#ims = []\n",
    "\n",
    "done = False\n",
    "#while not done:\n",
    "for num in range(100):\n",
    "\n",
    "    # 描画データ\n",
    "    rgb_data = env.render()\n",
    "    \n",
    "    # アクション（ランダム）\n",
    "    action = np.random.choice([0, 1])\n",
    "    next_state, reward, done, info, _ = env.step(action)\n",
    "    \n",
    "    # 表示データ蓄積\n",
    "    list.append([str(action), str(reward), str(next_state[0]), str(next_state[1]), str(next_state[2]), str(next_state[3]), str(done), str(info), str(_)])\n",
    "    num += 1\n",
    "    plt.subplot(row, col, num)\n",
    "    plt.imshow(rgb_data)\n",
    "    #ims.append([plt.imshow(rgb_data, animated=True)])\n",
    "    \n",
    "# データ表示\n",
    "print(tabulate(list, headers=[\"action\", \"reward\", \"CartPosition\", \"CartVelocity\", \"PoleAngle\", \"PoleVelocityAtTip\", \"done\", \"info\", \"_\"], tablefmt='grid'))\n",
    "#ani = animation.ArtistAnimation(fig, ims, interval=20, blit=True, repeat_delay=0)\n",
    "#ani.save('anim.gif', writer=\"imagemagick\")\n",
    "#ani.save('anim.mp4', writer=\"ffmpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f4939-a841-45c9-8adb-8ee18bd39be1",
   "metadata": {},
   "source": [
    "#### 経験再生（リプレイ・バッファ）\n",
    "- オンライン学習からミニバッチ学習に変更するのが目的。\n",
    "- 行動価値関数が時系列の影響を受けないケースでは時系列的な偏りが無い方が良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb06ed1-0777-4aa2-9b18-a2fd3881dc0d",
   "metadata": {},
   "source": [
    "##### クラス定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78edd40a-3927-49ba-8c45-3f4787646da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        # サンプルデータをランダムに抽出\n",
    "        data = random.sample(self.buffer, self.batch_size)\n",
    "        \n",
    "        # 状態・行動・報酬・次の状態・終了フラグを抽出\n",
    "        state = np.stack([x[0] for x in data])\n",
    "        action = np.array([x[1] for x in data])\n",
    "        reward = np.array([x[2] for x in data])\n",
    "        next_state = np.stack([x[3] for x in data])\n",
    "        done = np.array([x[4] for x in data]).astype(np.int32) # 因子型から整数型に変換\n",
    "        return state, action, reward, next_state, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16877d5d-1d98-463f-acd0-fee99aae9f4c",
   "metadata": {},
   "source": [
    "##### クラス試用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cc51a-8176-4eed-b719-7e5f417ada58",
   "metadata": {},
   "source": [
    "###### 記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08073e5-26a0-432e-a7c5-42317312bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インスタンスを作成\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# サンプルデータの保存数を指定\n",
    "buffer_size = 100\n",
    "# ミニバッチのデータ数を指定\n",
    "batch_size = 32\n",
    "\n",
    "replay_buffer = ReplayBuffer(buffer_size, batch_size)\n",
    "\n",
    "# エピソード数を指定\n",
    "episodes = 10\n",
    "\n",
    "# 繰り返しシミュレーション\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 状態を初期化\n",
    "    t = 0 # 時刻\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        t += 1\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = np.random.choice([0, 1])\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # サンプルデータを保存\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    # 当該エピソードのサンプルデータ数を表示\n",
    "    # dequeで実装されているので、サイズ以上に増えない。\n",
    "    print(\n",
    "        'episode ' + str(episode+1) + \n",
    "        ', T=' + str(t) + \n",
    "        ', buffer size:' + str(len(replay_buffer))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f9a1c-3f55-43c1-80a8-e9a5712c85fd",
   "metadata": {},
   "source": [
    "###### 再生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56511508-df5c-42f3-bdc8-99bc042dfa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の状態・行動・報酬・次の状態・終了フラグのミニバッチデータを取得\n",
    "state, action, reward, next_state, done = replay_buffer.get_batch()\n",
    "print(\"<state>\")\n",
    "print(state[:5].round(3))\n",
    "print(state.shape)\n",
    "print(\"\\n<action>\")\n",
    "print(action[:5])\n",
    "print(action.shape)\n",
    "print(\"\\n<reward>\")\n",
    "print(reward[:5])\n",
    "print(reward.shape)\n",
    "print(\"\\n<next_state>\")\n",
    "print(next_state[:5].round(3))\n",
    "print(next_state.shape)\n",
    "print(\"\\n<done>\")\n",
    "print(done[:5])\n",
    "print(done.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e53e4-9311-4a90-8fae-946ded5acb71",
   "metadata": {},
   "source": [
    "#### DQNの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e94567-fecc-4ddb-914e-7df0d26564b0",
   "metadata": {},
   "source": [
    "##### エージェントの実装2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f34ff-4554-4fc9-9824-5acb49f5db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQNのエージェントの実装\n",
    "class DQNAgent:\n",
    "    # 初期化メソッドの定義\n",
    "    def __init__(self):\n",
    "        # ハイパーパラメタを指定\n",
    "        self.gamma = 0.98 # 収益の計算用の割引率\n",
    "        self.lr = 0.0005 # 勾配降下法用の学習率\n",
    "        self.epsilon = 0.1 # ランダムに行動する確率\n",
    "        self.buffer_size = 10000 # サンプルデータの保存数\n",
    "        self.batch_size = 32 # ミニバッチのデータ数\n",
    "        self.action_size = 2 # 行動の種類数\n",
    "        \n",
    "        # インスタンスを作成\n",
    "        \n",
    "        # リプレイ・バッファ\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "        \n",
    "        # リプレイ・バッファを使う場合は方策オフ\n",
    "        self.qnet_target = QNet2(self.action_size) # 価値推定用の方策\n",
    "        self.qnet = QNet2(self.action_size) # データ収集用の方策\n",
    "        self.optimizer = optimizers.Adam(self.lr) # 最適化手法\n",
    "        self.optimizer.setup(self.qnet) # モデルを設定\n",
    "    \n",
    "    # 方策NNのパラメタ同期\n",
    "    def sync_qnet(self):\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "    \n",
    "    # データ収集用の方策からε-greedy法により行動を選択\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        else:\n",
    "            state = state[np.newaxis, :] # バッチ対応\n",
    "            qs = self.qnet(state)\n",
    "            return qs.data.argmax()\n",
    "    \n",
    "    # 方策反復（ステップ毎）\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        # ステップの履歴を追加（dequeで実装）\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # ミニバッチ学習なので学習にはバッチサイズ分のステップが必要\n",
    "        if len(self.replay_buffer) < self.batch_size: return None\n",
    "        \n",
    "        # バッチサイズ分の学習データを取り出し\n",
    "        state, action, reward, next_state, done = self.replay_buffer.get_batch()\n",
    "        \n",
    "        # next_q_maxを計算（価値推定用の方策を用いる）\n",
    "        next_qs = self.qnet_target(next_state) # 全ての行動\n",
    "        next_q_max = next_qs.max(axis=1) # 最大値の行動\n",
    "        next_q_max.unchain() # 勾配の計算から除外\n",
    "                \n",
    "        # ココがQ学習\n",
    "        # target = reward + self.gamma * next_q_max\n",
    "        target = reward + (1 - done) * self.gamma * next_q_max\n",
    "        \n",
    "        # ベルマン的先読みで精度を上げるのではなく、\n",
    "        # 正解ラベルとしてデータ収集用の方策NNを学習させて行動価値関数の精度を上げつつ、方策更新。\n",
    "        \n",
    "        # ・推定（順伝播）\n",
    "        qs = self.qnet(state)\n",
    "        q = qs[np.arange(self.batch_size), action]\n",
    "        \n",
    "        # ・学習（逆伝播）\n",
    "        loss = F.mean_squared_error(q, target) # 損失関数で損失計算\n",
    "        self.qnet.cleargrads() # 勾配を初期化\n",
    "        loss.backward() # loss = 損失関数()の微分（誤差逆伝播）\n",
    "        self.optimizer.update() # ミニバッチ学習でパラメタ更新\n",
    "        \n",
    "        # 価値推定用の方策NNは、数エピソード間隔で外から同期更新\n",
    "        \n",
    "        return loss.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e449db-6284-41e8-ab70-35eeb2d11341",
   "metadata": {},
   "source": [
    "##### エージェントの実行2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950f135-5d93-4bf3-9417-dfcc1b92f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = DQNAgent()\n",
    "\n",
    "# 方策NNのパラメタ同期タイミング\n",
    "sync_interval = 20\n",
    "# 推移の確認用のリストを初期化\n",
    "trace_loss = []\n",
    "trace_reward = []\n",
    "\n",
    "# 300エピソード\n",
    "episodes = 300\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境の初期化\n",
    "    state, info = env.reset()\n",
    "    # その他の初期化\n",
    "    t = 0 # 時刻\n",
    "    total_loss = 0.0\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    \n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        t += 1\n",
    "        \n",
    "        # ActionでStep\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # 都度学習（ミニバッチ化されている\n",
    "        loss = agent.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # 状態を更新\n",
    "        state = next_state\n",
    "        \n",
    "        # 合計損失を計算（実際に学習した場合）\n",
    "        if loss != None: total_loss += loss\n",
    "        # 合計報酬を計算\n",
    "        total_reward += reward\n",
    "        \n",
    "    # 方策NNのパラメタ同期\n",
    "    if episode % sync_interval == 0: agent.sync_qnet()\n",
    "        \n",
    "    # 平均損失・合計報酬を記録\n",
    "    trace_loss.append(total_loss / t)\n",
    "    trace_reward.append(total_reward)\n",
    "    \n",
    "    # 一定回数ごとに結果を表示\n",
    "    if (episode+1) % 20 == 0:\n",
    "        print(\n",
    "            'episode ' + str(episode+1) + \n",
    "            ', T=' + str(t) + \n",
    "            ', average loss=' + str(np.round(total_loss/t, 3)) + \n",
    "            ', total reward=' + str(total_reward)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1601072-0b89-4bee-b0e6-7df9204b1221",
   "metadata": {},
   "source": [
    "##### 実行結果の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbf93e-db65-424f-8d66-13bdaa320d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法名を取得\n",
    "optm_name = agent.optimizer.__class__.__name__\n",
    "\n",
    "# 学習率名を指定\n",
    "lr_name = 'alpha'\n",
    "\n",
    "# 学習率を取得\n",
    "lr = getattr(agent.optimizer, lr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544b7c91-e183-46d8-98c5-488fc3817c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均損失の推移を作図\n",
    "plt.figure(figsize=(8, 6), facecolor='white')\n",
    "plt.plot(np.arange(1, episodes+1), trace_loss)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('average loss')\n",
    "plt.suptitle('DQN', fontsize=20)\n",
    "plt.title(optm_name+': '+lr_name+'='+str(lr), loc='left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c624c-fecd-4055-96bc-b17b946c2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 総報酬の推移を作図\n",
    "plt.figure(figsize=(8, 6), facecolor='white')\n",
    "plt.plot(np.arange(1, episodes+1), trace_reward)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('total reward')\n",
    "plt.suptitle('DQN', fontsize=20)\n",
    "plt.title(optm_name+': '+lr_name+'='+str(lr), loc='left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
