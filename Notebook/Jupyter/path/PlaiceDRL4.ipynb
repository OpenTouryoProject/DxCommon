{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyのツノガレイ強化学習の本\n",
    "\n",
    "## 深層学習で方策勾配法\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- [方策勾配法](#方策勾配法)\n",
    "  - [エージェントの実装1](#エージェントの実装1)\n",
    "  - [エージェントの実行1](#エージェントの実行1)\n",
    "- [REINFORCE](#REINFORCE)\n",
    "  - [エージェントの実装2](#エージェントの実装2)\n",
    "  - [エージェントの実行2](#エージェントの実行2)\n",
    "- [Actor-Critic](#Actor-Critic)\n",
    "  - [エージェントの実装3](#エージェントの実装3)\n",
    "  - [エージェントの実行3](#エージェントの実行3)\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-4/tree/master/ch01\n",
    "- [強化学習（Reinforcement Learning） - .NET 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%EF%BC%88Reinforcement%20Learning%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39e2e7-67b7-464d-b254-68521ef5a3de",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea6589-87d6-459c-b8b8-55179ed99adf",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65c1a3d-fc30-4a7f-b180-a221b11e7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install tabulate\n",
    "!pip install matplotlib\n",
    "!pip install dezero\n",
    "!pip install dezerogym\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4d655-73aa-4cef-9def-0de5a4ef83d1",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854310-e2f8-48c9-986c-e05d242744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 廃止となったエイリアスを書く\n",
    "np.object = object\n",
    "np.bool = bool\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.typeDict = {k: v for k, v in np.sctypeDict.items() if isinstance(v, type)}\n",
    "\n",
    "from dezero import Variable\n",
    "from dezero import Model\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F\n",
    "import dezero.layers as L\n",
    "\n",
    "from dezerogym.gridworld import GridWorld\n",
    "import gym # OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a909-b830-4776-890f-b6cef29a456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836fb9c-95b7-4cce-9651-d948fcc6fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共通関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf142c5-8f83-43d4-b975-7db87492bdca",
   "metadata": {},
   "source": [
    "#### 総報酬表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81339f-5c49-4f6c-83db-574065089caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_total_reward(reward_history):\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.plot(range(len(reward_history)), reward_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018c6cd-e270-4b67-8c53-994665fda7b8",
   "metadata": {},
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1b145-4197-4219-85a4-b5b5a83f58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(Model):\n",
    "    def __init__(self, action_size):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(128)\n",
    "        self.l2 = L.Linear(action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.softmax(self.l2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8eef32-8c0c-440e-bcbb-676997a61e7f",
   "metadata": {},
   "source": [
    "#### PolicyNetとValueNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c389a7-1bd8-4e41-bb9c-e1207be54917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(Model):\n",
    "    def __init__(self, action_size=2):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(128)\n",
    "        self.l2 = L.Linear(action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "class ValueNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = L.Linear(128)\n",
    "        self.l2 = L.Linear(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81287d9b-764a-4346-80ed-3d8883c80495",
   "metadata": {},
   "source": [
    "### 方策勾配法\n",
    "- 方策勾配法は、エージェントの行動方策を直接最適化する手法であり、連続的な状態空間や行動空間に適しています。\n",
    "- 深層学習を用いることで、複雑な方策を学習できるため、現実的な応用において重要な手法となっている。\n",
    "- 代表的な方策勾配法にはREINFORCEやActor-Critic、PPOなどがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbc95e-d18a-4cf9-93c9-d6a7ae725fe9",
   "metadata": {},
   "source": [
    "#### エージェントの実装1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea397b42-1e9a-4908-9448-7072d34a6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.0002\n",
    "        self.action_size = 2\n",
    "\n",
    "        # 行動履歴\n",
    "        self.memory = []\n",
    "        \n",
    "        # 方策NN\n",
    "        self.pi = Policy(self.action_size)\n",
    "        self.optimizer = optimizers.Adam(self.lr)\n",
    "        self.optimizer.setup(self.pi)\n",
    "\n",
    "    # 方策からアクションを決める。\n",
    "    def get_action(self, state):\n",
    "        state = state[np.newaxis, :] # add batch axis\n",
    "        probs = self.pi(state)\n",
    "        probs = probs[0]\n",
    "        action = np.random.choice(len(probs), p=probs.data)\n",
    "        return action, probs[action]\n",
    "\n",
    "    # 行動履歴の記録\n",
    "    def add(self, reward, prob):\n",
    "        data = (reward, prob)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    # エピソードを学習（≒ミニバッチ学習）\n",
    "    def update(self):\n",
    "        \n",
    "        # 初期化\n",
    "        G, loss = 0, 0\n",
    "        self.pi.cleargrads()\n",
    "        \n",
    "        # 行動履歴から逆再生して報酬を計算\n",
    "        for reward, prob in reversed(self.memory):\n",
    "            G = reward + self.gamma * G\n",
    "\n",
    "        # 損失関数で損失計算\n",
    "        # 収益を行動確率を重みに合計\n",
    "        for reward, prob in self.memory:\n",
    "            loss += -F.log(prob) * G\n",
    "\n",
    "        # 微分（誤差逆伝播）\n",
    "        loss.backward() # loss = 損失関数()の微分\n",
    "        self.optimizer.update() # ミニバッチ学習で更新\n",
    "        self.memory = [] # 履歴のクリア"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ff616-00be-410e-833d-06ec1cb6a0dc",
   "metadata": {},
   "source": [
    "#### エージェントの実行1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e12d0d-a61a-4dc3-a7b7-4bcd6a892991",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent1()\n",
    "\n",
    "# 推移の確認用のリストを初期化\n",
    "reward_history = []\n",
    "\n",
    "# 3000エピソード\n",
    "episodes = 3000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境の初期化\n",
    "    state, info = env.reset()\n",
    "    # その他の初期化\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action, prob = agent.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # エピソードを記録\n",
    "        agent.add(reward, prob)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # エピソードを学習（≒ミニバッチ学習）\n",
    "    agent.update()\n",
    "\n",
    "    # 結果を記録\n",
    "    reward_history.append(total_reward)\n",
    "    if episode % 100 == 0:\n",
    "        print(\"episode :{}, total reward : {:.1f}\".format(episode, total_reward))\n",
    "\n",
    "# plot\n",
    "plot_total_reward(reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156ee50-ea2e-42d4-9bcf-a783ee043666",
   "metadata": {},
   "source": [
    "### REINFORCE\n",
    "- 最も基本的な方策勾配アルゴリズム。\n",
    "- エピソードごとに累積報酬を用いて方策の更新を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ad4fe-6d17-483f-af90-0a4926274268",
   "metadata": {},
   "source": [
    "#### エージェントの実装2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9ec4b-b400-4d3c-a287-6259da96fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr = 0.0002\n",
    "        self.action_size = 2\n",
    "\n",
    "        # 行動履歴\n",
    "        self.memory = []\n",
    "        \n",
    "        # 方策NN\n",
    "        self.pi = Policy(self.action_size)\n",
    "        self.optimizer = optimizers.Adam(self.lr)\n",
    "        self.optimizer.setup(self.pi)\n",
    "\n",
    "    # 方策からアクションを決める。\n",
    "    def get_action(self, state):\n",
    "        state = state[np.newaxis, :] # add batch axis\n",
    "        probs = self.pi(state)\n",
    "        probs = probs[0]\n",
    "        action = np.random.choice(len(probs), p=probs.data)\n",
    "        return action, probs[action]\n",
    "\n",
    "    # 行動履歴の記録\n",
    "    def add(self, reward, prob):\n",
    "        data = (reward, prob)\n",
    "        self.memory.append(data)\n",
    "\n",
    "    # エピソードを学習（≒ミニバッチ学習）\n",
    "    def update(self):\n",
    "        \n",
    "        # 初期化\n",
    "        G, loss = 0, 0\n",
    "        self.pi.cleargrads()\n",
    "\n",
    "        # 行動履歴から逆再生して報酬を計算\n",
    "        for reward, prob in reversed(self.memory):\n",
    "            G = reward + self.gamma * G\n",
    "            \n",
    "            # 損失関数で損失計算\n",
    "            # 収益を行動確率を重みに合計\n",
    "            # 方策勾配法と違って計算途中の収益Gを使っている。\n",
    "            loss += -F.log(prob) * G\n",
    "\n",
    "        # 微分（誤差逆伝播）\n",
    "        loss.backward() # loss = 損失関数()の微分\n",
    "        self.optimizer.update() # ミニバッチ学習で更新\n",
    "        self.memory = [] # 履歴のクリア"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b9298-1852-4f9d-a05f-77f850999cf0",
   "metadata": {},
   "source": [
    "#### エージェントの実行2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a09dbe-7668-4e6d-817a-177e8b2714bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent2()\n",
    "\n",
    "# 推移の確認用のリストを初期化\n",
    "reward_history = []\n",
    "\n",
    "# 3000エピソード\n",
    "episodes = 3000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境の初期化\n",
    "    state, info = env.reset()\n",
    "    # その他の初期化\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action, prob = agent.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # エピソードを記録\n",
    "        agent.add(reward, prob)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # エピソードを学習（ミニバッチ学習）\n",
    "    agent.update()\n",
    "\n",
    "    # 結果を記録\n",
    "    reward_history.append(total_reward)\n",
    "    if episode % 100 == 0:\n",
    "        print(\"episode :{}, total reward : {:.1f}\".format(episode, total_reward))\n",
    "\n",
    "# plot\n",
    "plot_total_reward(reward_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4571706-33f9-4091-9a79-76a654f6910a",
   "metadata": {},
   "source": [
    "### Actor-Critic\n",
    "- 方策を更新するアクター（Actor）と、行動価値関数を近似するクリティック（Critic）を組み合わせる手法。\n",
    "- クリティックは、状態価値関数や行動価値関数を学習し、その評価に基づいてアクターが方策を更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2415c3-5df4-44c6-8e07-d8dcac405e7f",
   "metadata": {},
   "source": [
    "#### エージェントの実装3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75637f0b-55af-4158-aa3d-8014223b5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.98\n",
    "        self.lr_pi = 0.0002\n",
    "        self.lr_v = 0.0005\n",
    "        self.action_size = 2\n",
    "\n",
    "        self.pi = PolicyNet() # Actor\n",
    "        self.optimizer_pi = optimizers.Adam(self.lr_pi).setup(self.pi)\n",
    "        self.v = ValueNet() # Critic\n",
    "        self.optimizer_v = optimizers.Adam(self.lr_v).setup(self.v)\n",
    "\n",
    "    # 方策（Actor）からアクションを決める。\n",
    "    def get_action(self, state):\n",
    "        state = state[np.newaxis, :] # add batch axis\n",
    "        probs = self.pi(state)\n",
    "        probs = probs[0]\n",
    "        action = np.random.choice(len(probs), p=probs.data)\n",
    "        return action, probs[action]\n",
    "\n",
    "    # オンライン学習\n",
    "    def update(self, state, action_prob, reward, next_state, done):\n",
    "        state = state[np.newaxis, :] # add batch axis\n",
    "        next_state = next_state[np.newaxis, :] # add batch axis\n",
    "\n",
    "        # ========== (1) Update V network ===========\n",
    "        v = self.v(state)\n",
    "        \n",
    "        # 以下の式はDQNのQ学習 \n",
    "        # target = reward + (1 - done) * self.gamma * next_q_max\n",
    "        target = reward + (1 - done) * self.gamma * self.v(next_state)\n",
    "        \n",
    "        # 損失関数で損失計算\n",
    "        target.unchain() # 勾配の計算から除外\n",
    "        loss_v = F.mean_squared_error(v, target)\n",
    "\n",
    "        # ========== (2) Update pi network ===========\n",
    "        delta = target - v\n",
    "        \n",
    "        # 損失関数で損失計算\n",
    "        delta.unchain() # 勾配の計算から除外\n",
    "        loss_pi = -F.log(action_prob) * delta\n",
    "\n",
    "        # ============================================\n",
    "        self.v.cleargrads()\n",
    "        self.pi.cleargrads()\n",
    "        \n",
    "        # 微分（誤差逆伝播）\n",
    "        loss_v.backward() # loss_v = 損失関数()の微分\n",
    "        loss_pi.backward() # loss_pi = 損失関数()の微分\n",
    "        \n",
    "        # オンライン学習で更新\n",
    "        self.optimizer_v.update()\n",
    "        self.optimizer_pi.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0400585-f539-46c2-a21c-0c4cc228ed70",
   "metadata": {},
   "source": [
    "#### エージェントの実行3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1d8d9-c2a8-43c1-a3ed-d9386f99f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = Agent3()\n",
    "\n",
    "# 推移の確認用のリストを初期化\n",
    "reward_history = []\n",
    "\n",
    "# 3000エピソード\n",
    "episodes = 3000\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # 環境の初期化\n",
    "    state, info = env.reset()\n",
    "    # その他の初期化\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    # done=tureまでが1エピソード\n",
    "    while not done:\n",
    "        \n",
    "        # ActionでStep\n",
    "        action, prob = agent.get_action(state)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # 都度学習（オンライン学習）\n",
    "        agent.update(state, prob, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # 結果を記録\n",
    "    reward_history.append(total_reward)\n",
    "    if episode % 100 == 0:\n",
    "        print(\"episode :{}, total reward : {:.1f}\".format(episode, total_reward))\n",
    "\n",
    "# plot\n",
    "plot_total_reward(reward_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
