{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f411ca43-7579-4ada-9c84-f8a4e38561d5",
   "metadata": {},
   "source": [
    "# o'reillyのカサゴ深層学習の本\n",
    "\n",
    "## MINST推論と学習\n",
    "- [多層パーセプトロン（ニューラルネットワーク](#多層パーセプトロン（ニューラルネットワーク)\n",
    "- [MINST順方向（推論](#MINST順方向（推論)\n",
    "- [MINST逆方向（学習](#MINST逆方向（学習)\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/notebooks/\n",
    "- [深層学習（deep learning） - 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88deep%20learning%EF%BC%89) > [ニューラルネットワーク](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdad9bd-8fb6-46b1-a27a-8415f37ae8b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 多層パーセプトロン（ニューラルネットワーク\n",
    "- コレはコレで関数のようなもの。\n",
    "- 重みを変更して現象に近似させる（近似関数的な）。  \n",
    "（結局、ディープラーニングは最小二乗法のお化けのようなもの）\n",
    "- ちなみに、重みは、y=ax^b で言ったら a とか b の事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410964e-9a22-4b48-b8ba-248dca9a6f5b",
   "metadata": {},
   "source": [
    "<img src=\"../work/3tier-neuralnetwork.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781df28b-c867-4cba-99ac-371ffdc558d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MINST順方向（推論\n",
    "https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%88%E6%8E%A8%E8%AB%96%EF%BC%89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c91b2-6cf8-4625-b74e-86d83e04ed41",
   "metadata": {},
   "source": [
    "### MINSTデータのロード\n",
    "- load_mnist() のプロキシ設定I/Fは環境変数のみ。\n",
    "- [dataset/mnist.py](Kasago/dataset/mnist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bbd10a-836f-4ace-93b0-9d81df9e0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "from IPython.display import display # Notebook用\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    #pil_img.show()\n",
    "    display(pil_img)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)  # 5\n",
    "\n",
    "print(img.shape)  # (784,)\n",
    "img = img.reshape(28, 28)  # 形状を元の画像サイズに変形\n",
    "print(img.shape)  # (28, 28)\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb6303-35f6-46e9-b831-53f5a00387dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MINSTデータの推論\n",
    "- ここでは多層パーセプトロンと学習済みの重みを使う。\n",
    "- 多層パーセプトロンは、行列で計算する。\n",
    "  - 例えば、  \n",
    "    - 入力層ベクトルがn（画素数：28*28=784\n",
    "    - 出力層ベクトルがm（MISNT：0-9の10\n",
    "  - の場合、重みはn行m列の行列になる。\n",
    "- なお活性化関数は、\n",
    "  - 入力層は、[シグモイド関数](KasagoDL1.ipynb)を使っている。\n",
    "  - 出力層は0-9の10個で、[ソフトマックス関数](KasagoDL1.ipynb)になる。\n",
    "- Import\n",
    "  - [dataset/mnist.py](Kasago/dataset/mnist.py)\n",
    "  - [common/functions.py](Kasago/common/functions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973caabd-3602-4b9a-bbe3-1588f99c3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import pickle\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.functions import sigmoid, softmax\n",
    "\n",
    "def get_data():\n",
    "    # MINSTデータのロード\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # パラメタ（重みとバイアス）の初期化\n",
    "    with open(\"./kasago/pkl/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    # パラメタ（重みとバイアス）\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    # 多層パーセプトロン\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "# テストデータ\n",
    "x, t = get_data()\n",
    "# パラメタ（重みとバイアス）の初期化\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    \n",
    "    # 推論（分類\n",
    "    y = predict(network, x[i])\n",
    "    \n",
    "    # yには、0-9の確率が格納されているので、\n",
    "    # 最も確率の高い要素のインデックスを取得\n",
    "    p = np.argmax(y)\n",
    "    \n",
    "    # 正解ならインクリメント\n",
    "    if p == t[i]: accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06992d49-454e-40d3-89a1-c94581dfbca3",
   "metadata": {},
   "source": [
    "### MINSTデータの推論（バッチ化\n",
    "- 入力ベクトルがn、出力ベクトルがmの場合、  \n",
    "入力を100行n列の行列にすると、重みを変えず（n行m列）、出力は100行n列の行列にできる。\n",
    "- Import\n",
    "  - [dataset/mnist.py](Kasago/dataset/mnist.py)\n",
    "  - [common/functions.py](Kasago/common/functions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa68d4-a496-4251-9df7-9272d8b8b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import pickle\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.functions import sigmoid, softmax\n",
    "\n",
    "def get_data():\n",
    "    # MINSTデータのロード\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # パラメタ（重みとバイアス）の初期化\n",
    "    with open(\"./kasago/pkl/sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    # パラメタ（重みとバイアス）\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    # 多層パーセプトロン\n",
    "    # １層\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    # ２層\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    # 出力層\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "# テストデータ\n",
    "x, t = get_data()\n",
    "# パラメタ（重みとバイアス）の初期化\n",
    "network = init_network()\n",
    "\n",
    "# パイパーパラメタ\n",
    "batch_size = 100 # バッチサイズ\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    \n",
    "    # 推論（分類\n",
    "    y_batch = predict(network, x_batch)\n",
    "\n",
    "    # yには、0-9の確率が格納されているので、\n",
    "    # 最も確率の高い要素のインデックスを取得\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    \n",
    "    # 正解ならインクリメント\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd8f00-b583-4b13-be7e-f537fca0abd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MINST逆方向（学習\n",
    "- 問題は、この重みをどのように設定するか？\n",
    "- 学習（[勾配法](#勾配降下法) → [誤差逆伝播法](KasagoDL3.ipynb)）により重みを自動計算する。\n",
    "\n",
    "https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%88%E5%AD%A6%E7%BF%92%EF%BC%89"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f5535-ac99-4ebc-9092-8e0822e12879",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb67159-fef4-41eb-ac39-d45e16b6f14c",
   "metadata": {},
   "source": [
    "#### 2乗和誤差\n",
    "- 回帰の場合（[恒等関数](KasagoDL1.ipynb)と相性が良く、出力と教師データの各要素の差の二乗の総和の２分の一。\n",
    "- なお、この[損失関数](#損失関数)は、入力をy（[恒等関数](KasagoDL1.ipynb)の出力）, t（教師データ）とした時、  \n",
    "[恒等関数](KasagoDL1.ipynb)の[逆伝播](KasagoDL3.ipynb)が y-t となるように設計されている。\n",
    "$$\n",
    "    E = \\frac{1}{2} \\sum_{k}(yk-tk)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8050f7-3bd8-4a71-9711-1a446be79a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 損失関数(2乗和誤差)\n",
    "def mean_squared_error(yk, tk):\n",
    "    return 0.5 * np.sum((yk - tk)**2)\n",
    "\n",
    "tk = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "yk = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(mean_squared_error(yk, tk))\n",
    "\n",
    "yk = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(mean_squared_error(yk, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7e78d-de92-47a3-990b-9a9f8c4533ab",
   "metadata": {},
   "source": [
    "#### 交差エントロピー誤差\n",
    "- 多値分類の場合（[ソフトマックス関数](KasagoDL1.ipynb)と相性が良く、正解ラベルのykデータの底がeの自然対数 log eを計算する。\n",
    "- なお、この[損失関数](#損失関数)は、入力をy（[ソフトマックス関数](KasagoDL1.ipynb)の出力）, t（教師データ）とした時、  \n",
    "[ソフトマックス関数](KasagoDL1.ipynb)の[逆伝播](KasagoDL3.ipynb)が y-t となるように設計されている。\n",
    "$$\n",
    "    E = - \\sum_{k}tk \\log _e yk\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d8199-8959-4068-8c40-8bc8ad49910c",
   "metadata": {},
   "source": [
    "##### オンラインの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba33c58-aedf-4479-ac51-50dd1bd8eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 損失関数(交差エントロピー誤差)\n",
    "def cross_entropy_error(yk, tk):\n",
    "    delta = 1e-7 # log(0)はマイナス∞になるのを微小な値を足して防止。\n",
    "    return -np.sum(tk * np.log(yk + delta))\n",
    "\n",
    "tk = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "yk = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(cross_entropy_error(yk, tk))\n",
    "\n",
    "yk = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(cross_entropy_error(yk, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46582ee8-8d14-4c9a-828d-dab2a43586d6",
   "metadata": {},
   "source": [
    "##### ミニバッチの場合\n",
    "ミニバッチの場合、平均を取れば良いと言うのがピンとこない。\n",
    "$$\n",
    "    E = - \\frac{1}{N} \\sum_{n} \\sum_{k}tk \\log _e yk\n",
    "$$\n",
    "- これは、近似する関数自体ではなく近似する関数の[損失関数](#損失関数)であること。\n",
    "- 各重みが[損失関数](#損失関数)のx、y、zになってコレを偏微分するイメージ。\n",
    "- なので、\n",
    "  - バッチ化して損失の平均値で勾配 → 重みを計算しても問題はない。\n",
    "  - 入力データ毎に近似関数の重みが変わると言う事ではないので。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96086f-1eb7-41cc-9e16-ff3afc546bab",
   "metadata": {},
   "source": [
    "###### OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4642c-ad48-4231-9145-3c04bcfad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 損失関数(交差エントロピー誤差)\n",
    "def mean_squared_error(ynk, tnk):\n",
    "    batch_size = ynk.shape[0]\n",
    "    print(\"batch_size:\" + str(batch_size))\n",
    "    delta = 1e-7 # log(0)はマイナス∞になるのを微小な値を足して防止。\n",
    "    return - (np.sum(tnk * np.log(ynk + delta))) / batch_size\n",
    "\n",
    "TNK = np.array( \\\n",
    "               [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0], \\\n",
    "                [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "YNK = np.array( \\\n",
    "               [[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0], \\\n",
    "                [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]])\n",
    "print(\"mean_squared_error:\" + str(mean_squared_error(YNK, TNK)))\n",
    "\n",
    "YNK = np.array( \\\n",
    "               [[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0], \\\n",
    "                [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]])\n",
    "print(\"mean_squared_error:\" + str(mean_squared_error(YNK, TNK)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872b53b-3e93-4c7a-95bf-4ee2791e60c0",
   "metadata": {},
   "source": [
    "###### OneHotでない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5566532-dcb8-48f2-8116-70a5aa63c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 損失関数(交差エントロピー誤差)\n",
    "def mean_squared_error(ynk, tnk):\n",
    "    batch_size = ynk.shape[0]\n",
    "    print(\"batch_size:\" + str(batch_size))\n",
    "    ynk += 1e-7 # log(0)はマイナス∞になるのを微小な値を足して防止。\n",
    "    return - (np.sum(np.log(ynk[np.arange(batch_size), tnk]))) / batch_size\n",
    "\n",
    "TNK = np.array([2, 2])\n",
    "YNK = np.array( \\\n",
    "               [[0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0], \\\n",
    "                [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]])\n",
    "print(\"mean_squared_error:\" + str(mean_squared_error(YNK, TNK)))\n",
    "\n",
    "YNK = np.array( \\\n",
    "               [[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0], \\\n",
    "                [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]])\n",
    "print(\"mean_squared_error:\" + str(mean_squared_error(YNK, TNK)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e84d0-b7ae-4326-b32e-cbb7f0754f8b",
   "metadata": {},
   "source": [
    "### 勾配降下法\n",
    "[損失関数](#損失関数)を微分して勾配を求めパラメタを更新していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e6143-2e07-4100-b1bc-40e767872c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 微分\n",
    "[損失関数](#損失関数)を微分して勾配を求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861cded-e109-4974-a832-66e330f3e800",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 数値微分\n",
    "「解析的」に解けない関数の微分係数を近似する。\n",
    "- hを0に近付けられないので、hに小さい値を使用し、\n",
    "- 中心差分f(x+h) - f(x-h)を使用して誤差を減らす。\n",
    "\n",
    "ただし、 [誤差逆伝播法](KasagoDL3.ipynb)では「解析的」に解く。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad7424-235c-443c-8ce3-e780bc81401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# 数値微分\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "# 微分対象の関数（曲線）\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "# 接線を返す関数\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "\n",
    "# x=0-20まで0.1刻み\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "\n",
    "# 微分対象関数のx, y\n",
    "y = function_1(x)\n",
    "\n",
    "# x=5の接線、y2=tf(x)でx, yを計算\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "# グラフ描画\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)  # 曲線\n",
    "plt.plot(x, y2) # 接戦\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87943897-c12f-4d18-a0ef-4d05279e00e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 偏微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6c885-6eef-4a11-8ef7-501fa0d111e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 曲面のイメージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6596f-e501-4ca6-a8e9-495fdd107bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0, x1 を -4 ～ +4 まで0.25刻み\n",
    "x = np.arange(-4, 4, 0.25)\n",
    "y = np.arange(-4, 4, 0.25)\n",
    "\n",
    "# 格子点（X, Y）\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# 格、格子点でのZ\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# ワイヤーフレーム図を作成\n",
    "fig = plt.figure(figsize=(8, 8)) # 図の設定\n",
    "ax = fig.add_subplot(projection='3d') # 3D用の設定\n",
    "ax.plot_wireframe(X, Y, Z) # ワイヤーフレーム図\n",
    "ax.set_xlabel('x') # x軸ラベル\n",
    "ax.set_ylabel('y') # y軸ラベル\n",
    "ax.set_zlabel('z') # z軸ラベル\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792e130-12e6-4d72-8279-94881a7bb62f",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 曲面の偏微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf4878-2df0-417c-ad7d-9dcad1760b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 偏微分（勾配を返す）\n",
    "def numerical_gradient(f, X):\n",
    "    h = 1e-4  # 0.0001\n",
    "    \n",
    "    # 要素の値が0の同じ形式のn行２列の行列\n",
    "    grad1 = np.zeros_like(X)\n",
    "        \n",
    "    for idx1, x in enumerate(X):\n",
    "        \n",
    "        # 要素の値が0の同じ形式の２列のベクトル\n",
    "        grad2 = np.zeros_like(x)\n",
    "        \n",
    "        for idx2 in range(x.size):\n",
    "            tmp_val = x[idx2]\n",
    "            \n",
    "            x[idx2] = float(tmp_val) + h\n",
    "            fxh1 = f(x)  # f(x+h)\n",
    "            \n",
    "            x[idx2] = tmp_val - h \n",
    "            fxh2 = f(x)  # f(x-h)\n",
    "            \n",
    "            grad2[idx2] = (fxh1 - fxh2) / (2*h)\n",
    "            x[idx2] = tmp_val  # 値を元に戻す\n",
    "        \n",
    "        grad1[idx1] = grad2\n",
    "        \n",
    "    return grad1\n",
    "\n",
    "# 偏微分対象の関数（曲面）\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# 曲面の格子点での勾配を計算\n",
    "# 格子点をベクトル化して\n",
    "_X = X.flatten()\n",
    "_Y = Y.flatten()\n",
    "# n行２列の全ペアにして（.Tは転置を意味）\n",
    "koushiten = np.array([_X, _Y]).T\n",
    "# 格子点での勾配を返す（.Tは転置を意味）\n",
    "grad = numerical_gradient(function_2, koushiten).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60613-cf5e-40a2-9ba1-0f54e0d08c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e7fbf-b69c-4fdc-814e-8033f17639cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# ベクトル場を表示\n",
    "plt.quiver(_X, _Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "# 範囲\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "# 凡例\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298adf4-58a5-43af-99de-d710288b9c19",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示２\n",
    "等高線を足してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20659636-58be-45c4-adef-9ab4d33e7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図示 \n",
    "plt.figure()\n",
    "\n",
    "# 等高線を表示 --------------------\n",
    "plt.contourf(X, Y, Z, alpha=0.5)\n",
    "# ---------------------------------\n",
    "\n",
    "# ベクトル場を表示\n",
    "plt.quiver(_X, _Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "\n",
    "# 範囲\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "# 凡例\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba36cc-7e8c-439a-a487-13870748cafa",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示３\n",
    "曲面に勾配を書いてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001b49b-fecc-4243-af2a-7390e79905d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格子点のX、Y軸方向の勾配\n",
    "dX = grad[0, :].reshape(X.shape)\n",
    "dY = grad[1, :].reshape(Y.shape)\n",
    "\n",
    "# Z軸方向の勾配を計算\n",
    "W = np.sqrt(dX**2 + dY**2)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8)) # 図の設定\n",
    "ax = fig.add_subplot(projection='3d') # 3D用の設定\n",
    "\n",
    "# 曲面\n",
    "ax.plot_wireframe(X, Y, Z) # ワイヤーフレーム図\n",
    "# 勾配\n",
    "ax.quiver(X, Y, Z, -dX/W, -dY/W, -W, \n",
    "          color='green', pivot='tail', arrow_length_ratio=0.1, length=0.5, label='grad') \n",
    "\n",
    "ax.set_xlabel('x') # x軸ラベル\n",
    "ax.set_ylabel('y') # y軸ラベル\n",
    "ax.set_zlabel('z') # z軸ラベル\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6f171-ed33-4e6e-8b69-14a27ebef307",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### パラメタ更新\n",
    "[勾配降下法](#勾配降下法)のパラメタ更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590dda7a-a7d0-44dd-bc13-49a5de42c8e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 曲面の最急降下法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4b35b-536c-4300-a927-1626c9be0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# 偏微分（勾配を返す）\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4  # 0.0001\n",
    "        \n",
    "    # 要素の値が0の同じ形式の２列のベクトル\n",
    "    grad2 = np.zeros_like(x)\n",
    "        \n",
    "    for idx2 in range(x.size):\n",
    "        tmp_val = x[idx2]\n",
    "            \n",
    "        x[idx2] = float(tmp_val) + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "            \n",
    "        x[idx2] = tmp_val - h \n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "            \n",
    "        grad2[idx2] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx2] = tmp_val  # 値を元に戻す\n",
    "        \n",
    "    return grad2\n",
    "\n",
    "# 偏微分対象の関数（曲面）\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "# 最急降下法\n",
    "# f=損失関数, init_x=開始地点, lr=学習率, step_num=計算回数\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return np.array(x_history)\n",
    "\n",
    "# パイパーパラメタ\n",
    "\n",
    "# 開始地点\n",
    "init_x = np.array([-3.0, 4.0])  \n",
    "# 計算回数\n",
    "step_num = 20\n",
    "# 学習率\n",
    "lr = 0.1\n",
    "\n",
    "# 最急降下法\n",
    "# f=損失関数, init_x=開始地点, lr=学習率, step_num=計算回数\n",
    "x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "# 図の設定\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "\n",
    "# 等高線\n",
    "plt.contour(X, Y, Z)\n",
    "# 更新値の推移\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "# 範囲\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "# ラベル\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe65ba6-90a6-460c-a476-abbad3b25b1e",
   "metadata": {},
   "source": [
    "##### 単層ニューラルネットワークでの勾配計算\n",
    "- 前述で曲面（[損失関数](#損失関数)に相当）を偏微分して勾配を計算したのに対し、  \n",
    "ニューラルネットワークにおける[損失関数](#損失関数)を偏微分して勾配を計算する。  \n",
    "- そもそも近似関数と[損失関数](#損失関数)の違いに注意。\n",
    "  - 近似関数には重み変数が含まれ、[損失関数](#損失関数)にも重み変数が含まれる。\n",
    "  - 近似関数の[損失関数](#損失関数)を重み変数で微分して勾配を求める。\n",
    "  - 勾配は[損失関数](#損失関数)の最小値となる点を求めるために使用。\n",
    "  - [損失関数](#損失関数)の最小値となる点の値は、近似関数で最適化された重みの値となる。\n",
    "- コレをニューラルネットワークに適用するとなると、\n",
    "  - ニューラルネットワーク自体も近似関数で、\n",
    "  - 近似関数の[損失関数](#損失関数)を重み変数で[数値微分](#数値微分)して勾配を計算する。\n",
    "  - 各重み（行列の要素）が[損失関数](#損失関数)のx、y、zになってコレを偏微分するイメージ。\n",
    "- Import\n",
    "  - [common/functions.py](Kasago/common/functions.py)\n",
    "  - [common/gradient.py](Kasago/common/gradient.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f92e64-ed75-415a-a889-2f097d858fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from kasago.common.functions import softmax, cross_entropy_error\n",
    "from kasago.common.gradient import numerical_gradient\n",
    "\n",
    "# 単層ニューラルネットワーク・クラス\n",
    "class simpleNet:\n",
    "\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        \n",
    "        # ランダムな重み\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 推論\n",
    "        \n",
    "        # ベクトルと重みの積算\n",
    "        y = np.dot(x, self.W)\n",
    "        \n",
    "        # ソフトマックス\n",
    "        return softmax(y)\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        # 勾配計算用関数\n",
    "        \n",
    "        # 推論\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # 損失関数\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "# 入力（ベクトル\n",
    "x = np.array([0.6, 0.9])\n",
    "# 教師（ベクトル\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "# 単層ニューラルネットワーク・インスタンス\n",
    "net = simpleNet()\n",
    "\n",
    "# 勾配計算用関数\n",
    "f = lambda w: net.loss(x, t)\n",
    "# 勾配計算（数値微分）\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a9424-8c10-4ddc-bc23-a04f2573ff93",
   "metadata": {},
   "source": [
    "##### ２層ニューラルネットワークの勾配\n",
    "２層ニューラルネットワークで実際に重みやバイアス毎に勾配を計算する。\n",
    "- 通常版：微分して勾配を計算する。\n",
    "- 高速版：[誤差逆伝播法](KasagoDL3.ipynb)で勾配を計算\n",
    "\n",
    "勾配によって更新した重みやバイアスを使用して、学習を繰り返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad7a0b-2d30-46be-aef3-9c8b4f437cc7",
   "metadata": {},
   "source": [
    "###### 学習＆推論\n",
    "- Import\n",
    "  - [dataset/mnist.py](Kasago/dataset/mnist.py)\n",
    "  - [common/functions.py](Kasago/common/functions.py)\n",
    "  - [common/gradient.py](Kasago/common/gradient.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f086a-28d9-42b8-a04e-cc38c6a85bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.functions import *\n",
    "from kasago.common.gradient import numerical_gradient\n",
    "\n",
    "# ２層ニューラルネットワーク・クラス\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # パラメタ（重みとバイアス）の初期化\n",
    "        self.params = {}\n",
    "        \n",
    "        # ランダムな重み\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        \n",
    "        # バイアス（０\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 推論\n",
    "        \n",
    "        # 重み\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        \n",
    "        # バイアス\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        # 隠れ層\n",
    "        # ベクトルと重みの積算＋バイアス\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        # 活性化関数（シグモイド関数\n",
    "        z1 = sigmoid(a1)\n",
    "        \n",
    "        # 出力層\n",
    "        # ベクトルと重みの積算＋バイアス\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        # 活性化関数（ソフトマックス関数\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):        \n",
    "        # 勾配計算用関数\n",
    "        \n",
    "        # 推論\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # 損失関数\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        # 正解率計算用関数\n",
    "        \n",
    "        # 推論（分類\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        # y, tを揃える\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        # 正解率の計算\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    # 学習（確認用）\n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        # 数値微分による勾配計算により、\n",
    "        # predictとLearnを行う。\n",
    "        \n",
    "        # 勾配計算用関数\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # 勾配計算\n",
    "        # ヤヤコシイが\n",
    "        #  - ココのnumerical_gradientはself.numerical_gradientじゃない。\n",
    "        #  - なお、self.predictは、self.lossの中で実行されている。\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    # 学習（高速版）\n",
    "    # x:入力データ, t:教師データ\n",
    "    def gradient(self, x, t):\n",
    "        # 誤差逆伝播法による勾配計算により、\n",
    "        # predictとLearnを行う。\n",
    "        \n",
    "        # 勾配の計算\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        # forward（predict\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward（Learn\n",
    "        grads = {}\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        # 出力層 --------------------\n",
    "        # softmaxレイヤの逆伝播\n",
    "        dy = (y - t) / batch_num\n",
    "        \n",
    "        # Affineレイヤの逆伝播\n",
    "        # np.dot(X, W) + B\n",
    "        ## 重み W\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        ## バイアス B\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        ## 入力 X\n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        # ---------------------------\n",
    "        \n",
    "        # 中間層 --------------------\n",
    "        # sigmoidレイヤの逆伝播\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        \n",
    "        # Affineレイヤの逆伝播\n",
    "        # np.dot(X, W) + B\n",
    "        ## 重み W\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        ## バイアス B\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "        ## 入力 X\n",
    "        # これ以上、逆伝播シないので計算不要\n",
    "        # ---------------------------\n",
    "\n",
    "        return grads\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# ２層ニューラルネットワークの初期化\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# パイパーパラメタ\n",
    "\n",
    "# 繰り返し回数を適宜設定する\n",
    "iters_num = 10000\n",
    "# 学習率\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 訓練データサイズ\n",
    "train_size = x_train.shape[0]\n",
    "# バッチサイズ\n",
    "batch_size = 100\n",
    "# サブセット数（イテレーション数）\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# データサイズ / バッチサイズ = サブセット数になる。\n",
    "# サブセット数は、そのまま、単位エポック毎のイテレーション数になる。\n",
    "# 繰り返し回数をサブセット数（イテレーション数）で割るとエポック数になる。\n",
    "# 繰り返し回数 * バッチサイズ = 実際に学習するデータの件数になる。\n",
    "print(\"iters_num = %d\" % iters_num)\n",
    "print(\"train_size : batch_size = %d : %d\" % (train_size, batch_size))\n",
    "print(\"iter_per_epoch ≒ train_size / batch_size = %d\" % iter_per_epoch)\n",
    "print(\"epoch_num = iters_num / iter_per_epoch = %d\" % (iters_num / iter_per_epoch))\n",
    "\n",
    "# 結果を格納する変数\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for i in range(iters_num): # 繰り返し回数\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 学習\n",
    "    \n",
    "    # 数値微分は、都度、推論が必要なので遅過ぎて使えない。\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # 誤差逆伝播法の実用可能な高速版\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメタ（重みとバイアス）の更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 損失の記録\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0: # エポック毎\n",
    "        \n",
    "        # 正解率の計算\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        # 正解率の記録\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "print(\"train and test have been completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117e322-8266-4649-838b-79649d4ed3a2",
   "metadata": {},
   "source": [
    "###### 推移の描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07ce32-92c2-4081-88bd-71bb845e4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 損失の推移の描画\n",
    "# タイトル\n",
    "plt.title('Cross Entropy Error', fontsize=20)\n",
    "# グリッド線\n",
    "plt.grid()\n",
    "# 折れ線グラフ\n",
    "plt.plot(np.arange(1, iters_num + 1), train_loss_list)\n",
    "# 軸ラベル\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "# 正解率の推移の描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "# 範囲\n",
    "plt.ylim(0, 1.0)\n",
    "# タイトル\n",
    "plt.title('Model accuracy', fontsize=20)\n",
    "# グリッド線\n",
    "plt.grid()\n",
    "# 折れ線グラフ\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "# 軸ラベル\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "# 凡例\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
