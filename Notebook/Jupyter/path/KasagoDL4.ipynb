{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f411ca43-7579-4ada-9c84-f8a4e38561d5",
   "metadata": {},
   "source": [
    "# o'reillyのカサゴ深層学習の本\n",
    "\n",
    "## ニューラルネットワークのオプション\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [勾配降下法アルゴリズムの選択](#勾配降下法アルゴリズムの選択)\n",
    "  - [アルゴリズム](#アルゴリズム)\n",
    "  - [検証用の曲面の説明](#検証用の曲面の説明)\n",
    "  - [実装と検証](#実装と検証)\n",
    "- [アクティベーション分布](#アクティベーション分布)\n",
    "  - [重み初期値](#重み初期値)\n",
    "  - [バッチ正規化_BatchNormalization](#バッチ正規化_BatchNormalization)\n",
    "- [過学習の対策＝汎化性能の向上](#過学習の対策＝汎化性能の向上)\n",
    "  - [過学習を荷重減衰による正則化で対策](#過学習を荷重減衰による正則化で対策)\n",
    "  - [過学習をドロップアウトで対策](#過学習をドロップアウトで対策)\n",
    "- [ハイパーパラメタのランダム・サーチ](#ハイパーパラメタのランダム・サーチ)\n",
    "\n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facae51-ad80-4d63-ba4b-ec1a151998dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 勾配降下法アルゴリズムの選択"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b8c4b-ffb6-4334-aa85-feb4d87a2bbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### アルゴリズム\n",
    "- [SGD](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#o2fe1f26)\n",
    "- [MomentumSGD](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#afd9f568)\n",
    "- [NesterovAG](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#ja838935)\n",
    "- [AdaGrad](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#u12d8d2f)\n",
    "- [RMSprop](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#kdd7cbe0)\n",
    "- [AdaDelta](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#dc5e4977)\n",
    "- [Adam](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#z75dbbb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bff0de-f9c4-44e4-a71d-882cff427a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 検証用の曲面の説明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb49366c-e2e5-49b5-b962-b02a07237b11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 曲面のイメージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c41dce-926c-41e6-a8a0-af19b4317d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# x0, x1 を -4 ～ +4 まで0.25刻み\n",
    "x = np.arange(-4, 4, 0.25)\n",
    "y = np.arange(-4, 4, 0.25)\n",
    "\n",
    "# 格子点（X, Y）\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# 格、格子点でのZ\n",
    "Z = X**2 / 20.0 + Y**2\n",
    "\n",
    "# ワイヤーフレーム図を作成\n",
    "fig = plt.figure(figsize=(8, 8)) # 図の設定\n",
    "ax = fig.add_subplot(projection='3d') # 3D用の設定\n",
    "ax.plot_wireframe(X, Y, Z) # ワイヤーフレーム図\n",
    "ax.set_xlabel('x') # x軸ラベル\n",
    "ax.set_ylabel('y') # y軸ラベル\n",
    "ax.set_zlabel('z') # z軸ラベル\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9abd35-bd97-4dd1-8b01-848ff26af228",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 曲面の偏微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70983e-2012-4c74-8a99-276a8eee07d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 偏微分して勾配を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f92a96-e68c-492a-a8d1-84f8862648b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 偏微分（勾配を返す）\n",
    "def numerical_gradient(f, X):\n",
    "    h = 1e-4  # 0.0001\n",
    "    \n",
    "    # 要素の値が0の同じ形式のn行２列の行列\n",
    "    grad1 = np.zeros_like(X)\n",
    "        \n",
    "    for idx1, x in enumerate(X):\n",
    "        \n",
    "        # 要素の値が0の同じ形式の２列のベクトル\n",
    "        grad2 = np.zeros_like(x)\n",
    "        \n",
    "        for idx2 in range(x.size):\n",
    "            tmp_val = x[idx2]\n",
    "            \n",
    "            x[idx2] = float(tmp_val) + h\n",
    "            fxh1 = f(x)  # f(x+h)\n",
    "            \n",
    "            x[idx2] = tmp_val - h \n",
    "            fxh2 = f(x)  # f(x-h)\n",
    "            \n",
    "            grad2[idx2] = (fxh1 - fxh2) / (2*h)\n",
    "            x[idx2] = tmp_val  # 値を元に戻す\n",
    "        \n",
    "        grad1[idx1] = grad2\n",
    "        \n",
    "    return grad1\n",
    "\n",
    "# 偏微分対象の関数（曲面）\n",
    "def function_3(x):\n",
    "    return x[0]**2 / 20.0 + x[1]**2\n",
    "\n",
    "# 曲面の格子点での勾配を計算\n",
    "# 格子点をベクトル化して\n",
    "_X = X.flatten()\n",
    "_Y = Y.flatten()\n",
    "# n行２列の全ペアにして（.Tは転置を意味）\n",
    "koushiten = np.array([_X, _Y]).T\n",
    "# 格子点での勾配を返す（.Tは転置を意味）\n",
    "grad = numerical_gradient(function_3, koushiten).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb13c7-21b7-4815-ada5-7615b4d8b487",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 結果の図示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa22c74-7ba5-4a33-a5ca-290549b1f7f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabd6dc-d4c1-40d7-a098-8ad7b7016e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図示 \n",
    "plt.figure()\n",
    "# ベクトル場を表示\n",
    "plt.quiver(_X, _Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "# 範囲\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "# 凡例\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9b722-5db6-499f-b4b4-156c9af5e929",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示２\n",
    "等高線を足してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97297b1d-e27d-4eb4-b8c0-d3f0b71e51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 図示 \n",
    "plt.figure()\n",
    "\n",
    "# 等高線を表示 --------------------\n",
    "plt.contourf(X, Y, Z, alpha=0.5)\n",
    "# ---------------------------------\n",
    "\n",
    "# ベクトル場を表示\n",
    "plt.quiver(_X, _Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
    "\n",
    "# 範囲\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "# 凡例\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "\n",
    "plt.grid()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4cff4-495d-46ea-8ae5-431555e3d64b",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 図示３\n",
    "曲面に勾配を書いてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf64d6-8713-4d40-a40d-d41d02ebece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格子点のX、Y軸方向の勾配\n",
    "dX = grad[0, :].reshape(X.shape)\n",
    "dY = grad[1, :].reshape(Y.shape)\n",
    "\n",
    "# Z軸方向の勾配を計算\n",
    "W = np.sqrt(dX**2 + dY**2)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8)) # 図の設定\n",
    "ax = fig.add_subplot(projection='3d') # 3D用の設定\n",
    "\n",
    "# 曲面\n",
    "ax.plot_wireframe(X, Y, Z) # ワイヤーフレーム図\n",
    "# 勾配\n",
    "ax.quiver(X, Y, Z, -dX/W, -dY/W, -W, \n",
    "          color='green', pivot='tail', arrow_length_ratio=0.1, length=0.5, label='grad') \n",
    "\n",
    "ax.set_xlabel('x') # x軸ラベル\n",
    "ax.set_ylabel('y') # y軸ラベル\n",
    "ax.set_zlabel('z') # z軸ラベル\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ca9d7-e05e-4ed0-b59b-2ceda137764e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 実装と検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e00be5-039e-44e6-ad1c-c93e89297152",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### アルゴリズム説明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb8909-cb07-4607-b9f3-aa18d04dde88",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### SGD\n",
    "勾配が最も急な向きに勾配を下る手法\n",
    "- 欠点の「最短距離でない、無駄に谷間を往復する。  \n",
    "学習率が大きい場合は無駄な往復が目立つ。」が顕著。\n",
    "- また、このケースでは他のアルゴリズムより学習率の初期値を  \n",
    "大きく設定しており、他のケースでのチューニングが難しい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f701e6-7b03-4965-ae51-9574067b35d5",
   "metadata": {},
   "source": [
    "##### Momentum\n",
    "[SGD](#SGD)に慣性（加速・減速）の概念を加えた最適化手法｡\n",
    "- 欠点の「加速しているため、極小値付近で止まり難い。」が顕著。\n",
    "- 確かに物理法則に則った様な動きをしており[SGD](#SGD)より効率的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d4f99-e1af-4724-87a7-8caccddef785",
   "metadata": {},
   "source": [
    "##### AdaGrad\n",
    "- 動いた量が増えたら更新が緩やかにする。\n",
    "- このブレーキ機能は学習率の減衰によって実現されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9975ea6e-237d-4893-a47d-8ccd5e82e6c6",
   "metadata": {},
   "source": [
    "##### Adam\n",
    "[Momentum](#Momentum)＋[AdaGrad](#AdaGrad)的な。\n",
    "- 慣性（加速・減速）とブレーキ機能を持ち合わせたアルゴリズム\n",
    "- AdaGradの方が効率的に見えるが、場合によっては最適解を発見できる可能性がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde41ad-59ed-4f0c-9dd2-7250326506d9",
   "metadata": {},
   "source": [
    "#### イメージの曲面で勾配降下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66a442-1c1e-4b38-8cdd-58ad9d17bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from kasago.common.optimizer import *\n",
    "\n",
    "# 曲面\n",
    "def f(x, y):\n",
    "    return x**2 / 20.0 + y**2\n",
    "\n",
    "# 勾配（微分）\n",
    "def df(x, y):\n",
    "    return x / 10.0, 2.0*y\n",
    "\n",
    "# 開始点\n",
    "init_pos = (-7.0, 2.0)\n",
    "\n",
    "# 現在地\n",
    "params = {}\n",
    "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "\n",
    "# 勾配\n",
    "grads = {}\n",
    "grads['x'], grads['y'] = 0, 0\n",
    "\n",
    "# 勾配降下法のアルゴリズム\n",
    "optimizers = OrderedDict()\n",
    "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
    "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
    "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
    "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
    "\n",
    "# 表のサイズなど\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# アルゴリズムを試す\n",
    "idx = 1\n",
    "for key in optimizers:\n",
    "    optimizer = optimizers[key]\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "    \n",
    "    # 勾配降下\n",
    "    for i in range(30):\n",
    "        x_history.append(params['x'])\n",
    "        y_history.append(params['y'])\n",
    "        \n",
    "        # 勾配\n",
    "        grads['x'], grads['y'] = df(params['x'], params['y'])\n",
    "        # 指定のアルゴリズムで（勾配降下\n",
    "        optimizer.update(params, grads)\n",
    "\n",
    "    # x, y を -10, -5 ～ 10, 5 まで0.01刻み\n",
    "    x = np.arange(-10, 10, 0.01)\n",
    "    y = np.arange(-5, 5, 0.01)\n",
    "    \n",
    "    # 格子点（X, Y）\n",
    "    X, Y = np.meshgrid(x, y) \n",
    "    # 格、格子点でのZ\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # for simple contour line  \n",
    "    mask = Z > 7\n",
    "    Z[mask] = 0\n",
    "    \n",
    "    # plot \n",
    "    plt.subplot(2, 2, idx)\n",
    "    idx += 1\n",
    "    plt.plot(x_history, y_history, 'o-', color=\"red\")\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.ylim(-6, 6)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.plot(0, 0, '+')\n",
    "    #colorbar()\n",
    "    #spring()\n",
    "    plt.title(key)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f9b05-6db4-4521-9b4d-114471ff38b9",
   "metadata": {},
   "source": [
    "#### 実際にMINST画像認識で勾配降下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb09f4-92ce-404a-8845-4f6f596e7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.util import smooth_curve\n",
    "from kasago.common.multi_layer_net import MultiLayerNet\n",
    "from kasago.common.optimizer import *\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# パイパーパラメタ\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "# アルゴリズム設定\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "# 1:実験の設定==========\n",
    "\n",
    "# モデルのリスト\n",
    "networks = {}\n",
    "\n",
    "# 損失の履歴\n",
    "train_loss = {}\n",
    "\n",
    "# アルゴリズム毎に異なるモデルを生成\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784,                        # 入力は28*28\n",
    "        hidden_size_list=[100, 100, 100, 100], # 中間層は100\n",
    "        output_size=10)                        # 出力は10クラス分類\n",
    "    train_loss[key] = []                       # 損失の履歴\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "# max_iterations回繰り返す。\n",
    "for i in range(max_iterations):\n",
    "    # train_sizeからbatch_size抜くインデックス\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # インデックスでXとYのバッチの訓練データを抜く\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # モデル毎に（アルゴリズムを替え）繰り返す。\n",
    "    for key in optimizers.keys():\n",
    "        # 勾配を求めて\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        # アルゴリズムで更新\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "        # 更新後の損失を求めて\n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        # 損失の履歴に格納\n",
    "        train_loss[key].append(loss)\n",
    "        \n",
    "    # 100回毎にアルゴリズム毎の損失を表示\n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "# 3.損失の履歴をグラフ描画==========\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e41efb-8dd5-4bbc-9a94-4cd60868e538",
   "metadata": {},
   "source": [
    "## アクティベーション分布\n",
    "- アクティベーションとは活性化関数の出力を言う。\n",
    "- このアクティベーションの分布が、\n",
    "  - 活性化関数の勾配０近辺に偏ると勾配消失が起きる。\n",
    "  - また、偏りがあると、表現力の制限問題が起きる。\n",
    "- これを回避するようアクティベーション分布をバラけさせる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d317c-b679-401e-8c35-df083cdeff8e",
   "metadata": {},
   "source": [
    "### 重み初期値\n",
    "アクティベーション分布をバラけさせるように重みを初期値する方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9479c19-2b6a-4af9-a053-0b678558f03a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ５層のアフィン変換でアクティベーション分布確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ac272-d666-4975-9947-8ff480ec2aa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### ５層のアフィン変換の関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fec60-3409-45f4-973e-33cb6f97a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 活性化関数\n",
    "## sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "## tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "## ReLU\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def activation_histogram(hidden_layer_size, activation_func, x, w, hight=7000, width=1):\n",
    "    # ここにアクティベーションの結果を格納する\n",
    "    activations = {}  \n",
    "\n",
    "    # 層の数だけアクティベーション\n",
    "    for i in range(hidden_layer_size):\n",
    "        # 前層の出力を取り出す。\n",
    "        if i != 0:\n",
    "            x = activations[i-1]\n",
    "        # 全結合\n",
    "        a = np.dot(x, w)\n",
    "        # 活性化関数\n",
    "        z = activation_func(a)\n",
    "        # 出力の保存\n",
    "        activations[i] = z\n",
    "\n",
    "    # ヒストグラムを描画\n",
    "    for i, a in activations.items():\n",
    "        plt.subplot(1, len(activations), i+1)\n",
    "        plt.title(str(i+1) + \"-layer\")\n",
    "        if i != 0: plt.yticks([], [])\n",
    "        plt.ylim(0, hight)\n",
    "        plt.xlim(0, width)\n",
    "        plt.hist(a.flatten(), 30, range=(0,1))\n",
    "        \n",
    "    # ヒストグラムを表示\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b6b4c-77dd-4597-8a51-999a46c9301d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 実行準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6eeed7-259e-4b5b-9d15-a60d6be582f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 5 # 隠れ層の数\n",
    "node_num = 100        # 層のノード数\n",
    "\n",
    "# 1000行node_num列の正規分布の乱数\n",
    "input_data = np.random.randn(1000, node_num)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bcb85-6c5f-450c-a72c-b461a3ae9d28",
   "metadata": {},
   "source": [
    "##### sigmoidの場合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ac7da-c820-4825-b1e8-c0edc7df3e3c",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差１\n",
    "０・１に偏る。表現力問題と勾配消失が起きる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7bf80-f7e7-4a8f-b4f4-dd8a1d167a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = sigmoid\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 1\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ac7df-9a97-4e77-ae3a-deb4c873acd2",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差0.01\n",
    "0.5に偏る。勾配消失は防いだが表現力問題が残る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4142fe0-6e78-497b-b7df-5d6a5568842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = sigmoid\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 0.01\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3734ee-b98d-4b4a-baf6-a166a230f53b",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差 Xavier\n",
    "- 広がりを持った分布で勾配消失や表現力問題が解消する。\n",
    "- 出力層に近づくにつれて若干、歪な分布になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cae264-e597-48ed-9053-00e17328bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = sigmoid\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51c1e0-b3e8-4c45-9616-28fbf7e6b36d",
   "metadata": {},
   "source": [
    "##### tanhの場合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74309c8-af9b-45b5-9d9b-ef02ce170f0f",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5153fc0-7538-4211-9fa2-64c03860363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = tanh\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 1\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb4cda-5b2c-4b01-9c10-d09bc5abef34",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429eb41-deba-4afb-a891-ffc64f89ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = tanh\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 0.01\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13df2c-3c65-4345-abb2-350cf9dfee08",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差 Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7291e-ce56-4a02-933f-58717c6ab7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = tanh\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561be56c-1859-4c71-8ff4-dacddf49523c",
   "metadata": {},
   "source": [
    "##### ReLUの場合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66b13a-32c8-42d3-9f6f-0090e14d5b34",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差１\n",
    "- マイナスは全て０なので偏り、深くなると更に偏る。\n",
    "- 表現力問題と勾配消失が起きる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7981d-21c9-40e7-8b51-0b851d7e16f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = ReLU\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 1\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6dfab8-ebd4-48a2-b50b-66be05675451",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差0.01\n",
    "標準偏差１より問題は若干緩和するが解決はしていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f3043-d909-4847-8b3d-4c07c4c471de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = ReLU\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * 0.01\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a1b19-e2f5-4636-8841-44b026a3b3c7",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差 Xavier\n",
    "偏りが解消されつつある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f813c-f973-4beb-ab82-5dd13adda056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = ReLU\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8a815-f955-436c-bd3c-4f80d1273e53",
   "metadata": {},
   "source": [
    "###### 重みの標準偏差 He\n",
    "偏りが解消された。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46d8a4-49f2-4812-899b-1f815a5d1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 活性化関数\n",
    "activation_func = ReLU\n",
    "# 重みの初期値\n",
    "w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "\n",
    "activation_histogram(hidden_layer_size, activation_func, input_data, w, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf937d-2f02-4466-ae3d-accb5a8f36ee",
   "metadata": {},
   "source": [
    "#### 実際にMINST画像認識で３つの重み初期値をテスト\n",
    "活性化関数はReLUなので、\n",
    "- Heの初期化が一番上手く学習できる。\n",
    "- 次いで、Xavierで上手く学習できる。\n",
    "- 標準偏差0.01では勾配消失で学習できない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61313665-38b1-4cfe-a460-4ce360c0ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.util import smooth_curve\n",
    "from kasago.common.multi_layer_net import MultiLayerNet\n",
    "from kasago.common.optimizer import SGD\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# パイパーパラメタ\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "# 重み初期化方法の設定\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "# 1:実験の設定==========\n",
    "\n",
    "# モデルのリスト\n",
    "networks = {}\n",
    "\n",
    "# 損失の履歴\n",
    "train_loss = {}\n",
    "\n",
    "# 重み初期化方法の異なるモデルの生成\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784,                        # 入力は28*28\n",
    "        hidden_size_list=[100, 100, 100, 100], # 中間層は100\n",
    "        output_size=10,                        # 出力は10クラス分類\n",
    "        weight_init_std=weight_type)           # 重み初期化方法の設定\n",
    "    train_loss[key] = []                       # 損失の履歴\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "# max_iterations回繰り返す。\n",
    "for i in range(max_iterations):\n",
    "    # train_sizeからbatch_size抜くインデックス\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # インデックスでXとYのバッチの訓練データを抜く\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 重み初期化方法の異なるモデル毎に繰り返す。\n",
    "    for key in weight_init_types.keys():\n",
    "        # 勾配を求めて\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        # アルゴリズムで更新\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "        # 更新後の損失を求めて\n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        # 損失の履歴に格納\n",
    "        train_loss[key].append(loss)\n",
    "        \n",
    "    # 100回毎にアルゴリズム毎の損失を表示\n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "            \n",
    "# 3.損失の履歴をグラフ描画==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5080d-e412-40e0-8511-a3eeeb6280e0",
   "metadata": {},
   "source": [
    "### [バッチ正規化_BatchNormalization](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#t3c6ab88)\n",
    "- [重み初期値](#重み初期値)ではなく、各層で強制的にアクティベーションう分布を調整する。\n",
    "- MultiLayerNetExtendの実装では、全ての全結合と活性化関数の間にバッチ正規化層が挿入されるもよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d963eb1-8ffb-4d0e-a49e-049df12e33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from kasago.common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from kasago.common.optimizer import SGD, Adam\n",
    "\n",
    "def __train(weight_init_std, x_train, y_train):\n",
    "    \n",
    "    # 1:実験の設定（重み初期化方法の設定）==========\n",
    "\n",
    "    # ハイパーパラメタ\n",
    "    train_size = x_train.shape[0]\n",
    "    batch_size = 100\n",
    "    max_epochs = 20\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    learning_rate = 0.01\n",
    "    optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "    # バッチ正規化の有無の異なるモデルの生成\n",
    "    ## バッチ正規化ナシ\n",
    "    network1 = MultiLayerNetExtend(\n",
    "        input_size=784,\n",
    "        hidden_size_list=[100, 100, 100, 100, 100],\n",
    "        output_size=10,\n",
    "        weight_init_std=weight_init_std)\n",
    "    \n",
    "    ## バッチ正規化アリ\n",
    "    network2 = MultiLayerNetExtend(\n",
    "        input_size=784,\n",
    "        hidden_size_list=[100, 100, 100, 100, 100],\n",
    "        output_size=10, \n",
    "        weight_init_std=weight_init_std,\n",
    "        use_batchnorm=True)\n",
    "    \n",
    "    # acc履歴\n",
    "    train_acc_list1 = []\n",
    "    train_acc_list2 = []\n",
    "    \n",
    "    # 2:訓練の開始==========\n",
    "    epoch_cnt = 0\n",
    "    # max_iterationsじゃなくて1000000000回繰り返す。\n",
    "    for i in range(1000000000):\n",
    "        # train_sizeからbatch_size抜くインデックス\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        # インデックスでXとYのバッチの訓練データを抜く\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "        \n",
    "        # モデル毎に繰り返す。\n",
    "        for _network in (network1, network2):\n",
    "            # 勾配を求めて\n",
    "            grads = _network.gradient(x_batch, t_batch)\n",
    "            # アルゴリズムで更新\n",
    "            optimizer.update(_network.params, grads)\n",
    "            \n",
    "        # エポック毎に\n",
    "        if i % iter_per_epoch == 0:\n",
    "            # ACCを求めて\n",
    "            train_acc1 = network1.accuracy(x_train, t_train)\n",
    "            train_acc2 = network2.accuracy(x_train, t_train)\n",
    "            # ACC履歴に格納\n",
    "            train_acc_list1.append(train_acc1)\n",
    "            train_acc_list2.append(train_acc2)\n",
    "            # ACCログ表示\n",
    "            print(\"epoch:\" + str(epoch_cnt)  + \", train acc1-2 : \" + str(train_acc1) + \" - \" + str(train_acc2))\n",
    "            # エポック超えたら完了\n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_epochs:\n",
    "                break\n",
    "                \n",
    "    return train_acc_list1, train_acc_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b73c8-b9d9-401f-9dc2-43df3ed45855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 学習データを削減\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "# 重みの初期値のリスト（0-0.0001の対数スケールが等間隔の数値16個）\n",
    "weight_scale_list = np.logspace(0, -4, num=16)\n",
    "\n",
    "for i, w in enumerate(weight_scale_list):\n",
    "    print(\"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "\n",
    "    # 1:実験の設定（重み初期化方法の設定）==========\n",
    "    # 2:訓練の開始==========\n",
    "    train_acc_list1, train_acc_list2 = __train(w, x_train, t_train)\n",
    "    \n",
    "    # 3.グラフの描画==========\n",
    "    # subplot\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    # タイトルに重みの初期値\n",
    "    plt.title(\"W:\" + str(w), fontsize=6)\n",
    "    \n",
    "    # x軸はエポックの意\n",
    "    x = np.arange(len(train_acc_list1))\n",
    "\n",
    "    if i == 15:\n",
    "        # 凡例\n",
    "        plt.plot(x, train_acc_list1, color='orange', linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
    "        plt.plot(x, train_acc_list2, color='blue',                     label='Batch Normalization', markevery=2)\n",
    "        \n",
    "    else:\n",
    "        # subplot\n",
    "        plt.plot(x, train_acc_list1, color='orange', linestyle=\"--\", markevery=2)\n",
    "        plt.plot(x, train_acc_list2, color='blue',                   markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # 最左部にaccuracy、最下部にepochs\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    \n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "        \n",
    "plt.legend(loc='lower right')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335b8aa-7b84-477d-b19d-3aaf9b60fe75",
   "metadata": {},
   "source": [
    "## 過学習の対策＝汎化性能の向上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7c8d6-63f8-42b6-b140-8997ba823942",
   "metadata": {},
   "source": [
    "### 過学習を荷重減衰による正則化で対策\n",
    "[正則化回帰分析](ScikitLearnTraining2.ipynb)のような正則化項を使った方法らしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76794ef2-e270-4f90-ab9f-ae889359d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.multi_layer_net import MultiLayerNet\n",
    "from kasago.common.optimizer import SGD\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# パイパーパラメタ\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "max_epochs = 201\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "learning_rate = 0.01\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# weight decay（荷重減衰）の設定 =======================\n",
    "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "# 1:実験の設定（重み初期化方法の設定）==========\n",
    "\n",
    "# モデルの生成\n",
    "network1 = MultiLayerNet(\n",
    "    input_size=784,\n",
    "    hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "    output_size=10)\n",
    "\n",
    "network2 = MultiLayerNet(\n",
    "    input_size=784,\n",
    "    hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "    output_size=10,\n",
    "    weight_decay_lambda=weight_decay_lambda)\n",
    "\n",
    "# ACCの履歴\n",
    "train_acc_list1 = []\n",
    "test_acc_list1 = []\n",
    "train_acc_list2 = []\n",
    "test_acc_list2 = []\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "epoch_cnt = 0\n",
    "# max_iterationsじゃなくて1000000000回繰り返す。\n",
    "for i in range(1000000000):\n",
    "    # train_sizeからbatch_size抜くインデックス\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # インデックスでXとYのバッチの訓練データを抜く\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配を求めて\n",
    "    grads1 = network1.gradient(x_batch, t_batch)\n",
    "    grads2 = network2.gradient(x_batch, t_batch)\n",
    "    # 勾配を求めて\n",
    "    optimizer.update(network1.params, grads1)\n",
    "    optimizer.update(network2.params, grads2)\n",
    "    \n",
    "    # エポック毎に\n",
    "    if i % iter_per_epoch == 0:\n",
    "        # ACCを求めて\n",
    "        train_acc1 = network1.accuracy(x_train, t_train)\n",
    "        test_acc1 = network1.accuracy(x_test, t_test)\n",
    "        train_acc2 = network2.accuracy(x_train, t_train)\n",
    "        test_acc2 = network2.accuracy(x_test, t_test)\n",
    "        # ACC履歴に格納\n",
    "        train_acc_list1.append(train_acc1)\n",
    "        test_acc_list1.append(test_acc1)\n",
    "        train_acc_list2.append(train_acc2)\n",
    "        test_acc_list2.append(test_acc2)\n",
    "        # ACCログ表示\n",
    "        print(\"nw1 epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc1) + \", test acc:\" + str(test_acc1))\n",
    "        print(\"nw2 epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc2) + \", test acc:\" + str(test_acc2))\n",
    "        # エポック超えたら完了\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "# 3.グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "# subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, train_acc_list1, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list1, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, train_acc_list2, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list2, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "#plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5417258-01a0-4207-9cdc-b44931a6e343",
   "metadata": {},
   "source": [
    "### 過学習を[ドロップアウト](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF#hf3c7a28)で対策\n",
    "MultiLayerNetExtendの実装では、全ての活性化関数の後ろにドロップアウト層が挿入されるもよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1005e7-9e6c-4533-9076-341ad9efbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from kasago.common.trainer import Trainer\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# パイパーパラメタ\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "# 1:実験の設定==========\n",
    "network1 = MultiLayerNetExtend(\n",
    "    input_size=784,\n",
    "    hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "    output_size=10)\n",
    "\n",
    "network2 = MultiLayerNetExtend(\n",
    "    input_size=784,\n",
    "    hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "    output_size=10,\n",
    "    use_dropout=use_dropout,\n",
    "    dropout_ration=dropout_ratio)\n",
    "\n",
    "# なにやらTrainerなるクラスが使えるもよう。\n",
    "trainer1 = Trainer(network1, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "\n",
    "trainer2 = Trainer(network2, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "\n",
    "# 2:訓練の開始==========\n",
    "trainer1.train()\n",
    "trainer2.train()\n",
    "\n",
    "train_acc_list1, test_acc_list1 = trainer1.train_acc_list, trainer1.test_acc_list\n",
    "train_acc_list2, test_acc_list2 = trainer2.train_acc_list, trainer2.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list1))\n",
    "\n",
    "# subplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, train_acc_list1, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list1, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, train_acc_list2, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list2, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "#plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd70898-dc4c-4835-b3a5-590413175458",
   "metadata": {},
   "source": [
    "## ハイパーパラメタのランダム・サーチ\n",
    "[機械学習の所で出てきたグリッドサーチ](ScikitLearnTraining5.ipynb)ならぬ、ランダムサーチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98d440-3cd8-4bc3-9b92-42bdcd9cbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.multi_layer_net import MultiLayerNet\n",
    "from kasago.common.util import shuffle_dataset\n",
    "from kasago.common.trainer import Trainer\n",
    "\n",
    "# 0:MNISTデータの読み込み==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 高速化のため訓練データの削減\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 検証データの分離\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    \n",
    "    # 1:実験の設定==========\n",
    "    network = MultiLayerNet(\n",
    "        input_size=784,\n",
    "        hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "        output_size=10, weight_decay_lambda=weight_decay)\n",
    "    \n",
    "    # なにやらTrainerなるクラスが使えるもよう。\n",
    "    # 検証データとは言うモノのIF上ではテストデータに指定する。\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    \n",
    "    # 2:訓練の開始==========\n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "# ハイパーパラメータのランダム探索======================================\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "optimization_trial = 100\n",
    "\n",
    "# optimization_trial回繰り返す。\n",
    "for _ in range(optimization_trial):\n",
    "    # ハイパーパラメータの範囲を指定===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    # 1:実験の設定（探索するハイパーパラメータを設定）==========\n",
    "    # 2:訓練の開始==========\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    # ACC履歴に格納\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "    # ACCログ表示\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "\n",
    "# グラフの描画========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20 # 上位20件表示\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num)) # 切り捨て\n",
    "\n",
    "i = 0\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    # 降順、lambdaは何をキーにsortするかで、keyに対応するACC履歴の末尾の値でソート\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    # subplot\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    \n",
    "    i += 1\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
