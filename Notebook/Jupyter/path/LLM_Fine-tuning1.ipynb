{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed99841d-0c3b-425b-a9d0-1473901b3b98",
   "metadata": {},
   "source": [
    "# Hugging Face Hub の Phi-3-mini モデル の LoRAによる効率的なファインチューニング手順\n",
    "- Pythonコード生成に特化させる。\n",
    "- https://github.com/microsoft/PhiCookBook/blob/main/code/04.Finetuning/Phi-3-finetune-lora-python.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e193ec-f1a9-45c8-b457-360726aed3d6",
   "metadata": {},
   "source": [
    "## インストールとインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1df0a5-80c6-4a02-82fc-d1e7eed900e2",
   "metadata": {},
   "source": [
    "### ライブラリのインストールと読み込み\n",
    "Installing and loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385b0a5-0d24-495b-9516-fb564f1f10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is used to install and upgrade several Python packages using pip, Python's package installer.\n",
    "# The '!' at the beginning allows us to run shell commands in the notebook.\n",
    "# The '-qqq' option is used to make the installation process less verbose.\n",
    "# '--upgrade' is used to ensure that the packages are upgraded to their latest versions if they are already installed.\n",
    "# The packages being installed are:\n",
    "# 'bitsandbytes' for efficient gradient accumulation,\n",
    "# 'transformers' for using transformer models like Phi-3,\n",
    "# 'peft' for efficient fine-tuning,\n",
    "# 'accelerate' for easy distributed training,\n",
    "# 'datasets' for loading and preprocessing datasets,\n",
    "# 'trl' for reinforcement learning,\n",
    "# 'flash_attn' for attention-based models.\n",
    "# 'wandb' stands for Weights & Biases. It is a tool for machine learning experiment tracking, dataset versioning, and model management. It allows you to log and visualize metrics from your code, share findings, and reproduce experiments.\n",
    "# 'torch' is a package that provides an open-source machine learning library used for building deep learning models.\n",
    "\n",
    "# このコマンドは、pipを使用して複数の Python パッケージをインストールおよびアップグレードする\n",
    "# ・先頭の '!' は、ノートブック上でシェルコマンドを実行するための記号\n",
    "#   ・'-qqq' オプションは、インストール処理の出力を抑えて静かに実行させるためのものです。\n",
    "#   ・'--upgrade' は、すでにインストールされているパッケージがある場合に、それを最新バージョンにアップグレードするために使用されます。\n",
    "# ・インストール対象のパッケージは以下の通りです：\n",
    "#   ・'bitsandbytes'：効率的な勾配蓄積のためのパッケージ\n",
    "#   ・'transformers'：Phi-3 のような Transformer モデルを使用するためのライブラリ\n",
    "#   ・'peft'：効率的なファインチューニングのためのライブラリ\n",
    "#   ・'accelerate'：分散学習を簡単に行うためのライブラリ\n",
    "#   ・'datasets'：データセットの読み込みと前処理のためのライブラリ\n",
    "#   ・'trl'：強化学習（Reinforcement Learning）用のライブラリ\n",
    "#   ・'flash_attn'：Attention ベースのモデルに関する処理のためのライブラリ\n",
    "#   ・'wandb'：Weights & Biases の略で、機械学習の実験追跡、データセットのバージョン管理、モデル管理などを行うツール。コードからメトリクスを記録・可視化し、結果の共有や実験の再現を可能にします。\n",
    "#   ・'torch'：深層学習モデルを構築するためのオープンソースの機械学習ライブラリ\n",
    "# !pip install -qqq --upgrade bitsandbytes transformers peft accelerate datasets trl flash_attn torch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cbc96-35af-4337-9c5d-78d64e237b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# そんな簡単にインストールできないのでバラして実行\n",
    "!pip install --upgrade bitsandbytes\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade peft\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade datasets\n",
    "!pip install --upgrade trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e7166-aa77-4b41-9069-afc805a277f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CUDAインストール環境にインストール\n",
    "# pipとsetuptoolsをアップグレード\n",
    "!pip install --upgrade pip setuptools\n",
    "\n",
    "# --no-build-isolationは、この事前ビルド済みのものをダウンロードしてきてインストール\n",
    "# 環境内のPython/CUDA/PyTorchなどのバージョンをチェックして選択している模様\n",
    "# !pip install flash_attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd4ca4-a363-4214-bd49-494c5fa39b3e",
   "metadata": {},
   "source": [
    "### 様々な問題対応\n",
    "ビルドを並列実行する方法と、ビルド済みのものを持ってくる方法があるらしい。\n",
    "\n",
    "#### ビルドを並列実行する方法\n",
    "- [bug] build is verrrrrrrrrrrrrrrrrrrry slow · Issue #945 · Dao-AILab/flash-attention  \n",
    "https://github.com/Dao-AILab/flash-attention/issues/945\n",
    "```bash\n",
    "$ top -d 10\n",
    "top - 06:28:47 up  3:37,  2 users,  load average: 1.99, 2.01, 2.01\n",
    "Tasks: 306 total,   3 running, 303 sleeping,   0 stopped,   0 zombie\n",
    "%Cpu(s): 25.6 us,  0.3 sy,  0.0 ni, 74.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st \n",
    "MiB Mem :  56191.6 total,  43355.9 free,   8707.5 used,   4902.0 buff/cache     \n",
    "MiB Swap:      0.0 total,      0.0 free,      0.0 used.  47484.1 avail Mem \n",
    "\n",
    "    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                               \n",
    "   7944 seigi     20   0 7618672   1.8g  38648 R 100.0   3.3   2:52.44 cicc                                  \n",
    "   7945 seigi     20   0 7618672   1.8g  38656 R 100.0   3.3   2:52.42 cicc                                  \n",
    "   2657 xrdp      20   0   40156  28668   8856 S   2.6   0.0   1:31.88 xrdp                                  \n",
    "   1854 seigi     20   0  485348 184864  87076 S   1.0   0.3   0:38.83 Xorg  \n",
    "```\n",
    "- CUDAプログラムのビルド中。ciccプロセスがCPUをフルに使っているが、全体としてCPUにもメモリにもまだ余裕がある。\n",
    "- ロードアベレージは高め（2.0以上）、コア数が2以上あるなら並列数を上げると良い可能性がある。\n",
    "- `export MAX_JOBS=N`で、ninjaビルドシステムが使用できる CPU コアを指定することができるらしい。\n",
    "- `Standard_NC8as_T4_v3`のvCPU数は8らしい。`lscpu`で`CPU(s):8`、`On-line CPU(s) list:0-7`\n",
    "```bash\n",
    "!pip install packaging\n",
    "!pip install ninja\n",
    "!MAX_JOBS=8 pip install flash-attn --no-build-isolation\n",
    "```\n",
    "#### ビルド済みのものを持ってくる方法\n",
    "- ColaboratoryでFlashAttentionをインストールするとめちゃめちゃ時間がかかっていた（過去）  \n",
    "https://zenn.dev/kun432/scraps/98b51455d20927\n",
    "  - https://x.com/mjun0812/status/1850706663982137615\n",
    "  - https://github.com/mjun0812/flash-attention-prebuild-wheels\n",
    "  - https://github.com/kun432/flash-attention-prebuild-wheels\n",
    "```bash\n",
    "pip install https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.7.4+cu128torch2.7-cp312-cp312-linux_x86_64.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a094d3b-2b1d-48d4-a59d-c4dfb7c79be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "!pip install packaging\n",
    "!pip install ninja\n",
    "# \n",
    "!pip cache purge\n",
    "!mkdir -p $mnt/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb731a2-362c-46da-9b42-dce712939efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「error: [Errno 28] No space left on device」の様なエラーになったので「TMPDIR」を追加\n",
    "!TMPDIR=$mnt/tmp MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf5e82-24ab-4fd5-96e0-9f9775f7eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5bb6e-13d2-46d6-b53f-a58e3010c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These commands are used to install two Python packages using pip, Python's package installer.\n",
    "# The '!' at the beginning allows us to run shell commands in the notebook.\n",
    "# 'huggingface_hub' is a library developed by Hugging Face that allows you to interact with the Hugging Face Model Hub.\n",
    "# It provides functionalities to download and upload models, as well as other utilities.\n",
    "\n",
    "# pip を使って 2 つのパッケージをインストールする\n",
    "# 'huggingface_hub' は Hugging Face Model Hub からの モデルのダウンロードやアップロード、その他のユーティリティ機能を提供\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# 'python-dotenv' is a library that allows you to specify environment variables in a .env file.\n",
    "# It's useful for managing secrets and configuration settings for your application.\n",
    "\n",
    "# 'python-dotenv' は、.env ファイルに環境変数を指定できるようにするライブラリでアプリケーションの機密情報や設定を管理するのに便利\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50b95e-b562-43b3-bf77-3b843b20b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is used to install three Python packages using pip, Python's package installer.\n",
    "# The '!' at the beginning allows us to run shell commands in the notebook.\n",
    "# 'absl-py' is a library developed by Google that provides several utilities for Python development, such as logging and command line argument parsing.\n",
    "# 'nltk' stands for Natural Language Toolkit. It is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
    "# 'rouge_score' is a library for calculating the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score, which is commonly used for evaluating automatic summarization and machine translation systems.\n",
    "\n",
    "# このコマンドは、pipdで3つのPythonパッケージをインストールする。\n",
    "# ・'absl-py' はGoogleが開発したライブラリで、ロギングやコマンドライン引数のパースなど、Python開発向けのさまざまなユーティリティを提供\n",
    "# ・'nltk' は Natural Language Toolkit の略で、人間の言語データを扱うPythonプログラムを構築するための代表的なプラットフォーム。50以上のコーパスや語彙資源に簡単にアクセスできるIFを提供。\n",
    "# ・'rouge_score' は ROUGE（Recall-Oriented Understudy for Gisting Evaluation）スコアを計算するためのライブラリで、自動要約や機械翻訳システムの評価によく用いられる。\n",
    "!pip install absl-py nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "500e7d07-4341-4c7b-b57f-157baee2b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers              4.52.3\n"
     ]
    }
   ],
   "source": [
    "# This command is used to list all installed Python packages and filter for the 'transformers' package.\n",
    "# The '!' at the beginning allows us to run shell commands in the notebook.\n",
    "# 'pip list' lists all installed Python packages.\n",
    "# The '|' character is a pipe. It takes the output from the command on its left (in this case, 'pip list') and passes it as input to the command on its right.\n",
    "# 'grep' is a command-line utility for searching plain-text data for lines that match a regular expression. Here it's used to filter the output of 'pip list' for lines that contain 'transformers.'.\n",
    "\n",
    "# このコマンドは、インストールされているすべての Python パッケージを一覧表示し、その中から 'transformers' パッケージをフィルタリングする\n",
    "# 先頭の '!' は、ノートブック内でシェルコマンドを実行する\n",
    "# 'pip list' は、インストールされているすべての Python パッケージを一覧表示\n",
    "# '|'（パイプ）は、左側のコマンド（この場合は 'pip list'）の出力を右側のコマンドに入力として渡す。\n",
    "# 'grep' は、正規表現に一致する行をプレーンテキストデータから検索するコマンドラインユーティリティ。\n",
    "# ここでは、'pip list' の出力から 'transformers' を含む行だけを抽出するために使われています。\n",
    "\n",
    "# So, this command will list details of the 'transformers' package if it's installed.\n",
    "# つまり、このコマンドを実行すると、'transformers' パッケージがインストールされている場合、その詳細が表示される。\n",
    "!pip list | grep transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fc7e02-9076-43e9-8875-8f03469e95de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "absl-py                   2.2.2\n",
      "accelerate                1.7.0\n",
      "aiohappyeyeballs          2.6.1\n",
      "aiohttp                   3.11.18\n",
      "aiosignal                 1.3.2\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.9.0\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.4\n",
      "bitsandbytes              0.45.5\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.4.26\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.2\n",
      "click                     8.2.1\n",
      "comm                      0.2.2\n",
      "datasets                  3.6.0\n",
      "debugpy                   1.8.14\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.8\n",
      "docker-pycreds            0.4.0\n",
      "einops                    0.8.1\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.13.1\n",
      "flash_attn                2.7.4.post1\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.6.0\n",
      "fsspec                    2024.6.1\n",
      "gitdb                     4.0.12\n",
      "GitPython                 3.1.44\n",
      "h11                       0.16.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.31.4\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   9.2.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "ipywidgets                8.1.7\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.5.1\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.2\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.15\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.1.3\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.4.4\n",
      "multiprocess              0.70.16\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.3\n",
      "ninja                     1.11.1.4\n",
      "nltk                      3.9.1\n",
      "notebook                  7.4.2\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.1.2\n",
      "nvidia-cublas-cu12        12.8.3.14\n",
      "nvidia-cuda-cupti-cu12    12.8.57\n",
      "nvidia-cuda-nvrtc-cu12    12.8.61\n",
      "nvidia-cuda-runtime-cu12  12.8.57\n",
      "nvidia-cudnn-cu12         9.7.1.26\n",
      "nvidia-cufft-cu12         11.3.3.41\n",
      "nvidia-cufile-cu12        1.13.0.11\n",
      "nvidia-curand-cu12        10.3.9.55\n",
      "nvidia-cusolver-cu12      11.7.2.55\n",
      "nvidia-cusparse-cu12      12.5.7.53\n",
      "nvidia-cusparselt-cu12    0.6.3\n",
      "nvidia-nccl-cu12          2.26.2\n",
      "nvidia-nvjitlink-cu12     12.8.61\n",
      "nvidia-nvtx-cu12          12.8.55\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "peft                      0.15.2\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.0.0\n",
      "pip                       25.1.1\n",
      "platformdirs              4.3.8\n",
      "prometheus_client         0.22.0\n",
      "prompt_toolkit            3.0.51\n",
      "propcache                 0.3.1\n",
      "protobuf                  6.31.0\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pyarrow                   20.0.0\n",
      "pycparser                 2.22\n",
      "pydantic                  2.11.5\n",
      "pydantic_core             2.33.2\n",
      "Pygments                  2.19.1\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.1.0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.4.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      14.0.0\n",
      "rouge_score               0.1.2\n",
      "rpds-py                   0.25.1\n",
      "safetensors               0.5.3\n",
      "Send2Trash                1.8.3\n",
      "sentry-sdk                2.29.1\n",
      "setproctitle              1.3.6\n",
      "setuptools                80.8.0\n",
      "six                       1.17.0\n",
      "smmap                     5.0.2\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "stack-data                0.6.3\n",
      "sympy                     1.13.3\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.1\n",
      "torch                     2.7.0+cu128\n",
      "torchaudio                2.7.0+cu128\n",
      "torchvision               0.22.0+cu128\n",
      "tornado                   6.5.1\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.52.3\n",
      "triton                    3.3.0\n",
      "trl                       0.17.0\n",
      "types-python-dateutil     2.9.0.20250516\n",
      "typing_extensions         4.12.2\n",
      "typing-inspection         0.4.1\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.4.0\n",
      "wandb                     0.19.11\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.14\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.20.0\n"
     ]
    }
   ],
   "source": [
    "# インストールされたパッケージを確認する。\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1d1fd-440b-4b7e-9feb-e3a6faae60d3",
   "metadata": {},
   "source": [
    "### ライブラリのインポート\n",
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32709428-7dd9-4d2a-81c5-206e24822b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is importing necessary modules and functions for fine-tuning a language model.\n",
    "# このコードブロックは、言語モデルのファインチューニングに必要なモジュールや関数をインポート\n",
    "\n",
    "# 'randrange' is a function from the 'random' module that generates a random number within the specified range.\n",
    "# 'randrange' は 'random' モジュールの関数で、指定された範囲内の乱数を生成\n",
    "from random import randrange\n",
    "\n",
    "# 'torch' is the PyTorch library, a popular open-source machine learning library for Python.\n",
    "# 'torch' は PyTorch ライブラリで、Python 向けの人気のあるオープンソースの機械学習ライブラリ\n",
    "import torch\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library by Hugging Face which allows you to load a dataset.\n",
    "# 'load_dataset' は、Hugging Face の 'datasets' ライブラリにある関数で、データセットを読み込むために使用\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 'LoraConfig' and 'prepare_model_for_kbit_training' are from the 'peft' library. \n",
    "# 'LoraConfig' is used to configure the LoRA (Learning from Random Architecture) model.\n",
    "# 'prepare_model_for_kbit_training' is a function that prepares a model for k-bit training.\n",
    "# 'TaskType' contains differenct types of tasks supported by PEFT\n",
    "# 'PeftModel' base model class for specifying the base Transformer model and configuration to apply a PEFT method to.\n",
    "\n",
    "# 「peft」ライブラリから提供\n",
    "# ・「LoraConfig」はLoRAモデルの構成を設定するために使われる。\n",
    "# ・「prepare_model_for_kbit_training」は、kビット訓練に対応するようモデルを準備する関数。\n",
    "# ・「TaskType」にはPEFTがサポートするさまざまなタスクの種類が含まれる。\n",
    "# ・「PeftModel」は、ベースとなるTransformerモデルと構成を指定してPEFT手法を適用するための基底モデルクラス。\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "\n",
    "# Several classes and functions are imported from the 'transformers' library by Hugging Face.\n",
    "# 'AutoModelForCausalLM' is a class that provides a generic transformer model for causal language modeling.\n",
    "# 'AutoTokenizer' is a class that provides a generic tokenizer class.\n",
    "# 'BitsAndBytesConfig' is a class for configuring the Bits and Bytes optimizer.\n",
    "# 'TrainingArguments' is a class that defines the arguments used for training a model.\n",
    "# 'set_seed' is a function that sets the seed for generating random numbers.\n",
    "# 'pipeline' is a function that creates a pipeline that can process data and make predictions.\n",
    "\n",
    "# Hugging Faceの「transformers」ライブラリから提供\n",
    "# ・'AutoModelForCausalLM'は因果言語モデリングのための汎用的なTransformerモデルを提供\n",
    "# ・'AutoTokenizer'は汎用的なTokenizerクラスを提供するクラス\n",
    "# ・'BitsAndBytesConfig'はBits and Bytes最適化器の設定を行うためのクラス\n",
    "# ・'TrainingArguments'はモデルの訓練に使用される引数を定義するクラス\n",
    "# ・'set_seed'はランダム数生成のシードを設定する関数\n",
    "# ・'pipeline'はデータを処理して予測を行うパイプラインを作成する関数\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# 'SFTTrainer' is a class from the 'trl' library that provides a trainer for soft fine-tuning.\n",
    "# 'SFTTrainer' は、ソフトファインチューニング用のトレーナーを提供する 'trl' ライブラリのクラス\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccae910-faa3-435a-95e5-1317ad5da301",
   "metadata": {},
   "source": [
    "## グローバルパラメータの設定\n",
    "Setting Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a424577-8a6e-45b6-899a-59a78ad2596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is setting up the configuration for fine-tuning a language model.\n",
    "# このコードブロックは、言語モデルのファインチューニングのための設定を行う。\n",
    "\n",
    "# 'model_id' and 'model_name' are the identifiers for the pre-trained model that you want to fine-tune. \n",
    "# In this case, it's the 'Phi-3-mini-4k-instruct' model from Microsoft.\n",
    "# 'model_id' および 'model_name' は、ファインチューニングしたい事前学習済みモデルの識別子\n",
    "# この場合は、Microsoft の 'Phi-3-mini-4k-instruct' モデル\n",
    "\n",
    "# Model Names \n",
    "# microsoft/Phi-3-mini-4k-instruct\n",
    "# microsoft/Phi-3-mini-128k-instruct\n",
    "# microsoft/Phi-3-small-8k-instruct\n",
    "# microsoft/Phi-3-small-128k-instruct\n",
    "# microsoft/Phi-3-medium-4k-instruct\n",
    "# microsoft/Phi-3-medium-128k-instruct\n",
    "# microsoft/Phi-3-vision-128k-instruct\n",
    "# microsoft/Phi-3-mini-4k-instruct-onnx\n",
    "# microsoft/Phi-3-mini-4k-instruct-onnx-web\n",
    "# microsoft/Phi-3-mini-128k-instruct-onnx\n",
    "# microsoft/Phi-3-small-8k-instruct-onnx-cuda\n",
    "# microsoft/Phi-3-small-128k-instruct-onnx-cuda\n",
    "# microsoft/Phi-3-medium-4k-instruct-onnx-cpu\n",
    "# microsoft/Phi-3-medium-4k-instruct-onnx-cuda\n",
    "# microsoft/Phi-3-medium-4k-instruct-onnx-directml\n",
    "# microsoft/Phi-3-medium-128k-instruct-onnx-cpu\n",
    "# microsoft/Phi-3-medium-128k-instruct-onnx-cuda\n",
    "# microsoft/Phi-3-medium-128k-instruct-onnx-directml\n",
    "# microsoft/Phi-3-mini-4k-instruct-gguf\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# 'dataset_name' is the identifier for the dataset that you want to use for fine-tuning. \n",
    "# In this case, it's the 'python_code_instructions_18k_alpaca' dataset from iamtarun (Ex: iamtarun/python_code_instructions_18k_alpaca).\n",
    "# Update Dataset Name to your dataset name\n",
    "\n",
    "# 'dataset_name' は、ファインチューニングに使用したいデータセットの識別子\n",
    "# \"iamtarun/python_code_instructions_18k_alpaca\"には、Python 言語で記述された問題の説明とコードが含まれる。\n",
    "# https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca\n",
    "dataset_name = \"iamtarun/python_code_instructions_18k_alpaca\" # \"Insert your dataset name here\"\n",
    "\n",
    "# 'dataset_split' is the split of the dataset that you want to use for training. \n",
    "# In this case, it's the 'train' split.\n",
    "# 'dataset_split' は、学習に使用したいデータセットの分割を指定\n",
    "# この場合は 'train'（訓練）分割（を使用するように指定）\n",
    "dataset_split = \"train\"\n",
    "\n",
    "# 'new_model' is the name that you want to give to the fine-tuned model.\n",
    "# 'new_model' はファインチューニングしたモデルに付けたい名前\n",
    "new_model = \"Phi-3-mini-4k-finetuned\"\n",
    "\n",
    "# 'hf_model_repo' is the repository on the Hugging Face Model Hub where the fine-tuned model will be saved. Update UserName to your Hugging Face Username\n",
    "# 'hf_model_repo' はファインチューニングされたモデルを保存するための Hugging Face Model Hub 上のリポジトリ。UserName を自身の Hugging Face ユーザー名に更新\n",
    "hf_userName = \"nishi74322014\"\n",
    "hf_model_repo = hf_userName + \"/\" + new_model\n",
    "\n",
    "# 'device_map' is a dictionary that maps the model to the GPU device. \n",
    "# In this case, the entire model is loaded on GPU 0.\n",
    "# 'device_map' は、モデルをGPUデバイスに割り当てる辞書\n",
    "# この場合、モデル全体がGPU 0に読み込まれます。\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# The following are parameters for the LoRA (Learning from Random Architecture) model.\n",
    "# 以下はLoRA（Learning from Random Architecture）モデルのパラメタ\n",
    "\n",
    "# 'lora_r' is the dimension of the LoRA attention.\n",
    "# 'lora_r' は LoRAアテンションの次元数\n",
    "lora_r = 16\n",
    "\n",
    "# 'lora_alpha' is the alpha parameter for LoRA scaling.\n",
    "# 'lora_alpha' は LoRA のスケーリングに用いられる α（アルファ）パラメタ\n",
    "lora_alpha = 16\n",
    "\n",
    "# 'lora_dropout' is the dropout probability for LoRA layers.\n",
    "# 'lora_dropout' は LoRA 層に対するドロップアウトの確率\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# 'target_modules' is a list of the modules in the model that will be replaced with LoRA layers.\n",
    "# 'target_modules' は、LoRA層に置き換えられるモデル内のモジュールのリスト\n",
    "target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "# 'set_seed' is a function that sets the seed for generating random numbers, which is used for reproducibility of the results.\n",
    "# 'set_seed' はランダムな数を生成するためのシードを設定する関数で結果の再現性を確保するために使用される。\n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a85a9e-206d-4e3b-b3d6-8f8a6a9d5844",
   "metadata": {},
   "source": [
    "## Huggingface Hubに接続する\n",
    "Connect to Huggingface Hub\n",
    "\n",
    "IMPORTANT: The upcoming section's execution will vary based on your code execution environment and the configuration of your API Keys.  \n",
    "重要: 次のセクションの実行は、コード実行環境と API キーの構成によって異なる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2e52c-7776-4faf-8876-666f897da695",
   "metadata": {},
   "source": [
    "### Hugging Face Hubへのインタラクティブログインが可能\n",
    "Interactive login to Hugging Face Hub is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a1fa4-5864-414d-ab64-c93c6bd0bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to log in to the Hugging Face Model Hub from a notebook.\n",
    "# このコードブロックは、notebookからHugging Face Model Hubにログインする。\n",
    "\n",
    "# 'notebook_login' is a function from the 'huggingface_hub' library that opens a new browser window \n",
    "# where you can log in to your Hugging Face account. After logging in, \n",
    "# your Hugging Face token will be stored in a configuration file on your machine, \n",
    "# which allows you to interact with the Hugging Face Model Hub from your notebook.\n",
    "\n",
    "# 'notebook_login'は、'huggingface_hub'ライブラリの関数で、新しいブラウザウィンドウを開き、 \n",
    "# そこでHugging Faceアカウントにログインできます。ログイン後、 \n",
    "# あなたのHugging Faceトークンは、あなたのマシンの設定ファイルに保存され、 \n",
    "# ノートブックからHugging Face Model Hubとやり取りできるようになる。\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Call the 'notebook_login' function to start the login process.\n",
    "# 'notebook_login' 関数を呼び出してログインプロセスを開始。\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab88a7-f6e6-45d3-9aed-57a01468d0b8",
   "metadata": {},
   "source": [
    "### または、Hugging Face トークンを含む .env ファイルを提供できる。\n",
    "Alternatively, you can supply a .env file that contains the Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defeb3f7-e573-46b3-94a2-d492d03100e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to log in to the Hugging Face Model Hub using an API token stored in an environment variable.\n",
    "# このコードブロックは、環境変数に格納されたAPIトークンを使用してHugging Face Model Hubにログインする。\n",
    "\n",
    "# 'login' is a function from the 'huggingface_hub' library that logs you in to the Hugging Face Model Hub using an API token.\n",
    "# 'login'は'huggingface_hub'ライブラリの関数で、APIトークンを使用してHugging Face Model Hubにログインする。\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 'load_dotenv' is a function from the 'python-dotenv' library that loads environment variables from a .env file.\n",
    "# 'load_dotenv' は 'python-dotenv' ライブラリの関数で、.env ファイルから環境変数をロード\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 'os' is a standard Python library that provides functions for interacting with the operating system.\n",
    "# 'os' は、オペレーティングシステムと対話するための標準の Python ライブラリ\n",
    "import os\n",
    "\n",
    "# Call the 'load_dotenv' function to load the environment variables from the .env file.\n",
    "# 'load_dotenv' 関数を呼び出して、.env ファイルから環境変数をロード\n",
    "load_dotenv()\n",
    "\n",
    "# Call the 'login' function with the 'HF_HUB_TOKEN' environment variable to log in to the Hugging Face Model Hub.\n",
    "# 'os.getenv' is a function that gets the value of an environment variable.\n",
    "# 'HF_HUB_TOKEN' 環境変数を使用して 'login' 関数を呼び出し、Hugging Face Model Hub にログイン\n",
    "# 'os.getenv' は環境変数の値を取得する関数\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491cf3e-b751-4a42-90fd-18760b2b94bf",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d1d8c-8fcc-4fac-a1dc-e2fbacbd0d1f",
   "metadata": {},
   "source": [
    "### 命令セット付きでデータセットを読み込む\n",
    "Load the dataset with the instruction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf0a46-eb87-48c3-9869-ef832e81ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load a dataset from the Hugging Face Dataset Hub, print its size, and show a random example from the dataset.\n",
    "# このコードブロックは、Hugging Face Dataset Hub からデータセットを読み込み、そのサイズ、ランダムなサンプルを表示\n",
    "\n",
    "# 'load_dataset' is a function from the 'datasets' library that loads a dataset from the Hugging Face Dataset Hub.\n",
    "# 'dataset_name' is the name of the dataset to load, and 'dataset_split' is the split of the dataset to load (e.g., 'train', 'test').\n",
    "# 'load_dataset' は 'datasets' ライブラリの関数で、Hugging Face Dataset Hub からデータセットを読み込む。\n",
    "# 'dataset_name' は読み込むデータセットの名前で、'dataset_split' は読み込むデータセット分割（例：'train', 'test'）を表す。\n",
    "dataset = load_dataset(dataset_name, split=dataset_split)\n",
    "\n",
    "# The 'len' function is used to get the size of the dataset, which is then printed.\n",
    "# 'len' 関数を使ってデータセットのサイズを取得し、それを表示。\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "\n",
    "# 'randrange' is a function from the 'random' module that generates a random number within the specified range.\n",
    "# Here it's used to select a random example from the dataset, which is then printed.\n",
    "# 'randrange' は 'random' モジュールの関数で、指定した範囲内のランダムな数値を生成\n",
    "# ここではデータセットからランダムなサンプルを選び、それを表示\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e911c-8f2f-4cc4-a7e5-0c6b8e3709a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code is used to display the structure of the 'dataset' object.\n",
    "# By simply writing the name of the object, Python will call its 'repr' (representation) method, \n",
    "# which returns a string that describes the object. \n",
    "# For a Hugging Face 'Dataset' object, this will typically show information such as the number of rows, \n",
    "# the column names, and the types of the data in each column.\n",
    "\n",
    "# このコード行は 'dataset' オブジェクトの構造を表示する\n",
    "# オブジェクトで、'repr'（表現）メソッドを呼び出し、オブジェクトを説明する文字列を返す。\n",
    "# Hugging Face の 'Dataset' オブジェクトの場合、通常は行数、列名、型の情報が表示される。\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e3393-4f0f-43aa-9481-71a14a0bd7dc",
   "metadata": {},
   "source": [
    "### トークナイザーを読み込んでデータセットを準備\n",
    "Load the tokenizer to prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5897926-e8ae-4a59-97b8-77be287dcb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load a tokenizer from the Hugging Face Model Hub.\n",
    "# このコードブロックは、Hugging Face Model Hub からトークナイザーを読み込む\n",
    "\n",
    "# 'tokenizer_id' is set to the 'model_id', which is the identifier for the pre-trained model.\n",
    "# This assumes that the tokenizer associated with the model has the same identifier as the model.\n",
    "# 'tokenizer_id' は 'model_id' に設定されています。これは事前学習済みモデルの識別子です。\n",
    "# モデルに関連付けられたトークナイザーがモデルと同じ識別子を持っていると仮定しています。\n",
    "tokenizer_id = model_id\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'tokenizer_id' is passed as an argument to specify which tokenizer to load.\n",
    "# 'AutoTokenizer.from_pretrained' は、Hugging Face Model Hub からトークナイザーを読み込むメソッドで\n",
    "# どのトークナイザーを読み込むかを指定するために 'tokenizer_id' が引数として渡される。\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# 'tokenizer.padding_side' is a property that specifies which side to pad when the input sequence is shorter than the maximum sequence length.\n",
    "# Setting it to 'right' means that padding tokens will be added to the right (end) of the sequence.\n",
    "# This is done to prevent warnings that can occur when the padding side is not explicitly set.\n",
    "# 'tokenizer.padding_side' は、入力シーケンスが最大長より短い場合に、どちら側にパディングを追加するかを指定するプロパティ。\n",
    "# 'right' に設定すると、パディングトークンがシーケンスの右（末尾）に追加\n",
    "# パディングの方向を明示的に設定しないと警告が出ることがあるため、この設定を行う。\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339aaa6-4a3f-4039-bc79-9452d1f12633",
   "metadata": {},
   "source": [
    "### データセットをChatML形式に適合させる。\n",
    "Function to create the appropiate format for our model.\n",
    "\n",
    "We are going to adapt our dataset to the ChatML format.  \n",
    "モデルに適した形式を作成する関数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ee4f8-6c18-45a1-8d0d-be8dfb6c64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines two functions that are used to format the dataset for training a chat model.\n",
    "# このコードブロックでは、チャットモデルの学習用データセットを整形するための2つの関数を定義\n",
    "\n",
    "# 'create_message_column' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'messages' key and a list of 'user' and 'assistant' messages as its value.\n",
    "# 'create_message_column' は、データセットの各行を受け取り 'messages' というキーを持つ辞書を返す。\n",
    "# この 'messages' の値は、'user' と 'assistant' のメッセージをリスト形式で格納したもの。\n",
    "def create_message_column(row):\n",
    "    # Initialize an empty list to store the messages.\n",
    "    # メッセージを格納するための空のリストを初期化する。\n",
    "    messages = []\n",
    "    \n",
    "    # Create a 'user' message dictionary with 'content' and 'role' keys.\n",
    "    # 「content」と「role」のキーを持つ「user」メッセージの辞書を作成\n",
    "    user = {\n",
    "        \"content\": f\"{row['instruction']}\\n Input: {row['input']}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'user' message to the 'messages' list.\n",
    "    # 'user' メッセージを 'messages' リストに追加\n",
    "    messages.append(user)\n",
    "    \n",
    "    # Create an 'assistant' message dictionary with 'content' and 'role' keys.\n",
    "    # 「content」と「role」キーを持つ「assistant」メッセージ辞書を作成\n",
    "    assistant = {\n",
    "        \"content\": f\"{row['output']}\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "    \n",
    "    # Append the 'assistant' message to the 'messages' list.\n",
    "    # 'assistant'メッセージを'messages'リストに追加\n",
    "    messages.append(assistant)\n",
    "    \n",
    "    # Return a dictionary with a 'messages' key and the 'messages' list as its value.\n",
    "    # 'messages'というキーとその値として'messages'リストを持つ辞書を返す。\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# 'format_dataset_chatml' is a function that takes a row from the dataset and returns a dictionary \n",
    "# with a 'text' key and a string of formatted chat messages as its value.\n",
    "# 'format_dataset_chatml' はデータセットから行を取り、'text' キーとその値としてフォーマットされたチャットメッセージの文字列を持つ辞書を返す関数。\n",
    "def format_dataset_chatml(row):\n",
    "    # 'tokenizer.apply_chat_template' is a method that formats a list of chat messages into a single string.\n",
    "    # 'add_generation_prompt' is set to False to not add a generation prompt at the end of the string.\n",
    "    # 'tokenize' is set to False to return a string instead of a list of tokens.\n",
    "    # 'tokenizer.apply_chat_template' は、チャットメッセージのリストを1つの文字列に整形するメソッド\n",
    "    # 'add_generation_prompt' を False に設定すると、文字列の末尾に生成用のプロンプトを追加しない。\n",
    "    # 'tokenize' を False に設定すると、トークンのリストではなく文字列を返す。    \n",
    "    return {\"text\": tokenizer.apply_chat_template(row[\"messages\"], add_generation_prompt=False, tokenize=False)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b0033b-dd69-4cc2-9c25-722e29f3b1b1",
   "metadata": {},
   "source": [
    "### ChatMLフォーマットをデータセットに適用\n",
    "Apply the ChatML format to our dataset\n",
    "\n",
    "The code block is used to prepare a dataset for training a chat model.  \n",
    "このコードブロックは、チャットモデルのトレーニング用にデータセットを準備\n",
    "\n",
    "The dataset.map(create_message_column) line applies the create_message_column function to each example in the dataset.  \n",
    "This function takes a row from the dataset and transforms it into a dictionary with a 'messages' key. The value of this key is a list of 'user' and 'assistant' messages.  \n",
    "`dataset.map(create_message_column)`の行は、`create_message_column`関数をデータセットの各例に適用。  \n",
    "この関数は、データセットの行を受け取り、それを「messages」キーを持つ辞書に変換。このキーの値は「user」と「assistant」のメッセージのリスト。\n",
    "\n",
    "The 'user' message is created by combining the 'instruction' and 'input' fields from the row, while the 'assistant' message is created from the 'output' field of the row.  \n",
    "These messages are appended to the 'messages' list in the order of 'user' and 'assistant'.  \n",
    "「user」メッセージは、行の「instruction」および「input」フィールドを組み合わせて作成され、「assistant」メッセージは行の「output」フィールドから作成される。  \n",
    "これらのメッセージは、「user」と「assistant」の順で「messages」リストに追加される。\n",
    "\n",
    "The dataset_chatml.map(format_dataset_chatml) line then applies the format_dataset_chatml function to each example in the updated dataset.  \n",
    "This function takes a row from the dataset and transforms it into a dictionary with a 'text' key. The value of this key is a string of formatted chat messages.  \n",
    "次に、`dataset_chatml.map(format_dataset_chatml)`の行が更新されたデータセットの各例に`format_dataset_chatml`関数を適用。\n",
    "この関数は、データセットの行を受け取り、それを「text」キーを持つ辞書に変換。このキーの値はフォーマットされたチャットメッセージの文字列。\n",
    "\n",
    "The tokenizer.apply_chat_template method is used to format the list of chat messages into a single string.  \n",
    "The 'add_generation_prompt' parameter is set to False to avoid adding a generation prompt at the end of the string,  \n",
    "and the 'tokenize' parameter is set to False to return a string instead of a list of tokens.  \n",
    "`tokenizer.apply_chat_template`メソッドは、チャットメッセージのリストを単一の文字列にフォーマットする。  \n",
    "「add_generation_prompt」パラメータは、文字列の最後に生成プロンプトを追加しないように`False`に設定され、  \n",
    "「tokenize」パラメータはトークンのリストではなく文字列を返すように`False`に設定される。\n",
    "\n",
    "The result of these operations is a dataset where each example is a dictionary with a 'text' key and a string of formatted chat messages as its value.  \n",
    "This format is suitable for training a chat model.  \n",
    "これらの操作の結果、各例が「text」キーとフォーマットされたチャットメッセージの文字列を値として持つ辞書であるデータセットが得られる。  \n",
    "このフォーマットは、チャットモデルのトレーニングに適している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c006a4-b5f6-40b4-af86-3ece7477c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to prepare the 'dataset' for training a chat model.\n",
    "# このコードブロックは、チャットモデルの学習用に「dataset」を準備する\n",
    "\n",
    "# 'dataset.map' is a method that applies a function to each example in the 'dataset'.\n",
    "# 'create_message_column' is a function that formats each example into a 'messages' format suitable for a chat model.\n",
    "# The result is a new 'dataset_chatml' with the formatted examples.\n",
    "\n",
    "# 「dataset.map」は「dataset」の各サンプルに関数を適用するメソッド\n",
    "# 「create_message_column」は、各サンプルをチャットモデルに適した「messages」形式に整形する関数\n",
    "# 結果として、整形されたサンプルを含む新しい「dataset_chatml」が得られる。\n",
    "\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "\n",
    "# 'dataset_chatml.map' is a method that applies a function to each example in the 'dataset_chatml'.\n",
    "# 'format_dataset_chatml' is a function that further formats each example into a single string of chat messages.\n",
    "# The result is an updated 'dataset_chatml' with the further formatted examples.\n",
    "\n",
    "# 「dataset_chatml.map」は、「dataset_chatml」内の各サンプルに関数を適用するメソッド\n",
    "# 「format_dataset_chatml」は、各サンプルをチャットメッセージの単一の文字列にさらに整形する関数\n",
    "# その結果、より整形されたサンプルで更新された「dataset_chatml」が得られる。\n",
    "\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5f6bc-abc0-44d8-82ff-a1bcec48b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code is used to access and display the first example from the 'dataset_chatml'.\n",
    "# このコード行は、'dataset_chatml'から最初の例を取得して表示する\n",
    "\n",
    "# 'dataset_chatml[0]' uses indexing to access the first example in the 'dataset_chatml'.\n",
    "# In Python, indexing starts at 0, so 'dataset_chatml[0]' refers to the first example.\n",
    "# The result is a dictionary with a 'text' key and a string of formatted chat messages as its value.\n",
    "\n",
    "# 'dataset_chatml[0]' はインデックス指定により、'dataset_chatml'の最初の例にアクセスする。\n",
    "# Pythonではインデックスは0から始まるため、'dataset_chatml[0]'は最初の例を指す。\n",
    "# 結果として得られるのは、'text'というキーを持ち、その値としてフォーマットされたチャットメッセージの文字列を含む辞書\n",
    "dataset_chatml[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeefcc0-7c88-4601-a2b3-a1591d88a27f",
   "metadata": {},
   "source": [
    "### データセットを訓練用とテスト用に分割する\n",
    "Split the dataset into a train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f7502-78d8-43b6-8408-d8c610629ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to split the 'dataset_chatml' into training and testing sets.\n",
    "# このコードブロックは、'dataset_chatml' をトレーニング用とテスト用に分割する\n",
    "\n",
    "# 'dataset_chatml.train_test_split' is a method that splits the 'dataset_chatml' into a training set and a testing set.\n",
    "# 'test_size' is a parameter that specifies the proportion of the 'dataset_chatml' to include in the testing set. Here it's set to 0.05, meaning that 5% of the 'dataset_chatml' will be included in the testing set.\n",
    "# 'seed' is a parameter that sets the seed for the random number generator. This is used to ensure that the split is reproducible. Here it's set to 1234.\n",
    "# 'dataset_chatml.train_test_split' は、'dataset_chatml' をトレーニングセットとテストセットに分割するメソッド\n",
    "# 'test_size' は、'dataset_chatml' のうちテストセットに含める割合を指定するパラメタ。ここでは 0.05 、つまり 'dataset_chatml' の 5% がテストセットに含まれることを意味する。\n",
    "# 'seed' は乱数生成器のシードを設定するパラメタ。これは分割の再現性を確保するために使用され、ここでは 1234 に設定されている。\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "\n",
    "# This line of code is used to display the structure of the 'dataset_chatml' after the split.\n",
    "# It will typically show information such as the number of rows in the training set and the testing set.\n",
    "# このコード行は、分割後の 'dataset_chatml' の構造を表示するためのものです。\n",
    "# 通常は、トレーニングセットとテストセットの行数などの情報が表示されます。\n",
    "dataset_chatml\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc285f29-a5a3-46a5-b048-f196f410f576",
   "metadata": {},
   "source": [
    "## ファインチューニングの設定\n",
    "Instruction fine-tune a Phi-3-mini model using LORA and trl  \n",
    "Phi-3-miniモデルをLORAとtrlを用いてInstructionチューニングする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe975e-bc42-48a0-a90e-a14f8e8a3257",
   "metadata": {},
   "source": [
    "### まず、GPUを特定してみましょう。\n",
    "First, we try to identify out GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99298e4-d889-42aa-b22f-83ba70102437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to set the compute data type and attention implementation based on whether bfloat16 is supported on the current CUDA device.\n",
    "# このコードブロックは、現在のCUDAデバイスでbfloat16がサポートされているかどうかに基づいて、計算データ型とアテンションの実装方法を設定するためのもの。\n",
    "\n",
    "# 'torch.cuda.is_bf16_supported()' is a function that checks if bfloat16 is supported on the current CUDA device.\n",
    "# If bfloat16 is supported, 'compute_dtype' is set to 'torch.bfloat16' and 'attn_implementation' is set to 'flash_attention_2'.\n",
    "# 'torch.cuda.is_bf16_supported()' は、現在のCUDAデバイスでbfloat16がサポートされているかを確認する関数。\n",
    "# bfloat16がサポートされている場合、'compute_dtype' は 'torch.bfloat16' に、'attn_implementation' は 'flash_attention_2' に設定される。\n",
    "if torch.cuda.is_bf16_supported():\n",
    "  compute_dtype = torch.bfloat16\n",
    "  attn_implementation = 'flash_attention_2'\n",
    "# If bfloat16 is not supported, 'compute_dtype' is set to 'torch.float16' and 'attn_implementation' is set to 'sdpa'.\n",
    "# bfloat16 がサポートされていない場合、'compute_dtype' は 'torch.float16' に設定され、'attn_implementation' は 'sdpa' に設定される。\n",
    "else:\n",
    "  compute_dtype = torch.float16\n",
    "  attn_implementation = 'sdpa'\n",
    "\n",
    "# This line of code is used to print the value of 'attn_implementation', which indicates the chosen attention implementation.\n",
    "# このコード行は、「attn_implementation」の値を表示する。これは、選択されたアテンションの実装方式を示す。\n",
    "print(attn_implementation)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710a9ec-7363-4ade-9e1b-c70a33f70e8a",
   "metadata": {},
   "source": [
    "### トークナイザーとモデルをロード\n",
    "Load the tokenizer and model to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35d8ad-5860-4f4c-83ea-767dca25da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load a pre-trained model and its associated tokenizer from the Hugging Face Model Hub.\n",
    "# このコードブロックは、Hugging Face Model Hubから事前学習済みのモデルと関連するトークナイザーを読み込む\n",
    "\n",
    "# 'model_name' is set to the identifier of the pre-trained model.\n",
    "# 'model_name'は、事前学習済みモデルの識別子に設定（設定済みのためコメントアウト）\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a tokenizer from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which tokenizer to load.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the tokenizer files.\n",
    "# 'add_eos_token' is set to True to add an end-of-sentence token to the tokenizer.\n",
    "# 'use_fast' is set to True to use the fast version of the tokenizer.\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' は、Hugging Face Model Hub からトークナイザーをロードするメソッド\n",
    "# ・'model_id' は、ロードするトークナイザーを指定\n",
    "# ・'trust_remote_code' は、トークナイザーファイル内のリモートコード信頼\n",
    "# ・'add_eos_token' は、トークナイザーに文の終了トークン追加\n",
    "# ・'use_fast' は、トークナイザーの高速バージョン使用\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, add_eos_token=True, use_fast=True)\n",
    "\n",
    "# The padding token is set to the unknown token.\n",
    "# パディングトークンは未知のトークンに設定。\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# The ID of the padding token is set to the ID of the unknown token.\n",
    "# パディングトークンのIDは、未知のトークンのIDに設定。\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "# The padding side is set to 'left', meaning that padding tokens will be added to the left (start) of the sequence.\n",
    "# パディングの位置は「左」に設定（パディングトークンはシーケンスの左側（開始部分）に追加される。\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained model for causal language modeling from the Hugging Face Model Hub.\n",
    "# 'model_id' is passed as an argument to specify which model to load.\n",
    "# 'torch_dtype' is set to the compute data type determined earlier.\n",
    "# 'trust_remote_code' is set to True to trust the remote code in the model files.\n",
    "# 'device_map' is passed as an argument to specify the device mapping for distributed training.\n",
    "# 'attn_implementation' is set to the attention implementation determined earlier.\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' は、Hugging Face Model Hub から因果言語モデリング用の事前学習済みモデルを読み込むメソッド\n",
    "# ・'model_id' は、読み込むモデルを指定する\n",
    "# ・'torch_dtype' は、先に決定された計算データ型に設定\n",
    "# ・'trust_remote_code' は、モデルファイル内のリモートコード信頼\n",
    "# ・'device_map' は、分散トレーニングのためのデバイス割り当て\n",
    "# ・'attn_implementation' は、先に決定された attention の実装方法\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_id, torch_dtype=compute_dtype, trust_remote_code=True, device_map=device_map,\n",
    "          attn_implementation=attn_implementation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb0881-0a46-44c3-9ab2-1ad014d95d3c",
   "metadata": {},
   "source": [
    "### LoRAプロパティの設定\n",
    "Configure the LoRA properties\n",
    "\n",
    "The SFTTrainer offers seamless integration with peft, simplifying the process of instruction tuning LLMs.  \n",
    "All we need to do is create our LoRAConfig and supply it to the trainer.  \n",
    "However, before initiating the training process, we must specify the hyperparameters we intend to use, which are defined in TrainingArguments.\n",
    "\n",
    "SFTTrainerはpeftとのシームレスな統合を提供し、LLMの指示調整プロセスを簡素化します。  \n",
    "必要なのは、LoRAConfigを作成し、それをトレーナーに提供することだけです。  \n",
    "しかし、トレーニングプロセスを開始する前に、使用するハイパーパラメータを指定する必要があり、これらはTrainingArgumentsで定義されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070da32a-ea5b-4e84-b6ee-df429b308752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to define the training arguments for the model.\n",
    "# このコードブロックは、モデルのトレーニング用の引数を定義する\n",
    "\n",
    "# 'TrainingArguments' is a class that holds the arguments for training a model.\n",
    "# 'output_dir' is the directory where the model and its checkpoints will be saved.\n",
    "# 'evaluation_strategy' is set to \"steps\", meaning that evaluation will be performed after a certain number of training steps.\n",
    "# 'do_eval' is set to True, meaning that evaluation will be performed.\n",
    "# 'optim' is set to \"adamw_torch\", meaning that the AdamW optimizer from PyTorch will be used.\n",
    "# 'per_device_train_batch_size' and 'per_device_eval_batch_size' are set to 8, meaning that the batch size for training and evaluation will be 8 per device.\n",
    "# 'gradient_accumulation_steps' is set to 4, meaning that gradients will be accumulated over 4 steps before performing a backward/update pass.\n",
    "# 'log_level' is set to \"debug\", meaning that all log messages will be printed.\n",
    "# 'save_strategy' is set to \"epoch\", meaning that the model will be saved after each epoch.\n",
    "# 'logging_steps' is set to 100, meaning that log messages will be printed every 100 steps.\n",
    "# 'learning_rate' is set to 1e-4, which is the learning rate for the optimizer.\n",
    "# 'fp16' is set to the opposite of whether bfloat16 is supported on the current CUDA device.\n",
    "# 'bf16' is set to whether bfloat16 is supported on the current CUDA device.\n",
    "# 'eval_steps' is set to 100, meaning that evaluation will be performed every 100 steps.\n",
    "# 'num_train_epochs' is set to 3, meaning that the model will be trained for 3 epochs.\n",
    "# 'warmup_ratio' is set to 0.1, meaning that 10% of the total training steps will be used for the warmup phase.\n",
    "# 'lr_scheduler_type' is set to \"linear\", meaning that a linear learning rate scheduler will be used.\n",
    "# 'report_to' is set to \"wandb\", meaning that training and evaluation metrics will be reported to Weights & Biases.\n",
    "# 'seed' is set to 42, which is the seed for the random number generator.\n",
    "# このコードブロックは、モデルのトレーニング用の引数を定義するためのものです。\n",
    "\n",
    "# 'TrainingArguments' は、モデルのトレーニングに関する引数を保持するクラス\n",
    "# ・'output_dir' は、モデルおよびそのチェックポイントが保存されるディレクトリ\n",
    "# ・'evaluation_strategy' = \"steps\" は一定のステップごとに評価を実施する\n",
    "# ・'do_eval' = True は評価する実施する。\n",
    "# ・'optim' = \"adamw_torch\" はPyTorch の AdamW オプティマイザを使用\n",
    "# ・'per_device_train_batch_size' および 'per_device_eval_batch_size' = 8 はトレーニングおよび評価時のバッチサイズ = 8\n",
    "# ・'gradient_accumulation_steps' = 4 は、勾配が4ステップ分蓄積されてから逆伝播および更新を実行\n",
    "# ・'log_level' = \"debug\" は、すべてのログメッセージを表示\n",
    "# ・'save_strategy' = \"epoch\" は、各エポック後にモデルを保存\n",
    "# ・'logging_steps' = 100 は、100ステップごとにログメッセージを表示\n",
    "# ・'learning_rate' = 1e-4 は、オプティマイザに使われる学習率\n",
    "# ・'fp16' は現在の CUDA デバイスが bfloat16 をサポートしているかどうかに応じて逆の値に設定\n",
    "# ・'bf16' は現在の CUDA デバイスが bfloat16 をサポートしているかどうかに基づいて設定\n",
    "# ・'eval_steps' = 100 は、100ステップごとに評価を実行\n",
    "# ・'num_train_epochs' = 3 は、モデルが3エポック分トレーニング\n",
    "# ・'warmup_ratio' = 0.1 は、トレーニング全体の10%をウォームアップフェーズとする。\n",
    "# ・'lr_scheduler_type' = \"linear\" は、線形スケジューラを使用\n",
    "# ・'report_to' = \"wandb\" は、トレーニングおよび評価の指標 Weights & Biases を有効化\n",
    "# ・'seed' = 42 は、乱数生成器のシード\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"./phi-3-mini-LoRA\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=100,\n",
    "        num_train_epochs=3,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "# LoraConfig object is created with the following parameters:\n",
    "# 'r' (rank of the low-rank approximation) is set to 16,\n",
    "# 'lora_alpha' (scaling factor) is set to 16,\n",
    "# 'lora_dropout' dropout probability for Lora layers is set to 0.05,\n",
    "# 'task_type' (set to TaskType.CAUSAL_LM indicating the task type),\n",
    "# 'target_modules' (the modules to which LoRA is applied) choosing linear layers except the output layer.\n",
    "\n",
    "# LoraConfig オブジェクトを以下のパラメタで作成：\n",
    "# ・'r'（低ランク近似のランク）は 16 に設定\n",
    "# ・'lora_alpha'（スケーリング係数）は 16 に設定\n",
    "# ・'lora_dropout'（LoRA層のドロップアウト確率）は 0.05 に設定\n",
    "# ・'task_type'（タスクの種類）は TaskType.CAUSAL_LM に設定（これは因果言語モデルタスクを示す）\n",
    "# ・'target_modules'（LoRA を適用するモジュール）には、出力層を除いた線形層を選択\n",
    "peft_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3701e6-58e9-4514-aa2e-736e158a7dac",
   "metadata": {},
   "source": [
    "### wandb と接続を確立\n",
    "Establish Connection with wandb and Initiate the Project and Experiment\n",
    "wandb と接続を確立し、プロジェクトと実験を開始する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc8c21-6b6e-4e3e-bd83-1e4cb94691c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize Weights & Biases (wandb), a tool for tracking and visualizing machine learning experiments.\n",
    "# このコードブロックは、Weights & Biases（wandb）という機械学習実験のトラッキングと可視化ツールを初期化\n",
    "\n",
    "# 'import wandb' is used to import the wandb library.\n",
    "# 'import wandb' で wandb ライブラリをインポート\n",
    "import wandb\n",
    "\n",
    "# 'wandb.login()' is a method that logs you into your Weights & Biases account.\n",
    "# If you're not already logged in, it will prompt you to log in.\n",
    "# Once you're logged in, you can use Weights & Biases to track and visualize your experiments.\n",
    "\n",
    "# 'wandb.login()' は、Weights & Biases のアカウントにログインするためのメソッド\n",
    "# まだログインしていない場合は、ログインを促すプロンプトが表示される。\n",
    "# ログインすると、Weights & Biases を使って実験のトラッキングと可視化ができる。\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d97d37-1e22-45b8-88c1-3f2e79ec68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize a Weights & Biases (wandb) run.\n",
    "# このコードブロックは、Weights & Biases（wandb）の実行を初期化\n",
    "\n",
    "# 'project_name' is set to the name of the project in Weights & Biases.\n",
    "# 'project_name' は、Weights & Biases 上のプロジェクト名を設定\n",
    "project_name = \"Phi3-mini-ft-python-code\"\n",
    "\n",
    "# 'wandb.init' is a method that initializes a new Weights & Biases run.\n",
    "# 'project' is set to 'project_name', meaning that the run will be associated with this project.\n",
    "# 'name' is set to \"phi-3-mini-ft-py-3e\", which is the name of the run.\n",
    "# Each run has a unique name which can be used to identify it in the Weights & Biases dashboard.\n",
    "\n",
    "# 'wandb.init' は、新しい Weights & Biases の実行を初期化するメソッド\n",
    "# ・'project' に 'project_name' を設定、実行がプロジェクト名に紐づけられる。\n",
    "# ・'name' に \"phi-3-mini-ft-py-3e\" を設定、これはこの実行の名前。\n",
    "# 各実行には一意の名前があり、Weights & Biases のダッシュボード上で識別するために使用。\n",
    "wandb.init(project=project_name, name = \"phi-3-mini-ft-py-3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5901f-e693-4d4a-979b-e76f4a296979",
   "metadata": {},
   "source": [
    "### SFTTrainerを構築\n",
    "We now possess all the necessary components to construct our SFTTrainer and commence the training of our model.  \n",
    "私たちは現在、SFTTrainer を構築し、モデルのトレーニングを開始するために必要なすべての要素を揃えています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1970160-b861-441d-9007-b847a3766ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to initialize the SFTTrainer, which is used to train the model.\n",
    "# このコードブロックは、モデルを学習させるために使用される SFTTrainer を初期化する\n",
    "\n",
    "# 'model' is the model that will be trained.\n",
    "# 'train_dataset' and 'eval_dataset' are the datasets that will be used for training and evaluation, respectively.\n",
    "# 'peft_config' is the configuration for peft, which is used for instruction tuning.\n",
    "# 'dataset_text_field' is set to \"text\", meaning that the 'text' field of the dataset will be used as the input for the model.\n",
    "# 'max_seq_length' is set to 512, meaning that the maximum length of the sequences that will be fed to the model is 512 tokens.\n",
    "# 'tokenizer' is the tokenizer that will be used to tokenize the input text.\n",
    "# 'args' are the training arguments that were defined earlier.\n",
    "\n",
    "# ・'model' は学習対象のモデルです。\n",
    "# ・'train_dataset' と 'eval_dataset' は、それぞれ学習と評価に使用されるデータセット\n",
    "# ・'peft_config' は instruction tuning に使用される peft の設定\n",
    "# ・'dataset_text_field' は \"text\" に設定されており、データセットの 'text' フィールドがモデルへの入力として使用される。\n",
    "# ・'max_seq_length' は 512 に設定されており、モデルに入力されるシーケンスの最大長が 512 トークンであることを意味する。\n",
    "# ・'tokenizer' は入力テキストをトークナイズするために使用されるトークナイザー\n",
    "# ・'args' は前のステップで定義された学習用引数\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset_chatml['train'],\n",
    "        eval_dataset=dataset_chatml['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e533a78-0987-4e5b-8788-ac644ba0a648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ファインチューニング実行\n",
    "と、モデルの保存も行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76ff3e-6453-42ce-a3b8-d0b3e27406d6",
   "metadata": {},
   "source": [
    "### Trainer インスタンスで train() メソッドを呼び出して、モデルのトレーニング プロセスを初期化\n",
    "Initiate the model training process by invoking the train() method on our Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94696e-7ccb-444f-b7aa-06dcc3ce8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to train the model and save it locally.\n",
    "# このコードブロックは、モデルのトレーニングとローカルへの保存に使用\n",
    "\n",
    "# 'trainer.train()' is a method that starts the training of the model.\n",
    "# It uses the training dataset, evaluation dataset, and training arguments that were provided when the trainer was initialized.\n",
    "\n",
    "# 'trainer.train()' は、モデルのトレーニングを開始するメソッド\n",
    "# トレーナーの初期化時に指定されたトレーニングデータセット、評価データセット、およびトレーニング引数を使用\n",
    "trainer.train()\n",
    "\n",
    "# 'trainer.save_model()' is a method that saves the trained model locally.\n",
    "# The model will be saved in the directory specified by 'output_dir' in the training arguments.\n",
    "\n",
    "# 'trainer.save_model()' は、トレーニング済みモデルをローカルに保存するメソッド\n",
    "# モデルは、トレーニング引数で指定された 'output_dir' ディレクトリに保存される\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a913c1-e2c1-4410-a5f1-6e76215fefdb",
   "metadata": {},
   "source": [
    "### アダプターをHugging Face Hubに保管\n",
    "Store the adapter on the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54716259-f4c5-4fe7-bb6c-900509dbdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to save the adapter to the Hugging Face Model Hub.\n",
    "# このコードブロックは、アダプターを Hugging Face Model Hub に保存するために使用されます。\n",
    "\n",
    "# 'trainer.push_to_hub' is a method that pushes the trained model (or adapter in this case) to the Hugging Face Model Hub.\n",
    "# The argument \"edumunozsala/adapter-phi-3-mini-py_code\" is the name of the repository on the Hugging Face Model Hub where the adapter will be saved.\n",
    "# 'trainer.push_to_hub' は、学習済みモデル（この場合はアダプター）を Hugging Face Model Hub にアップロードするためのメソッド\n",
    "# 引数 \"edumunozsala/adapter-phi-3-mini-py_code\" は、アダプターが保存 Hugging Face Model Hub 上のリポジトリ名\n",
    "trainer.push_to_hub(\"HuggingFaceUser/adapter-name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11141fee-75f7-4911-8462-224614a19e3e",
   "metadata": {},
   "source": [
    "### モデルとアダプターの保存前にメモリを開放\n",
    "Merge the model and the adapter and save it\n",
    "\n",
    "Combine the model and the adapter, then save it.  \n",
    "モデルとアダプターを結合してから保存してください。  \n",
    "It's necessary to clear the memory when operating on a T4 instance.  \n",
    "T4インスタンス上で操作する際には、メモリを解放することが必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c86c1-1408-4dfb-9334-795ba3009323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to free up GPU memory.\n",
    "# このコードブロックはGPUメモリを解放するために使用\n",
    "\n",
    "# 'del model' and 'del trainer' are used to delete the 'model' and 'trainer' objects. \n",
    "# This removes the references to these objects, allowing Python's garbage collector to free up the memory they were using.\n",
    "# 'del model' および 'del trainer' は、'model' および 'trainer' オブジェクトを削除するために使用\n",
    "# これにより、これらのオブジェクトへの参照が削除され、Pythonのガベージコレクタがそれらの使用していたメモリを解放できる\n",
    "del model\n",
    "del trainer\n",
    "\n",
    "# 'import gc' is used to import Python's garbage collector module.\n",
    "# 'import gc' は、Pythonのガベージコレクタモジュールをインポートする\n",
    "import gc\n",
    "\n",
    "# 'gc.collect()' is a method that triggers a full garbage collection, which can help to free up memory.\n",
    "# It's called twice here to ensure that all unreachable objects are collected.\n",
    "# 'gc.collect()' は、完全なガベージコレクションを強制的に実行するメソッド。\n",
    "# これによりメモリの解放が促進される。2回呼び出し到達不能なオブジェクトも確実に回収\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbdbdf-3a1e-4066-89ad-aab9f1d0f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'torch.cuda.empty_cache()' is a PyTorch method that releases all unoccupied cached memory currently held by \n",
    "# the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "# 'torch.cuda.empty_cache()' は PyTorch のメソッドで、現在保持されている未使用のキャッシュメモリを解放する。\n",
    "# これにより他の GPU アプリケーションでそのメモリを使用できるようになり、nvidia-smi 上でも確認できるようになる。\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773f7dc-1c6d-421a-a98d-b0d116eed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()の実行後にさらに実行すると意味がある\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53c0ba-75d3-4914-a1a6-72cfac6a0b5b",
   "metadata": {},
   "source": [
    "### モデルを結合し完全なモデルとして保存\n",
    "以前に学習・保存されたモデルを読み込み、それらを結合して、完全なモデルとして保存  \n",
    "Load the previously trained and stored model, combine it, and then save the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32d63e-0f38-4ab3-88e7-a3949220c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load the trained model, merge it, and save the merged model.\n",
    "# このコードブロックは、学習済みモデルを読み込み、マージして、マージ後のモデルを保存\n",
    "\n",
    "# 'AutoPeftModelForCausalLM' is a class from the 'peft' library that provides a causal language model with PEFT (Performance Efficient Fine-Tuning) support.\n",
    "# 'AutoPeftModelForCausalLM' は、PEFT（効率的なファインチューニング）に対応した因果言語モデルを提供する 'peft' ライブラリのクラス\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# 'AutoPeftModelForCausalLM.from_pretrained' is a method that loads a pre-trained model (adapter model) and its base model.\n",
    "#  The adapter model is loaded from 'args.output_dir', which is the directory where the trained model was saved.\n",
    "# 'low_cpu_mem_usage' is set to True, which means that the model will use less CPU memory.\n",
    "# 'return_dict' is set to True, which means that the model will return a 'ModelOutput' (a named tuple) instead of a plain tuple.\n",
    "# 'torch_dtype' is set to 'torch.bfloat16', which means that the model will use bfloat16 precision for its computations.\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'device_map' is the device map that will be used by the model.\n",
    "\n",
    "# 'AutoPeftModelForCausalLM.from_pretrained' は、事前学習済みモデル（アダプターモデル）とそのベースモデルを読み込むメソッド\n",
    "# アダプターモデルは 'args.output_dir' （学習済みモデルが保存されているディレクトリ）から読み込まれる。\n",
    "# ・'low_cpu_mem_usage' を True に設定すると、モデルはCPUメモリの使用量を抑える。\n",
    "# ・'return_dict' を True に設定すると、モデルは普通のタプルではなく 'ModelOutput'（名前付きタプル）を返す。\n",
    "# ・'torch_dtype' を 'torch.bfloat16' に設定すると、モデルはbfloat16精度で計算を行う。\n",
    "# ・'trust_remote_code' を True に設定すると、リモートコードの実行を信頼して許可する。\n",
    "# ・'device_map' はモデルが使用するデバイスのマッピングを指定する。\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16, #torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# 'new_model.merge_and_unload' is a method that merges the model and unloads it from memory.\n",
    "# The merged model is stored in 'merged_model'.\n",
    "# 'new_model.merge_and_unload' は、モデルをマージし、メモリからアンロードするメソッド\n",
    "# マージされたモデルは 'merged_model' に格納される。\n",
    "merged_model = new_model.merge_and_unload()\n",
    "\n",
    "# 'merged_model.save_pretrained' is a method that saves the merged model.\n",
    "# The model is saved in the directory \"merged_model\".\n",
    "# 'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# 'safe_serialization' is set to True, which means that the model will use safe serialization.\n",
    "\n",
    "# 'merged_model.save_pretrained' は、マージされたモデルを保存するメソッドで、\"merged_model\" ディレクトリに保存。\n",
    "# ・'trust_remote_code' を True に設定すると、リモートコードの実行を信頼して許可\n",
    "# ・'safe_serialization' を True に設定すると、安全なシリアライズ方式で保存\n",
    "merged_model.save_pretrained(\"merged_model\", trust_remote_code=True, safe_serialization=True)\n",
    "\n",
    "# 'tokenizer.save_pretrained' is a method that saves the tokenizer.\n",
    "# The tokenizer is saved in the directory \"merged_model\".\n",
    "# 'tokenizer.save_pretrained' は、トークナイザーを保存するメソッド\n",
    "# トークナイザーは \"merged_model\" ディレクトリに保存される\n",
    "tokenizer.save_pretrained(\"merged_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc04c6-4641-4df0-8ff1-0459b9884999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to push the merged model and the tokenizer to the Hugging Face Model Hub.\n",
    "# このコードブロックは、マージ済みモデルとトークナイザーを Hugging Face Model Hub にプッシュ（アップロード）する\n",
    "\n",
    "# 'merged_model.push_to_hub' is a method that pushes the merged model to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the model will be saved.\n",
    "# 'merged_model.push_to_hub' は、マージ済みモデルを Hugging Face Model Hub にプッシュするメソッド\n",
    "# 'hf_model_repo' は、モデルが保存される Hugging Face Model Hub 上のリポジトリ名\n",
    "merged_model.push_to_hub(hf_model_repo)\n",
    "\n",
    "# 'tokenizer.push_to_hub' is a method that pushes the tokenizer to the Hugging Face Model Hub.\n",
    "# 'hf_model_repo' is the name of the repository on the Hugging Face Model Hub where the tokenizer will be saved.\n",
    "# 'tokenizer.push_to_hub' は、トークナイザーを Hugging Face Model Hub にプッシュするメソッド\n",
    "# 'hf_model_repo' は、トークナイザーが保存される Hugging Face Model Hub 上のリポジトリ名\n",
    "tokenizer.push_to_hub(hf_model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f56c3-503a-4368-8de1-7e9f19622f6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## モデルの推論と評価の準備\n",
    "For model inference and evaluation, we will download the model we created from the Hugging Face Hub and test it to ensure its functionality.  \n",
    "モデルの推論と評価を行うために、作成したモデルをHugging Face Hubからダウンロードし、正常に機能するかをテストする。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d59269-a9ee-4ca9-9007-10be8f770482",
   "metadata": {},
   "source": [
    "### リポジトリを設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff6eb2-a99c-4dbd-badc-a5f4ce940eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hf_model_repo' is a variable that holds the name of the repository on the Hugging Face Model Hub.\n",
    "# This is where the trained and merged model, as well as the tokenizer, have been saved.\n",
    "# 'hf_model_repo' は、Hugging Face Model Hub 上のリポジトリ名を保持する変数\n",
    "# このリポジトリには、学習およびマージされたモデルやトークナイザーが保存されている。\n",
    "hf_model_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c06851-4154-4ff0-abd1-d1d70d4ddb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'hf_model_repo' is a variable that holds the name of the repository on the Hugging Face Model Hub.\n",
    "# This is where the trained and merged model, as well as the tokenizer, have been saved.\n",
    "# If 'hf_model_repo' is not defined, it is set to 'username/modelname'.\n",
    "# This is the default repository where the model and tokenizer will be saved if no other repository is specified.\n",
    "\n",
    "# 'hf_model_repo' はHugging Face Model Hub上のリポジトリ名を保持する変数で、ここに、訓練済みおよび統合されたモデル、およびトークナイザーが保存されている。\n",
    "# 'hf_model_repo' が定義されていない場合、'username/modelname' に設定される。これは、デフォルトのリポジトリ。\n",
    "# hf_model_repo = 'username/modelname' if not hf_model_repo else hf_model_repo # 初期化の段階でリテラルのようになるように変更したので不要になった。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab5efe-b652-47f3-87f4-fb6f0168f404",
   "metadata": {},
   "source": [
    "### Hugging Face Hub からモデルとトークナイザーを取得\n",
    "Retrieve the model and tokenizer from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1aec8-abf1-40f3-bcee-5c6759a135e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to load the model and tokenizer from the Hugging Face Model Hub.\n",
    "# このコードブロックは、Hugging Face Model Hub からモデルとトークナイザーを読み込むために使用\n",
    "\n",
    "# 'torch' is a library that provides a wide range of functionalities for tensor computations with strong GPU acceleration support.\n",
    "# 'AutoTokenizer' and 'AutoModelForCausalLM' are classes from the 'transformers' library that provide a tokenizer and a causal language model, respectively.\n",
    "# 'set_seed' is a function from the 'transformers' library that sets the seed for generating random numbers, which can be used for reproducibility.\n",
    "\n",
    "# 'torch' は、GPUによる強力な加速サポートを備えたテンソル計算のためのさまざまな機能を提供するライブラリ\n",
    "# 'AutoTokenizer' と 'AutoModelForCausalLM' は、'transformers' ライブラリからのクラスで、それぞれトークナイザーと因果関係言語モデルを提供\n",
    "# 'set_seed' は、乱数を生成するためのシードを設定する 'transformers' ライブラリの関数で、再現性を確保するために使用\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "# 'set_seed(1234)' sets the seed for generating random numbers to 1234.\n",
    "# 'set_seed(1234)' は、乱数生成のためのシードを 1234 に設定\n",
    "set_seed(1234)  # For reproducibility # 再現性のため\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' is a method that loads a pre-trained tokenizer.\n",
    "# The tokenizer is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the tokenizer was saved.\n",
    "# 'trust_remote_code' is set to True, which means that the tokenizer will trust and execute remote code.\n",
    "\n",
    "# 'AutoTokenizer.from_pretrained' は、事前学習済みトークナイザーを読み込むためのメソッド\n",
    "# トークナイザーは Hugging Face Model Hubのリポジトリ名：'hf_model_repo' から読み込まれる。\n",
    "# 'trust_remote_code' = True は、トークナイザーがリモートコードを信頼して実行\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo,trust_remote_code=True)\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' is a method that loads a pre-trained causal language model.\n",
    "# The model is loaded from 'hf_model_repo', which is the name of the repository on the Hugging Face Model Hub where the model was saved.\n",
    "# ・'trust_remote_code' is set to True, which means that the model will trust and execute remote code.\n",
    "# ・'torch_dtype' is set to \"auto\", which means that the model will automatically choose the data type for its computations.\n",
    "# ・'device_map' is set to \"cuda\", which means that the model will use the CUDA device for its computations.\n",
    "\n",
    "# 'AutoModelForCausalLM.from_pretrained' は、事前学習済み因果関係言語モデルを読み込むためのメソッドです。\n",
    "# モデルは Hugging Face Model Hubのリポジトリ名：'hf_model_repo' から読み込まれる。\n",
    "# ・'trust_remote_code' = True は、モデルがリモートコードを信頼して実行\n",
    "# ・'torch_dtype' = \"auto\" は、モデルが自動的に計算のためのデータ型を選択\n",
    "# ・'device_map' = \"cuda\" は、モデルがCUDAデバイスを使用して計算を行う\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo, trust_remote_code=True, torch_dtype=\"auto\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32efeb6-145f-40c9-9f4e-f93557a56036",
   "metadata": {},
   "source": [
    "### データセットを前と同じ方法で整理\n",
    "We arrange the dataset in the same manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29164360-e415-41e5-83f2-da3ef741353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to prepare the dataset for model training.\n",
    "# このコードブロックはモデル学習用のデータセットを準備する\n",
    "\n",
    "# 'dataset.map(create_message_column)' applies the 'create_message_column' function to each element in the 'dataset'.\n",
    "# This function is used to create a new column in the dataset.\n",
    "\n",
    "# 'dataset.map(create_message_column)' は 'dataset' の各要素に 'create_message_column' 関数を適用\n",
    "# この関数はデータセットに新しい列を作成する\n",
    "dataset_chatml = dataset.map(create_message_column)\n",
    "\n",
    "# 'dataset_chatml.map(format_dataset_chatml)' applies the 'format_dataset_chatml' function to each element in 'dataset_chatml'.\n",
    "# This function is used to format the dataset in a way that is suitable for chat ML.\n",
    "# 'dataset_chatml.map(format_dataset_chatml)' は 'dataset_chatml' の各要素に 'format_dataset_chatml' 関数を適用\n",
    "# この関数はチャット向けの機械学習に適した形式にデータセットを整形する\n",
    "dataset_chatml = dataset_chatml.map(format_dataset_chatml)\n",
    "\n",
    "# 'dataset_chatml.train_test_split(test_size=0.05, seed=1234)' splits 'dataset_chatml' into a training set and a test set.\n",
    "# 'test_size=0.05' means that 5% of the data will be used for the test set.\n",
    "# 'seed=1234' is used for reproducibility.\n",
    "\n",
    "# 'dataset_chatml.train_test_split(test_size=0.05, seed=1234)' は 'dataset_chatml' を訓練用データとテスト用データに分割\n",
    "# 'test_size=0.05' はデータの5%をテストセットとして使用\n",
    "# 'seed=1234' は再現性を確保するために使用\n",
    "dataset_chatml = dataset_chatml.train_test_split(test_size=0.05, seed=1234)\n",
    "\n",
    "# 'dataset_chatml' is printed to the console to inspect its contents.\n",
    "# 'dataset_chatml' の内容を確認するためにコンソールに出力します。\n",
    "dataset_chatml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf785622-93d4-4831-960f-28a651c2cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dataset_chatml['test'][0]' is used to access the first element of the test set in the 'dataset_chatml' dataset.\n",
    "# This can be used to inspect the first test sample to understand its structure and contents.\n",
    "# 'dataset_chatml['test'][0]' は、'dataset_chatml' データセットのテストセット内の最初の要素にアクセス、最初のテストサンプルの構造や内容を確認\n",
    "dataset_chatml['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c9572-37f4-4bd5-b58f-e51cabf8d425",
   "metadata": {},
   "source": [
    "### 推論を実行するためのテキスト生成パイプラインを作成\n",
    "Create a text generation pipeline to run the inference\n",
    "\n",
    "※ 純粋な推論の実行なので、pipelineはココが初出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff49559-a218-4cec-b350-95829e485217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'pipeline' is a function from the 'transformers' library that creates a pipeline for text generation.\n",
    "# 'text-generation' is the task that the pipeline will perform.\n",
    "# 'model' is the pre-trained model that the pipeline will use.\n",
    "# 'tokenizer' is the tokenizer that the pipeline will use to tokenize the input text.\n",
    "# The created pipeline is stored in the 'pipe' variable.\n",
    "\n",
    "# 'pipeline' は 'transformers' ライブラリの関数で、テキスト生成のためのパイプラインを作成\n",
    "# ・'text-generation' はパイプラインが実行するタスク\n",
    "# ・'model' はパイプラインで使用する事前学習済みモデル\n",
    "# ・'tokenizer' は入力テキストをトークン化するためにパイプラインで使用するトークナイザー\n",
    "# ・作成されたパイプラインは 'pipe' 変数に保存される\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba05f1f-c13e-41b4-8d72-f5620d1510c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is used to test the chat template.\n",
    "# このコードブロックは、チャットテンプレートのテストに使用される。\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' is a method that applies the chat template to a list of messages.\n",
    "# The list of messages is [{\"role\": \"user\", \"content\": dataset_chatml['test'][0]['messages'][0]['content']}], which is the first message in the test set of 'dataset_chatml'.\n",
    "# 'tokenize' is set to False, which means that the method will not tokenize the messages.\n",
    "# 'add_generation_prompt' is set to True, which means that the method will add a generation prompt to the messages.\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' は、メッセージのリストにチャットテンプレートを適用するメソッド\n",
    "# メッセージのリストは [{\"role\": \"user\", \"content\": dataset_chatml['test'][0]['messages'][0]['content']}] であり、\n",
    "# ・これは 'dataset_chatml' のテストセットに含まれる最初のメッセージ\n",
    "# ・'tokenize' を False に設定すると、メソッドはメッセージをトークン化しない。\n",
    "# ・'add_generation_prompt' を True に設定すると、メッセージに生成用プロンプトが追加される。\n",
    "pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": dataset_chatml['test'][0]['messages'][0]['content']}], tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8ed5b-771e-4cde-9cdf-d2e64647dfee",
   "metadata": {},
   "source": [
    "### 入力を整理し、個々のサンプルに対して推論を実行する関数を開発\n",
    "Develop a function that organizes the input and performs inference on an individual sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67724242-434a-442d-8925-dbf7a041a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'test_inference' that performs inference on a given prompt.\n",
    "# このコードブロックでは、与えられたプロンプトに対して推論を行う関数 'test_inference' を定義\n",
    "\n",
    "# 'prompt' is the input to the function. It is the text that the model will generate a response to.\n",
    "# 'prompt' は関数への入力で、モデルが応答を生成するためのテキスト\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' is a method that applies the chat template to the prompt.\n",
    "# The prompt is wrapped in a list and formatted as a dictionary with \"role\" set to \"user\" and \"content\" set to the prompt.\n",
    "# 'tokenize' is set to False, which means that the method will not tokenize the prompt.\n",
    "# 'add_generation_prompt' is set to True, which means that the method will add a generation prompt to the prompt.\n",
    "# The formatted prompt is stored back in the 'prompt' variable.\n",
    "\n",
    "# 'pipe.tokenizer.apply_chat_template' は、チャットテンプレートをプロンプトに適用するメソッド\n",
    "# ・'prompt' はリストにラップされ「role」を \"user\" に、「content」をプロンプトの内容に設定した辞書\n",
    "# ・'tokenize' は False に設定されており、プロンプトはトークナイズされない。\n",
    "# ・'add_generation_prompt' は True に設定されており、生成プロンプトが追加される。\n",
    "# ・フォーマットされたプロンプトは再び 'prompt' 変数に格納される。\n",
    "\n",
    "# 'pipe' is the text generation pipeline that was created earlier.\n",
    "# It is called with the formatted prompt and several parameters that control the text generation process.\n",
    "# 'max_new_tokens=256' limits the maximum number of new tokens that can be generated.\n",
    "# 'do_sample=True' enables sampling, which means that the model will generate diverse responses.\n",
    "# 'num_beams=1' sets the number of beams for beam search to 1, which means that the model will generate one response.\n",
    "# 'temperature=0.3' controls the randomness of the responses. Lower values make the responses more deterministic.\n",
    "# 'top_k=50' limits the number of highest probability vocabulary tokens to consider for each step.\n",
    "# 'top_p=0.95' enables nucleus sampling and sets the cumulative probability of parameter tokens to 0.95.\n",
    "# 'max_time=180' limits the maximum time for the generation process to 180 seconds.\n",
    "# The generated responses are stored in the 'outputs' variable.\n",
    "\n",
    "# 'pipe' は以前に作成されたテキスト生成パイプライン\n",
    "# フォーマットされたプロンプトと、テキスト生成プロセスを制御するいくつかのパラメタとともに呼び出される。\n",
    "# ・'max_new_tokens=256' は、生成される新しいトークンの最大数を 256 に制限\n",
    "# ・'do_sample=True' はサンプリングを有効にし、多様な応答を生成\n",
    "# ・'num_beams=1' はビームサーチのビーム数を 1 に設定し、1 つの応答のみを生成\n",
    "# ・'temperature=0.3' は応答のランダム性を制御。値が小さいほど応答は決定論的。\n",
    "# ・'top_k=50' は各ステップで考慮する語彙の上位確率トークン数を 50 に制限\n",
    "# ・'top_p=0.95' は nucleus sampling を有効にし、累積確率が 0.95 になるようにトークンを選ぶ。\n",
    "# ・'max_time=180' は生成処理の最大時間を 180 秒に制限\n",
    "# ・生成された応答は 'outputs' 変数に格納される。\n",
    "\n",
    "# The function returns the first generated response.\n",
    "# The response is stripped of the prompt and any leading or trailing whitespace.\n",
    "# この関数はプロンプト部分と前後の空白を取り除いてから最初の生成応答を返す。\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, num_beams=1, temperature=0.3, top_k=50, top_p=0.95,\n",
    "                   max_time= 180) #, eos_token_id=eos_token)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31fbb9-8550-4637-b79f-0d397471e12b",
   "metadata": {},
   "source": [
    "### 準備モデルの推論を試行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb67e97-c659-4ac7-b076-2005b9120c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block calls the 'test_inference' function with the first message in the test set of 'dataset_chatml' as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is printed to the console.\n",
    "\n",
    "# このコードブロックは、'dataset_chatml' のテストセットの最初のメッセージをプロンプトとして 'test_inference' 関数を呼び出す\n",
    "# 'test_inference' はそのプロンプトに対して推論を行い、生成された応答を返す。応答はコンソールに出力される。\n",
    "test_inference(dataset_chatml['test'][0]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86948584-8ae5-4bf0-8613-fa6dfe5fdb79",
   "metadata": {},
   "source": [
    "### モデルの評価メトリックの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e90b41-2b2e-4b7d-acda-d78a599b2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'load_metric' is a function from the 'datasets' library that loads a metric for evaluating the model.\n",
    "# Metrics are used to measure the performance of the model on certain tasks.\n",
    "# 'load_metric' は 'datasets' ライブラリからの関数で、モデルの評価に使用するメトリックを読み込む\n",
    "# メトリックは、特定のタスクにおけるモデルのパフォーマンスを測定するために使用される。\n",
    "from datasets import load_metric\n",
    "\n",
    "# 'load_metric(\"rouge\", trust_remote_code=True)' loads the ROUGE metric from the 'datasets' library.\n",
    "# ROUGE is a set of metrics used to evaluate automatic summarization and machine translation.\n",
    "# We'll employ the ROUGE metric to assess performance. While it may not be the optimal metric, it's straightforward and convenient to utilize.\n",
    "# 'trust_remote_code' is set to True, which means that the metric will trust and execute remote code.\n",
    "# The loaded metric is stored in the 'rouge_metric' variable.\n",
    "\n",
    "# 'load_metric(\"rouge\", trust_remote_code=True)' は ROUGE 指標を読み込む\n",
    "# パフォーマンス評価にはROUGE指標を使用、自動要約や機械翻訳の評価に使用される一連の指標で、最適ではないかもしれないが、シンプルで使いやすい指標\n",
    "# 'trust_remote_code' を True に設定することで、リモートコードを信頼\n",
    "# 読み込まれた指標は 'rouge_metric' 変数に格納される。\n",
    "rouge_metric = load_metric(\"rouge\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86a9c8-27f1-4a5b-b859-c383b94d49c7",
   "metadata": {},
   "source": [
    "### 推論を実行し、インスタンスを評価するための関数を開発\n",
    "Develop a function for performing inference and assessing an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e726aa-2de9-4efa-a5b1-b713adc2233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block defines a function 'calculate_rogue' that calculates the ROUGE score for a given row in the dataset.\n",
    "# このコードブロックは、データセットの各行に対して ROUGE スコアを計算する関数 'calculate_rogue' を定義\n",
    "\n",
    "# 'row' is the input to the function. It is a row in the dataset that contains a message and its corresponding output.\n",
    "# 'row' は関数への入力でメッセージとそれに対応する出力を含むデータセットの1行\n",
    "\n",
    "# 'test_inference(row['messages'][0]['content'])' calls the 'test_inference' function with the first message in the row as the prompt.\n",
    "# 'test_inference' performs inference on the prompt and returns a generated response.\n",
    "# The response is stored in the 'response' variable.\n",
    "\n",
    "# 'test_inference(row['messages'][0]['content'])' は、その行の最初のメッセージをプロンプトとして 'test_inference' 関数を呼び出す。\n",
    "# 'test_inference' はプロンプトに対して推論を行い、生成された応答を返し 'response' という変数に保存。\n",
    "\n",
    "# 'rouge_metric.compute' is a method that calculates the ROUGE score for the generated response and the corresponding output in the row.\n",
    "# 'predictions' is set to the generated response and 'references' is set to the output in the row.\n",
    "# 'use_stemmer' is set to True, which means that the method will use a stemmer to reduce words to their root form.\n",
    "# The calculated ROUGE score is stored in the 'result' variable.\n",
    "\n",
    "# 'rouge_metric.compute' は、生成された応答と行に含まれる正解出力との ROUGE スコアを計算するメソッド\n",
    "# 'predictions' には生成された応答を、'references' には正解出力を指定\n",
    "# 'use_stemmer' を True に設定することで、単語を語幹に変換して比較\n",
    "# 計算された ROUGE スコアは 'result' 変数に保存\n",
    "\n",
    "# The 'result' dictionary is updated to contain the F-measure of each ROUGE score multiplied by 100.\n",
    "# The F-measure is a measure of a test's accuracy that considers both the precision and the recall of the test.\n",
    "\n",
    "# 'result' 辞書は、各 ROUGE スコアの F値（F-measure）に100を掛けた値（%）を持つように更新\n",
    "# F値は、精度（precision）と再現率（recall）の両方を考慮したテストの精度指標です。\n",
    "\n",
    "# The 'response' is added to the 'result' dictionary.\n",
    "# 'response' を 'result' 辞書に追加\n",
    "\n",
    "# The function returns the 'result' dictionary.\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response']=response\n",
    "    return result\n",
    "\n",
    "# 関数は 'result' 辞書を返します。\n",
    "def calculate_rogue(row):\n",
    "    response = test_inference(row['messages'][0]['content'])\n",
    "    result = rouge_metric.compute(predictions=[response], references=[row['output']], use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result['response'] = response\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225c437-674d-48fb-8728-6147dd87856e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## モデルの推論と評価の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91eb09-55e4-4cd7-bcdf-451aa4839aee",
   "metadata": {},
   "source": [
    "Now, we have the ability to execute inference on a collection of samples. For simplicity, the process isn't optimized at this stage. In the future, we plan to perform inference in batches to enhance performance. However, for the time being,\n",
    "現在、複数のサンプルに対して推論を実行できるようになりました。簡略化のため、現時点ではプロセスは最適化されていません。将来的には、パフォーマンスを向上させるために推論をバッチ処理で実行する予定です。ただし、当面は。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df050cd-7390-49a6-926a-bf9246099a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '%%time' is a magic command in Jupyter notebooks that measures the execution time of the cell.\n",
    "# '%%time'はJupyterノートブックのマジックコマンドで、セルの実行時間を測定\n",
    "%%time\n",
    "\n",
    "# 'dataset_chatml['test'].select(range(0,500))' selects the first 500 elements from the test set in the 'dataset_chatml' dataset.\n",
    "# 'dataset_chatml['test'].select(range(0,500))'は、'dataset_chatml'データセットのテストセットから最初の500要素を選択\n",
    "\n",
    "# '.map(calculate_rogue, batched=False)' applies the 'calculate_rogue' function to each element in the selected subset.\n",
    "# 'calculate_rogue' calculates the ROUGE score for each element.\n",
    "# 'batched' is set to False, which means that the function will be applied to each element individually, not in batches.\n",
    "\n",
    "# '.map(calculate_rogue, batched=False)'は、選択したサブセットの各要素に'calculate_rogue'関数を適用\n",
    "# 'calculate_rogue'は、各要素に対してROUGEスコアを計算\n",
    "# 'batched'がFalseに設定されているため、関数はバッチ処理ではなく、各要素に個別に適用\n",
    "\n",
    "# The results are stored in the 'metricas' variable.\n",
    "# 結果は'metricas'変数に格納されます。\n",
    "metricas = dataset_chatml['test'].select(range(0,500)).map(calculate_rogue, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5a455-ccc1-481d-a68f-1f2f1d812ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'numpy' is a library in Python that provides support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "# 'import numpy as np' imports the 'numpy' library and gives it the alias 'np'. This allows us to use 'np' instead of 'numpy' when calling its functions.\n",
    "# 'numpy'はPythonのライブラリで、大規模な多次元配列や行列のサポートを提供し、これらの配列に対して操作を行うための多くの高レベルな数学関数を提供します。\n",
    "# 'import numpy as np'は'numpy'ライブラリをインポートし、それにエイリアス'numpy'の代わりに使用できる'np'を付けます。これにより、関数を呼び出す際に'numpy'の代わりに'np'を使用できます。\n",
    "import numpy as np\n",
    "\n",
    "# This code block prints the mean of the ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores in the 'metricas' dictionary.\n",
    "# このコードブロックは、'metricas' 辞書に含まれる ROUGE-1、ROUGE-2、ROUGE-L、および ROUGE-Lsum スコアの平均を出力\n",
    "\n",
    "# 'np.mean(metricas['rouge1'])' calculates the mean of the ROUGE-1 scores. ROUGE-1 スコアの平均を計算\n",
    "# 'np.mean(metricas['rouge2'])' calculates the mean of the ROUGE-2 scores. ROUGE-2 スコアの平均を計算\n",
    "# 'np.mean(metricas['rougeL'])' calculates the mean of the ROUGE-L scores. ROUGE-L スコアの平均を計算\n",
    "# 'np.mean(metricas['rougeLsum'])' calculates the mean of the ROUGE-Lsum scores. ROUGE-Lsum スコアの平均を計算\n",
    "\n",
    "# 'print' is used to print the calculated means to the console.\n",
    "# 'print' を使用して、計算された平均値をコンソールに表示します。\n",
    "\n",
    "print(\"Rouge 1 Mean: \",np.mean(metricas['rouge1']))\n",
    "print(\"Rouge 2 Mean: \",np.mean(metricas['rouge2']))\n",
    "print(\"Rouge L Mean: \",np.mean(metricas['rougeL']))\n",
    "print(\"Rouge Lsum Mean: \",np.mean(metricas['rougeLsum']))     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
