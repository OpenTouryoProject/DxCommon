{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3df4459-b70f-432f-b772-676ed4fa6be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# o'reillyのカサゴ深層学習の本\n",
    "\n",
    "## ニューラルネットワークの構成要素\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- 深層のCNN\n",
    "  - [深層のCNNの実装](#深層のCNNの実装)\n",
    "  - [深層のCNNを訓練してセーブ](#深層のCNNを訓練してセーブ)\n",
    "  - [深層のCNNをロードして推論](#深層のCNNをロードして推論)\n",
    "  \n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch/blob/master/notebooks/\n",
    "- [深層学習（deep learning） - 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%EF%BC%88deep%20learning%EF%BC%89) > [ニューラルネットワーク\n",
    "](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?plugin=related&page=%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF) > [畳み込みニューラルネットワーク（CNN）\n",
    "](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%88CNN%EF%BC%89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab256dd-c180-4767-983b-9b07561ee7e2",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344608e2-352b-4412-a078-4c6b7bf9dc58",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5516c3b4-3cc4-4bc2-b127-f42850c5efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from kasago.common.layers import *\n",
    "from kasago.dataset.mnist import load_mnist\n",
    "from kasago.common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e5f60-b94f-4a30-a88b-c6b9b596734d",
   "metadata": {},
   "source": [
    "### 共通関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e6e97-98cd-4211-a54a-1f2611f1961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7681cb-5001-47ce-ac28-a6d1b8db753c",
   "metadata": {},
   "source": [
    "## 深層のCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fedb81-7d2e-430d-bea9-4d37f58d5387",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 深層のCNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c6135-bf0c-47e3-b412-faf26b0547e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepConvNet:\n",
    "    \"\"\"認識率99%以上の高精度なConvNet\n",
    "\n",
    "    ネットワーク構成は下記の通り\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout -\n",
    "        affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50,\n",
    "                 output_size=10):\n",
    "        \n",
    "        # 重みの初期化===========\n",
    "        self.params = {}\n",
    "\n",
    "        # チャンネル数\n",
    "        pre_channel_num = input_dim[0]\n",
    "        \n",
    "        # ReLUを使う場合に推奨されるHeの初期値（平均0、標準偏差√(2/前層ノード数)である正規分布\n",
    "        # 各層のニューロン数は自動計算だが重みは手動計算\n",
    "        # この前層ノード数はconvの重みの行数に一致するので（FH * FW * CH）で計算できる。\n",
    "        # ただし、最後のW7だけは Affine層なので要素の個数は1つ前の要素数で\n",
    "        # フィルタ数 * 特徴マップのサイズだが HWS = 2 の Poolingを3回通るので\n",
    "        #  特徴マップのサイズは、28*)28/2 → 14/2 → 7/2 → 4(*4 になる。\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)\n",
    "        \n",
    "        # conv\n",
    "        for idx, conv_param in enumerate(\n",
    "            [conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            # convの重みの行列サイズは（FH * FW * CH数）行 * （フィルタ数）列\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            # convのバイアス？\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            # 初回は前チャネル数だが以降は前フィルタ数になる。\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "            \n",
    "        # affine\n",
    "        # W7はAffine層なので要素の個数は1つ前の要素数（64*4*4行50列）\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        # 同様に、W8もAffine層なので要素の個数は1つ前の要素数（50行10列）\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        # (100,1,28,28)\n",
    "        \n",
    "        #conv1 FN:16, FH:3, FW:3, P:1, S:1\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        # (100,16,28,28)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,16,28,28)\n",
    "        \n",
    "        #conv2 FN:16, FH:3, FW:3, P:1, S:1\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        # (100,16,28,28)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,16,28,28)\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        # (100,16,14,14)\n",
    "        \n",
    "        #conv3 FN:32, FH:3, FW:3, P:1, S:1\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        # (100,32,14,14)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,32,14,14)\n",
    "        \n",
    "        #conv4 FN:32, FH:3, FW:3, P:2, S:1\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        # (100,32,16,16) pad=2なのでサイズは+2\n",
    "        self.layers.append(Relu())\n",
    "        # (100,32,16,16)\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        # (100,32,8,8)\n",
    "        \n",
    "        #conv5 FN:64, FH:3, FW:3, P:1, S:1\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        # (100,64,8,8)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,64,8,8)\n",
    "        \n",
    "        #conv6 FN:64, FH:3, FW:3, P:1, S:1\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        # (100,64,8,8)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,64,8,8)\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        # (100,64,4,4)\n",
    "        \n",
    "        # affine W:(64,4,4, 100)\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        # (100,50)\n",
    "        self.layers.append(Relu())\n",
    "        # (100,50)\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        # (100,50)\n",
    "        \n",
    "        # Output(affine W:(50, 10)\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        # (100,10)\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        # (100,10)\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        # (100,10)\n",
    "        \n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b63734-eb01-47f4-9644-f085b0eb0ee0",
   "metadata": {},
   "source": [
    "### 深層のCNNを訓練してセーブ\n",
    "処理が重いので既定でコメントアウト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215ba86-2ca1-44fb-8c09-e7be829c8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "x_train, t_train = x_train[:500], t_train[:500]\n",
    "x_test, t_test = x_test[:100], t_test[:100]\n",
    "\n",
    "max_epochs = 20\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"../work/deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff6caa9-ad9d-46ff-bb6e-add195fba6b9",
   "metadata": {},
   "source": [
    "### 深層のCNNをロードして推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc075d16-015b-499f-bb27-4b9c2073e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"../work/deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids = classified_ids.flatten()\n",
    " \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "            \n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
