{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a588b2f-aa6c-44dc-9ef4-fbd6061051c6",
   "metadata": {},
   "source": [
    "# LlamaIndexでRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a79c5e-15d5-465c-b9f5-6283775d97de",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- [概要](#概要)\n",
    "- [参考](#参考)\n",
    "- [チェック](#チェック)\n",
    "- [チュートリアル](#チュートリアル)\n",
    "  - [準備](#準備)\n",
    "    - [使用する変数](#使用する変数)\n",
    "    - [インストレーション](#インストレーション)\n",
    "    - [ライブラリ読み込み](#ライブラリ読み込み)\n",
    "  - [LLM](#LLM)\n",
    "    - [OpenAI](#OpenAI)\n",
    "      - [追加のインストレーション](#追加のインストレーション1)\n",
    "      - [追加のライブラリ読み込み](#追加のライブラリ読み込み1)\n",
    "      - [API Keyの確認](#API_Keyの確認)\n",
    "    - [Ollama](#Ollama)\n",
    "      - [追加のインストレーション](#追加のインストレーション2)\n",
    "      - [追加のライブラリ読み込み](#追加のライブラリ読み込み2)\n",
    "      - [追加のライブラリの設定](#追加のライブラリの設定)\n",
    "  - [共通](#共通)\n",
    "    - [インデックス作成](#インデックス作成)\n",
    "    - [RAGのRetrieval部](#RAGのRetrieval部)\n",
    "    - [ログの有効化](#ログの有効化)\n",
    "    - [永続化して実行](#永続化して実行)\n",
    "- [詳細](#詳細)\n",
    "  - [Queryingのカスタマイズ](#Queryingのカスタマイズ)\n",
    "    - LLMに投げる前にチャンクを確認\n",
    "    - 段階毎のカスタマイズ\n",
    "  - [Evaluation](#Evaluation)\n",
    "    - 準備\n",
    "    - 関連評価\n",
    "    - 回答評価\n",
    "    - 検索評価\n",
    "  - [StoringでChromaDBを使用](#StoringでChromaDBを使用)\n",
    "    - 追加のインストレーション3\n",
    "    - 追加のライブラリ読み込み3\n",
    "    - パーツ毎に分解して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019bec2-7b0b-41bb-95c3-f735a91fe213",
   "metadata": {},
   "source": [
    "## 概要\n",
    "- LlamaIndex（公式）をトレースして基本的な利用方法を確認する。\n",
    "- 破壊的に変更が発生するまで使えるでしょう。\n",
    "- 破壊的に変更が発生後は、公式サイトの当該バージョンの情報（≒一次情報）をあたって。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab5958-4219-4f63-a243-b5590119a676",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58e6cb-cad7-4f43-af45-5d04219952c7",
   "metadata": {},
   "source": [
    "LLMのRAG - .NET 開発基盤部会 Wiki  \n",
    "https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?LLM%E3%81%AERAG\n",
    "- 知識情報の分割\n",
    "- 知識情報の埋め込み\n",
    "- 質問の入力（Query Input）\n",
    "- 質問の埋め込み（Query Embedding）\n",
    "- 情報の検索（Information Retrieval）\n",
    "- 情報の生成（Information Generation）\n",
    "- 回答の提供（Answer Delivery）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915e0c1-5cba-4407-bae8-6481179a3f69",
   "metadata": {},
   "source": [
    "LlamaIndex - .NET 開発基盤部会 Wiki  \n",
    "https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?LlamaIndex\n",
    "- Loading\n",
    "- Indexing\n",
    "- Storing\n",
    "- Querying\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0387006-4f25-4817-a3b1-d05e56305c57",
   "metadata": {},
   "source": [
    "## チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729cb641-a9ac-4b7c-ac1f-9b69d031bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c57768c-0bc1-4cc8-95a2-9cd89443fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3c041-3b69-4045-9fea-dc9422515527",
   "metadata": {},
   "source": [
    "## チュートリアル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a092e-a3a8-44b2-86e4-73576b7a5c10",
   "metadata": {},
   "source": [
    "### 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550afdc8-f737-4426-83a6-10be1d75fece",
   "metadata": {},
   "source": [
    "#### 使用する変数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d63013ab-97bf-49c9-ad27-b6d28deadac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./llamaindex/data/paul_graham_essay\"\n",
    "PERSIST_DIR = \"./llamaindex/storage/paul_graham_essay\"\n",
    "CHROMA_DIR = \"./llamaindex/chroma_db/paul_graham_essay\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c5bd2-e782-4418-9e29-21d80e344d7e",
   "metadata": {},
   "source": [
    "#### インストレーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019d5799-3e1d-439f-a631-c2eada36f88e",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install llama-index\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110f189-d281-4a84-8840-6c1e2e646a48",
   "metadata": {},
   "source": [
    "#### ライブラリ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35c6b69-695b-49db-8871-70acc0e8849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f988e3-2447-47b8-897f-aa45146ca501",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d889f94-d4b9-43e3-8713-c3e3e1c9abf7",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba669365-3663-47d2-906b-3839f85b3bc1",
   "metadata": {},
   "source": [
    "##### 追加のインストレーション1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befedc74-e9e4-4160-a5bf-d21b2da868c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要（OpenAIは依存関係パッケージらしい）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3bf94-916f-4e1a-906d-67e58c68f67d",
   "metadata": {},
   "source": [
    "##### 追加のライブラリ読み込み1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e6d089-334e-49f8-8c97-a61f2399c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価の所で使う\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d0f81-1a72-468b-8684-3446cd1fc02c",
   "metadata": {},
   "source": [
    "##### API_Keyの確認\n",
    "- 準備は、OpenAIにログインしてAPIからKeyを取得、カード登録してチャージするだけ。\n",
    "- プログラムから使用できるようにするには、以下の環境変数に、API Keyが設定されていれば良い。\n",
    "- 冷静に考えると、評価の所でLLMを明示的に指定しているがそれ以外でなんのモデルを使っているか不明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c6053-20bb-404e-8d77-2c9019af2065",
   "metadata": {},
   "source": [
    "```Python\n",
    "import os\n",
    "print(os.environ['OPENAI_API_KEY'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708855a8-d529-4696-9a80-a7101e2251bc",
   "metadata": {},
   "source": [
    "#### Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9453ebc-d6a2-4bd6-bb84-aa833ef54302",
   "metadata": {},
   "source": [
    "##### 追加のインストレーション2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64235616-cf96-449d-bebd-7bff7a523fdc",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install llama-index-llms-ollama\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a7303-3110-4536-a8a4-868d61f931d6",
   "metadata": {},
   "source": [
    "##### 追加のライブラリ読み込み2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bc56f-4112-476c-a474-8fa9286fc02d",
   "metadata": {},
   "source": [
    "```Python\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254e6aa-7aa5-4258-91e4-ec30ef26dd62",
   "metadata": {},
   "source": [
    "##### 追加のライブラリの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16609769-89f7-4f1e-b15e-aede2076556b",
   "metadata": {},
   "source": [
    "```Python\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"Llama3\", request_timeout=360.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d9f9bc-43a0-4563-a1fa-f52c0ad109fb",
   "metadata": {},
   "source": [
    "### 共通\n",
    "以降のコードは、OpenAI・Ollamaで共通になる。  \n",
    "（ただし、評価で使うLLMにはOpenAIを使用）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143f648-d3c5-40e8-b748-a272fafcf1ef",
   "metadata": {},
   "source": [
    "#### インデックス作成\n",
    "最も基礎的で、オンメモリのセマンティック検索のインデックス。\n",
    "- RAGで言うと：知識情報の分割、知識情報の埋め込み\n",
    "- LlamaIndexで言うと：Loading、Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa86da-f8ad-4192-b602-9ed2a1a6ff16",
   "metadata": {},
   "source": [
    "##### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f1625f-1c79-45ab-82b3-abf22bd89d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956547c7-0c3f-42e4-a691-544155865ebd",
   "metadata": {},
   "source": [
    "##### Indexing\n",
    "- 以下では、VectorStoreIndexを使用している。\n",
    "- SummaryIndexやKnowledgeGraphIndexなどのindexもある。\n",
    "- 質問内容によって最適なIndexが異なる可能性がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16070c37-5f39-4fa9-b910-c4aac63b18e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eab513-0888-47f3-8c83-6fb31b6f5aa8",
   "metadata": {},
   "source": [
    "#### RAGのRetrieval部\n",
    "- RAGで言うと：Query Input、Query Embedding、Information Retrieval\n",
    "- LlamaIndexで言うと：Querying、Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39165cc2-50f1-4eb2-ae9d-8d38f4ca1628",
   "metadata": {},
   "source": [
    "##### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e50d344b-2dad-4208-b1cf-3ee59d594f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing short stories and programming, starting with early attempts on an IBM 1401 in 9th grade using an early version of Fortran. Later, the author transitioned to working with microcomputers, building simple games and programs on a TRS-80.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b871eb-5235-4659-a932-9b7a1d862eb6",
   "metadata": {},
   "source": [
    "#### ログの有効化\n",
    "抽象化度が高いので、ログの有効化をしても良いが、結局、欲しい所が出きってない感もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a2aa20-2dcb-45a1-b097-b5a3a6e88613",
   "metadata": {},
   "source": [
    "```Python\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a4374-4291-4d11-b177-e82b0375d936",
   "metadata": {},
   "source": [
    "#### 永続化して実行\n",
    "前述のコードに、LlamaIndexで言うStoringの概念を追加したIndexing & Storing版。\n",
    "- 永続化は、Document Store、Vector Store、Index Storeに、Storage Contextを設定する。\n",
    "- 既出の、Loadingの所で、Document Storeから読み出している。\n",
    "- Indexingでは、Vector Store、Index Storeに書き出し（永続化し）す。\n",
    "\n",
    "OpenAI・Ollamaを切り替える際は、Embeddingが異なるようなので、一度、PERSIST_DIRのIndexを削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068ddce5-e376-4dc3-8d61-6e81f3237c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing short stories and programming, particularly on an IBM 1401 in 9th grade using an early version of Fortran. Later, the author got a TRS-80 computer and started programming more extensively, creating simple games, a rocket prediction program, and a word processor.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c021c-54c3-435a-a107-8e3ee1fe3be1",
   "metadata": {},
   "source": [
    "## 詳細"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709baa1-4ecb-4c04-b5e2-f92c167c15c0",
   "metadata": {},
   "source": [
    "### Queryingのカスタマイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3666d-11c2-4800-8eb4-121d90f35396",
   "metadata": {},
   "source": [
    "#### LLMに投げる前にチャンクを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27f2bc-6b1e-45c7-86ba-581918390f07",
   "metadata": {},
   "source": [
    "#### no_textを使う\n",
    "オプションでLLMにリクエストしない方法がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a47bc27-2b33-4553-91f9-e915fb491a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either way we can now query the index\n",
    "query_engine = index.as_query_engine(response_mode='no_text')\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "# print(response.source_nodes) # プロパティを確認したい場合、ココを実行して出力を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bf7f8-9f9a-4891-888d-94153cafd07c",
   "metadata": {},
   "source": [
    "#### as_retrieverを使う\n",
    "そもそも、別メソッドが用意されているらしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95965bbe-5f66-4c18-9a6f-7e48b49d63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "nodes = retriever.retrieve(\"What did the author do growing up?\")\n",
    "# print(response.source_nodes) # プロパティを確認したい場合、ココを実行して出力を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d82899-37ed-4348-bdeb-a574d413b457",
   "metadata": {},
   "source": [
    "#### MockLLMを使う\n",
    "MockLLMを使うという方法もあるらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6003084-0a92-4186-a874-aad1c0ea6880",
   "metadata": {},
   "source": [
    "```Python\n",
    "from llama_index.core.llms import MockLLM\n",
    "llm = MockLLM()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3c04e-e7f9-4ffd-9224-74c6e57fab14",
   "metadata": {},
   "source": [
    "#### source_nodesを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3766526b-fe26-4c74-870d-a065203283b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score  0.8108766833727953\n",
      "id_ ef903a82-4999-448a-b007-452ab00100d5\n",
      "file_name paul_graham_essay.txt\n",
      "------------------------------------------------------\n",
      "score  0.804940970839821\n",
      "id_ 4aa4469e-4ece-4418-94ac-3a824149c490\n",
      "file_name paul_graham_essay.txt\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(\"score \", node.score)\n",
    "    print(\"id_\", node.id_)\n",
    "    print(\"file_name\", node.metadata[\"file_name\"])\n",
    "    # print(\"text\", node.text) # テキストを確認したい場合、ココを実行して出力を確認\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566be1c-0fa9-42a3-9c3e-e383d90e1626",
   "metadata": {},
   "source": [
    "#### 段階毎のカスタマイズ\n",
    "クエリによる検索、後処理、応答合成の各段階をカスタマイズできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412ed4f-7da4-4a16-a134-1dbb3a74ad0c",
   "metadata": {},
   "source": [
    "##### 検索\n",
    "K=3検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a52155-129c-414f-989f-ca7925fc9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# configure retriever\n",
    "# トップ「10」セマンティック検索\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021ef50-4775-403f-8097-479a9c6d0efd",
   "metadata": {},
   "source": [
    "##### 後処理＆応答合成\n",
    "閾値を指定すると０件参照になったりするので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6765b921-58f0-42e5-99a4-70193fceac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# 後処理\n",
    "# configure node postprocessors\n",
    "# 類似度スコアにしきい値を設定してノードをフィルタリング\n",
    "node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)]\n",
    "\n",
    "# 応答合成\n",
    "# configure response synthesizer\n",
    "# response_modeを指定する。COMPACTはチャンク毎のLLM呼び出しを\n",
    "# 最大プロンプト サイズ内に収まる限り多くのチャンクを詰め込む。\n",
    "# また、Prompt Templateを与えデフォルトのPrompt Templateをカスタマイズ可能らしい。\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970d7c4-490d-4293-8599-a47a2318be73",
   "metadata": {},
   "source": [
    "##### 最終的な組み立て"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac161f59-7405-4d10-9243-ca6dd803e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849e51e-7c9b-4f8b-b937-07b8adaa0573",
   "metadata": {},
   "source": [
    "##### カスタマイズされたクエリの実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33f73a-3365-42c3-8614-9726af10e0f5",
   "metadata": {},
   "source": [
    "###### チャンクが＋１された結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae99bbf-3020-4f09-87c3-b545d3a34250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author worked on writing short stories and programming, particularly on an IBM 1401 computer in 9th grade using an early version of Fortran. Later on, the author got a microcomputer kit and started programming on a TRS-80, writing simple games and a word processor.\n"
     ]
    }
   ],
   "source": [
    "# Either way we can now query the index\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20116bb6-7b55-4552-bcc2-c24497068bff",
   "metadata": {},
   "source": [
    "###### Kに対応したノード数になっているハズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afb429af-c928-48e1-ac80-28687aa0e24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score  0.8108766833727953\n",
      "id_ ef903a82-4999-448a-b007-452ab00100d5\n",
      "file_name paul_graham_essay.txt\n",
      "------------------------------------------------------\n",
      "score  0.804940970839821\n",
      "id_ 4aa4469e-4ece-4418-94ac-3a824149c490\n",
      "file_name paul_graham_essay.txt\n",
      "------------------------------------------------------\n",
      "score  0.8017418372702448\n",
      "id_ 79ed1d41-9d29-424d-982f-c33a4358bd0d\n",
      "file_name paul_graham_essay.txt\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(\"score \", node.score)\n",
    "    print(\"id_\", node.id_)\n",
    "    print(\"file_name\", node.metadata[\"file_name\"])\n",
    "    # print(\"text\", node.text) # テキストを確認したい場合、ココを実行して出力を確認\n",
    "    print(\"------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce1e648-6003-4ea7-aa52-afd4c691c2de",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "[共通](#共通)で説明したLoading、Indexing、Storing、Queryingの続き。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c780b6-4aec-4b14-a235-54711927e004",
   "metadata": {},
   "source": [
    "#### 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d306e-ff20-4d40-a6ee-9e65baa81398",
   "metadata": {},
   "source": [
    "##### クエリを固定し評価用LLMを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1174c67-c19d-4933-be29-26685a097cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the author do growing up?\"\n",
    "llm = OpenAI(model=\"gpt-4\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d3e66-96a6-43e1-9430-e43e86b5fd6b",
   "metadata": {},
   "source": [
    "##### 非同期実行のおまじない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44f5efc-013d-4e3f-99c0-412a12a6d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2b74f-915c-442b-9a47-e32e165e7820",
   "metadata": {},
   "source": [
    "#### 関連評価\n",
    "リクエストとレスポンスの関連を評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18991463-5efd-4893-94ec-a9328d848295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What did the author do growing up?\n",
      "response: The author worked on writing short stories and programming, particularly on an IBM 1401 in 9th grade using an early version of Fortran. Later, with the advent of microcomputers, the author began programming on a TRS-80 and wrote simple games, a rocket prediction program, and a word processor.\n",
      "score: 1.0\n",
      "passing: True\n",
      "feedback: YES\n",
      "invalid_result: False\n",
      "invalid_reason: None\n",
      "pairwise_source: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "\n",
    "# define evaluator\n",
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# evaluate response\n",
    "relevancy_eval_result = evaluator.evaluate_response(query=query, response=response)\n",
    "print(\"query: \"           + str(relevancy_eval_result.query))\n",
    "print(\"response: \"        + str(relevancy_eval_result.response))\n",
    "print(\"score: \"           + str(relevancy_eval_result.score))\n",
    "print(\"passing: \"         + str(relevancy_eval_result.passing))         # バイナリ評価結果（合格か不合格か）\t\n",
    "print(\"feedback: \"        + str(relevancy_eval_result.feedback))        # フィードバックまたは回答の理由\n",
    "print(\"invalid_result: \"  + str(relevancy_eval_result.invalid_result))  # 評価結果が無効かどうか。\t\n",
    "print(\"invalid_reason: \"  + str(relevancy_eval_result.invalid_reason))  # 無効な評価の理由。\n",
    "print(\"pairwise_source: \" + str(relevancy_eval_result.pairwise_source)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f34c37-80dd-4fab-ad81-64efa82f025b",
   "metadata": {},
   "source": [
    "##### contexts\n",
    "出力させると長いので概要だけ説明すると、  \n",
    "質問（\"What did the author do growing up?\"）  \n",
    "に関係するRAGのチャンクが抜き出されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7660ee2-4c78-4029-be35-c1f1dec3d56e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"contexts: \" + str(relevancy_eval_result.contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf819825-2d88-4a3a-bd2e-952926d54d94",
   "metadata": {},
   "source": [
    "#### 回答評価\n",
    "応答は（RAGによって）取得したコンテキストと一致しているか？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9ff5e-03e5-4d01-ae33-52a50bb79ffe",
   "metadata": {},
   "source": [
    "##### 実際の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b81c5a0d-73d0-421e-b521-efb2140bcd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: None\n",
      "response: The author worked on writing short stories and programming, particularly on an IBM 1401 in 9th grade using an early version of Fortran. Later, the author transitioned to working with microcomputers, building simple games and a word processor on a TRS-80.\n",
      "score: 1.0\n",
      "passing: True\n",
      "feedback: YES\n",
      "invalid_result: False\n",
      "invalid_reason: None\n",
      "pairwise_source: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "# define evaluator\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "\n",
    "# query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# evaluate response\n",
    "response_eval_result = evaluator.evaluate_response(response=response)\n",
    "print(\"query: \"           + str(response_eval_result.query))\n",
    "print(\"response: \"        + str(response_eval_result.response))\n",
    "print(\"score: \"           + str(response_eval_result.score))\n",
    "print(\"passing: \"         + str(response_eval_result.passing))         # バイナリ評価結果（合格か不合格か）\t\n",
    "print(\"feedback: \"        + str(response_eval_result.feedback))        # フィードバックまたは回答の理由\n",
    "print(\"invalid_result: \"  + str(response_eval_result.invalid_result))  # 評価結果が無効かどうか。\t\n",
    "print(\"invalid_reason: \"  + str(response_eval_result.invalid_reason))  # 無効な評価の理由。\n",
    "print(\"pairwise_source: \" + str(response_eval_result.pairwise_source)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdca7fd-c2ac-4d01-ba86-d8b8a3bc2406",
   "metadata": {},
   "source": [
    "##### contexts\n",
    "出力させると長いので概要だけ説明すると、  \n",
    "質問（\"What did the author do growing up?\"）  \n",
    "に関係するRAGのチャンクが抜き出されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a14b0-7043-4d67-89b9-bb1344c5cd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"contexts: \" + str(response_eval_result.contexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ddfa8-dc9e-45c6-82f9-4f8cedac1e59",
   "metadata": {},
   "source": [
    "#### 検索評価\n",
    "RAGによって取得されたソースはクエリに関連しているか？（先に使用した `retriever` を再利用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f32a27e-f129-40a4-8e76-a0ff5d6407f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What did the author do growing up?\n",
      "expected_ids: ['node_id1', 'node_id2']\n",
      "expected_texts: None\n",
      "retrieved_ids: ['ef903a82-4999-448a-b007-452ab00100d5', '4aa4469e-4ece-4418-94ac-3a824149c490', '79ed1d41-9d29-424d-982f-c33a4358bd0d']\n",
      "mode: RetrievalEvalMode.TEXT\n",
      "metric_dict: {'mrr': RetrievalMetricResult(score=0.0, metadata={}), 'hit_rate': RetrievalMetricResult(score=0.0, metadata={})}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# define evaluator\n",
    "# hit_rate: 取得した上位k個のコンテキスト内に正しい答えが含まれている割合を計算\n",
    "# mrr: 応答のリストを正解確率順に並べたプロセスを評価する統計的尺度（平均逆順位）\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    ")\n",
    "\n",
    "# query index\n",
    "retrieval_eval_result = retriever_evaluator.evaluate(\n",
    "    query=query,\n",
    "    expected_ids=[\"node_id1\", \"node_id2\"]\n",
    ")\n",
    "\n",
    "# evaluate response\n",
    "response_eval_result = evaluator.evaluate_response(response=response)\n",
    "print(\"query: \"           + str(retrieval_eval_result.query))\n",
    "print(\"expected_ids: \"    + str(retrieval_eval_result.expected_ids))\n",
    "print(\"expected_texts: \"  + str(retrieval_eval_result.expected_texts))\n",
    "print(\"retrieved_ids: \"   + str(retrieval_eval_result.retrieved_ids))\n",
    "print(\"mode: \"            + str(retrieval_eval_result.mode))\n",
    "print(\"metric_dict: \"     + str(retrieval_eval_result.metric_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d727c-8a60-45f3-a36e-eaff786e4bd5",
   "metadata": {},
   "source": [
    "##### retrieved_texts\n",
    "出力させると長いので概要だけ説明すると、  \n",
    "質問（\"What did the author do growing up?\"）  \n",
    "に関係するRAGのチャンクが抜き出されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c870075-acd7-4c87-a0e6-dec574b10ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"retrieved_texts: \" + str(retrieval_eval_result.retrieved_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba2bf6-8d21-41ef-9d1a-ee88c5d6a59c",
   "metadata": {},
   "source": [
    "### StoringでChromaDBを使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12144141-facd-41bf-88ea-6b4a83ea3cdc",
   "metadata": {},
   "source": [
    "#### 追加のインストレーション3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d1829-ba4b-43b2-88ea-867d6550d1ab",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install chromadb\n",
    "!pip install llama-index-vector-stores-chroma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64a2c0-49ce-4826-b1fd-b00a218a67b2",
   "metadata": {},
   "source": [
    "#### 追加のライブラリ読み込み3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb1325b7-caf2-4eea-850d-cd6ad95367e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587732ff-5c18-4337-bb54-8d44d6a2ac07",
   "metadata": {},
   "source": [
    "#### パーツ毎に分解して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a3323-64a6-435e-8ec3-a747c606142f",
   "metadata": {},
   "source": [
    "##### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "964620e0-90be-476e-89a1-eca273e5ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c289430-bb00-44e7-ae85-eb7ada3957e1",
   "metadata": {},
   "source": [
    "##### Settings\n",
    "Vector Store の Storage Context に Chroma DB を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e75281c-90db-4300-b675-20977629f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43700e9-648d-4176-8a94-de27c081b7b4",
   "metadata": {},
   "source": [
    "##### Indexing & Storing\n",
    "Index Store の Storage Context にも Chroma DB を使うので、  \n",
    "Vector Store の Storage Context を Index Store にも設定する。  \n",
    "※ OpenAI・Ollamaを切り替える際は、Embeddingが異なるようなので、一度、PERSIST_DIRのIndexを削除する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ce632-bfd5-42d8-8993-1e197175c50e",
   "metadata": {},
   "source": [
    "###### 初回"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011012c1-48e6-4dc5-a377-c20cbbe24f89",
   "metadata": {},
   "source": [
    "```Python\n",
    "# create your index and save index to chromadb\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c85aa-41c6-4196-9d84-8ab60e178acd",
   "metadata": {},
   "source": [
    "###### 2回目以降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83c24974-e254-4589-98a4-43cc0a6fb5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed249a7c-c364-4a8b-8d08-f24dd91279b5",
   "metadata": {},
   "source": [
    "###### 追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2dea1b-2b09-4495-aa62-c3707aa01b64",
   "metadata": {},
   "source": [
    "```Python\n",
    "for doc in documents:\n",
    "    index.insert(doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f211fd-1844-41c2-b4e3-2b3f27518288",
   "metadata": {},
   "source": [
    "##### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e1b05a3-8896-40a9-a3a7-4324427515c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a philosophical question that has been debated for centuries by various thinkers and scholars.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/seigi/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/seigi/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/seigi/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/seigi/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1894, in _run_once\n",
      "    handle = self._ready.popleft()\n",
      "IndexError: pop from an empty deque\n"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
