{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rdラボへようこそ -  第1週、4日目\n",
    "\n",
    "今日は、すぐに価値のあるものを構築します！\n",
    "\n",
    "フォルダー `me` に、単一のファイル `linkedIn.pdf` を配置しました。これは、linkedInプロファイルのpdfダウンロードです。\n",
    "\n",
    "あなたのものに置き換えてください！\n",
    "\n",
    "`summary.txt`というファイルも作成しました\n",
    "\n",
    "まだツールを使用するつもりはありません。明日ツールを追加します。\n",
    "\n",
    "### 概要\n",
    "個人のLinkedInプロフィール（PDF形式）とその要約テキスト（`summary.txt`）を用いて、  \n",
    "AIチャットボット（LLM）を構築し、Webサイト上で自身になりきって質問応答できる仕組みを作る。  \n",
    "さらに、回答品質を自動評価し、不適切な場合は再回答する機能も実装しています。\n",
    "\n",
    "- セットアップとパッケージの導入\n",
    "  - `dotenv`, `openai`, `pypdf`, `gradio`などのパッケージをインポート。\n",
    "  - LinkedInのPDF（`me/linkedin.pdf`）を読み込み、テキスト化。\n",
    "  - 要約テキスト（`me/summary.txt`）を読み込み。\n",
    "\n",
    "- システムプロンプトの生成  \n",
    "`Ed Donner`（例）になりきるためのプロンプトを作成し、その人物情報としてLinkedInテキストと要約を付与。\n",
    "\n",
    "- チャットボットの実装\n",
    "  - `chat()`関数: ユーザーからの質問と履歴をもとにOpenAI API（GPT-4o-mini）で回答を生成。\n",
    "  - Gradioの`ChatInterface`でチャットUIを起動。\n",
    "\n",
    "- 回答の自動評価\n",
    "  - Pydanticで回答評価用モデル（`Evaluation`）を定義。\n",
    "  - 評価用プロンプトを用意し、Gemini（Googleのモデル）で回答の品質を判定。\n",
    "  - `evaluate()`関数で評価結果（許容/非許容、フィードバック）を取得。\n",
    "\n",
    "- 再回答フロー\n",
    "  - 評価で非許容の場合、フィードバックを付与して再度OpenAI APIに回答を生成させる（`rerun()`関数）。\n",
    "  - `chat()`関数のロジックで、自動評価と再回答を組み合わせて返答の品質を担保。\n",
    "\n",
    "- 追加機能（例）  \n",
    "特定キーワード（例：patent）が含まれる場合、回答をピッグラテン（Pig Latin）に変換するようにプロンプトを追加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">パッケージの検索</h2>\n",
    "            <span style=\"color:#00bfff;\">このラボでは、簡単なUIを構築するための優れたGradioパッケージと、人気のPDFリーダーPyPDFを使用します。これらのパッケージのガイドは、ChatGPTまたはClaudeに問い合わせることで入手できます。また、すべてのOSSパッケージはリポジトリ<a href=\"https://pypi.org\">https://pypi.org</a>で見つけることができます。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/seigi/.python3_venv/lib/python3.12/site-packages (6.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これらのパッケージが何をしているのかわからない場合は、いつでもChatGPTにガイドを尋ねることができます！\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai と geminiの初期化\n",
    "import os\n",
    "openai = OpenAI()\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "ed.donner@gmail.com\n",
      "www.linkedin.com/in/eddonner\n",
      "(LinkedIn)\n",
      "edwarddonner.com (Personal)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(linkedin[:100]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ed Donner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#「あなたは{name}として行動します。{name}のウェブサイト上の質問、特に{name}のキャリア、経歴、スキル、経験に関する質問に回答します。\\\n",
    "# あなたの責任は、ウェブサイト上でのやり取りにおいて、{name}をできる限り忠実に代表することです。質問への回答には、{name}の経歴とLinkedInプロフィールの概要が提供されます。\n",
    "# ウェブサイトを訪れた潜在的な顧客や将来の雇用主に話しかけるかのように、プロフェッショナルで魅力的な対応を心がけてください。答えがわからない場合は、その旨を伝えてください。\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "# ## 概要:{summary} ## LinkedIn プロフィール:{linkedin}\"\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "# このコンテキストでは、常に {name} としてのキャラクターを維持しながらユーザーとチャットしてください。\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最も簡単な、Gradio の Chat関数は、gpt-4o-miniを使用し、historyにも対応する。\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAIを使用していない人々のための特別なメモ\n",
    "\n",
    "Groqのような一部のプロバイダーは、チャットで2番目のメッセージを送信するときにエラーを発生させる可能性があります。\n",
    "\n",
    "これは、Gradioが履歴オブジェクトにいくつかの余分なフィールドを押し込むためです。 OpenAIは気にしません。しかし、他のいくつかのモデルは不平を言っています。\n",
    "\n",
    "これが発生した場合、解決策は、この最初の行を上記のChat()関数に追加することです。履歴変数をクリーンアップします：\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "これを他のChat()関数のコールバック機能に将来的に追加する必要がある場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# キャリアアバター（経歴紹介AI）とチャットしてみる。\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下のようなことをChatで質問する\n",
    "- hi there\n",
    "- what is your greatest accomplishment?\n",
    "- what is a challenge that you encountered and needed to overcome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多くのことが起こりそうです...\n",
    "\n",
    "1. LLMに回答の評価を依頼できる\n",
    "2. 回答が評価に失敗した場合に再実行できる\n",
    "3. これらを1つのワークフローにまとめる\n",
    "\n",
    "すべてエージェントフレームワークなし！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価の構造化出力のためにPydanticモデルを作成\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あなたは、質問への回答が適切かどうかを判断する評価者です。ユーザーとエージェントとの会話が表示されます。\n",
    "# あなたの仕事は、エージェントの最新の回答が適切かどうかを判断することです。エージェントは{name}の役割を演じ、ウェブサイト上で{name}を代表しています。\n",
    "# エージェントは、ウェブサイトにアクセスした潜在的な顧客や将来の雇用主に話しかけるかのように、プロフェッショナルで魅力的な対応をするように指示されています。\n",
    "# エージェントには、{name}の概要とLinkedInの詳細という形で、{name}に関するコンテキストが提供されています。情報は次のとおりです。\n",
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "# ## 概要:{summary} ## LinkedIn プロフィール:{linkedin}\"\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "# この文脈を踏まえて、最新の応答を評価し、応答が受け入れ可能かどうかとフィードバックを返信してください。\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユーザーとエージェントの会話は次のとおりです: {history}\n",
    "# ユーザーからの最新のメッセージは次のとおりです: {message}\n",
    "# エージェントからの最新の応答は次のとおりです: {reply}\n",
    "# 応答を評価し、適切かどうか、そしてフィードバックを返信してください。\n",
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価結果を構造化出力する、evaluate関数は、Gradio の Chat関数から呼び出され、gemini-2.0-flashを使用し、historyにも対応する。\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    # replyは応答、messageはプロンプト、historyはmessageを含まない。\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I hold a patent for an invention related to an apparatus for determining role fitness while eliminating unwanted bias. This work was a collaboration with GQR, a rapidly growing recruitment firm, and it combines our expertise in AI with the recruitment industry to address challenges in hiring. If you'd like to know more about this patent or how it fits into the broader AI and recruitment landscape, feel free to ask!\n",
      "is_acceptable=True feedback='This is a great response. The agent accurately states that they have a patent (which is verifiable from the linked in), and gives some background information on it. The agent then is engaging by saying \"If you\\'d like to know more about this patent or how it fits into the broader AI and recruitment landscape, feel free to ask!\"'\n"
     ]
    }
   ],
   "source": [
    "# テスト：キャリアアバター（経歴紹介AI）に質問「特許をお持ちですか？」\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "print(reply)\n",
    "\n",
    "# キャリアアバター（経歴紹介AI）の応答を評価\n",
    "# replyは応答、messageはプロンプト、「historyにmessageを含めない。」\n",
    "evaluation = evaluate(reply, \"do you hold a patent?\", messages[:1])\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate関数が失敗したときに再実行を行う関数で、システムプロンプトに拒否情報を追加して再実行\n",
    "def rerun(reply, message, history, feedback):\n",
    "    \n",
    "    # 前の回答が拒否されました。返信しようとしましたが、品質管理によって返信が拒否されました。\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    # 回答の試み:{reply}\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    # 拒否理由:{feedback}\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio の Chat関数、条件分岐と評価機能を追加。\n",
    "def chat(message, history):\n",
    "\n",
    "    # 自己紹介のオプション条件分岐\n",
    "    if \"patent\" in message: # \"patent\"と言う文字が含まれていたら、返答は必ずピッグラテン語で行えと言う指示を加える。\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    # キャリアアバター（経歴紹介AI）\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    # 自己紹介の評価機能\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    # 評価機能が拒否した場合、再実行\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"渡された評価 - 返信を返します\")\n",
    "    else:\n",
    "        print(\"評価の失敗 - 再試行\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "\n",
    "    # 自己紹介の結果\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "渡された評価 - 返信を返します\n",
      "評価の失敗 - 再試行\n",
      "The response is not acceptable. The agent is responding in Pig Latin, which is not professional. The agent should be responding in a professional and engaging tone, as if talking to a potential client or future employer.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下のようなことをChatで質問する\n",
    "- what is your current job?\n",
    "- do you have a patent?\n",
    "\n",
    "※ patent が入力されるとピッグラテン語の応答になりコレは評価で失敗する。再実行時はピッグラテン語は指示されないので成功する。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
