{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87b4d10-5704-4796-8d16-5797523647c9",
   "metadata": {},
   "source": [
    "# o'reillyのネゴザメ言語モデルの本\n",
    "\n",
    "## 文字列のベクトル化（１）\n",
    "カウントベースの手法 > 共起頻度アプローチ\n",
    "\n",
    "## [目次](TableOfContents.ipynb)\n",
    "- [環境準備](#環境準備)\n",
    "  - [インストール](#インストール)\n",
    "  - [インポート](#インポート)\n",
    "  - [共通関数](#共通関数)\n",
    "- カウントベースの手法 > 共起頻度アプローチ\n",
    "  - [共起行列](#共起行列)\n",
    "  - [コサイン類似度](#コサイン類似度)\n",
    "  - [類似単語ランキング](#類似単語ランキング)\n",
    "  - [共起行列をPPMI行列に変換](#共起行列をPPMI行列に変換)\n",
    "  - [SVDによる次元削減](#SVDによる次元削減)\n",
    "  - [大規模データセットの次元削減](#大規模データセットの次元削減)\n",
    "  \n",
    "## 参考\n",
    "- https://github.com/oreilly-japan/deep-learning-from-scratch-2/tree/master/ch01\n",
    "- [言語処理（AI）> ベクトル化 - 開発基盤部会 Wiki](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%EF%BC%88AI%EF%BC%89#k2009143) > [カウントベースの手法 > 共起頻度アプローチ\n",
    "](https://dotnetdevelopmentinfrastructure.osscons.jp/index.php?%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%EF%BC%88AI%EF%BC%89#o8dc8c2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f149dd-6504-4850-99b9-626887faba7d",
   "metadata": {},
   "source": [
    "## 環境準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d975a-b9ce-4eff-a3f5-ae65d56655a9",
   "metadata": {},
   "source": [
    "### インストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89147361-440c-47a1-b02b-942f39590bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377a319-9b97-4c21-9d8f-8664c955b33e",
   "metadata": {},
   "source": [
    "### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814784dc-0f79-448a-9608-e9952549ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b638958-9d39-4dd1-9d46-f2f97d5849fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a911b4d-bd11-42a5-ad32-4e055f661ddc",
   "metadata": {},
   "source": [
    "## カウントベースの手法 > 共起頻度アプローチ\n",
    "コーパスの共起関係を行列として表し行列分解の手法を適用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7c6f2-b9f3-41be-9584-805908fa6c79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共起行列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b653c8-8903-4bb6-9389-f58a1e3e5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print('\\n corpus:')\n",
    "print(corpus)\n",
    "print('\\n word_to_id:')\n",
    "print(word_to_id)\n",
    "print('\\n id_to_word:')\n",
    "print(id_to_word)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# 共起行列\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "print('\\n 共起行列:')\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b2e5f-ed2a-4a83-9cad-3a5ec4623f73",
   "metadata": {},
   "source": [
    "### コサイン類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e1d00-b572-4410-bf76-bb1b7f79094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = C[word_to_id['you']]  #「you」の単語ベクトル\n",
    "c1 = C[word_to_id['i']]  #「i」の単語ベクトル\n",
    "print('\\n コサイン類似度:')\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2334f-d955-407e-b11e-ac2a0bc08010",
   "metadata": {},
   "source": [
    "### 類似単語ランキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8c6eb-562f-4697-a634-b83c677cff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "print('\\n 類似単語ランキング:')\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15295c3d-0799-4b3c-a1b0-1db545f92b3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 共起行列をPPMI行列に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d6a1a-70a3-4955-a2a3-9842eb4425c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁3桁で表示\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "# 共起行列\n",
    "print('co-occurrence matrix')\n",
    "C = create_co_matrix(corpus, len(word_to_id))\n",
    "print(C)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "# PPMI行列\n",
    "W = ppmi(C)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e9ff6-378b-4326-a9b0-65ded051fb38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SVDによる次元削減\n",
    "SVD(特異値分解)  \n",
    "- SVDでスパース（疎）な大規模行列をデンス（密）な大規模行列に変換\n",
    "- Uの先頭の次元が重要度が高いのでスライシングで切り捨てる。\n",
    "- ${X} = {U}{S}{V}^{T}$\n",
    "  - Sの中の特異値が小さいものは重要度が低い。\n",
    "  - 小さい特異値を削除することでSの行方向と列方向を削減し、\n",
    "  - Uを列方向に削減したU'、Vを行方向に削減したV'が作成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc435d8-b6d5-4e8a-a39b-471f0bd3c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id) # 語彙サイズ\n",
    "window_size = 1 # 共起の最大距離\n",
    "\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "\n",
    "print('counting  co-occurrence  matrix...')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('calculating PPMI ...')\n",
    "print(W)\n",
    "print('-'*50)\n",
    "print('calculating SVD ...')\n",
    "print('U:')\n",
    "print(U)\n",
    "print('S:')\n",
    "print(S)\n",
    "print('V:')\n",
    "print(V)\n",
    "\n",
    "# 先頭２要素をplot\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d88bb7-1171-4318-8c08-cb16652f45b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 大規模データセットの次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105da01b-eb71-4ee6-8c03-b0c861b630f5",
   "metadata": {},
   "source": [
    "#### 大規模なPTBデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84351859-c626-4c66-ae47-7222c40443ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "print()\n",
    "print('corpus size:', len(corpus))\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9cd23f-583c-4c19-b6cc-ab98aaba839e",
   "metadata": {},
   "source": [
    "#### LSAによる次元削減\n",
    "LSA(潜在意味解析)  \n",
    "- LSAはTruncated SVDとも言う、SVDを使った次元削減法\n",
    "- 特異値の大きなものに限定して計算することで高速化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e038f-e300-4e9a-a45d-4ab2abf41e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nekozame.common.util import most_similar, create_co_matrix, ppmi\n",
    "from nekozame.dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "wordvec_size = 100\n",
    "vocab_size = len(word_to_id) # 語彙サイズ\n",
    "window_size = 2 # 共起の最大距離\n",
    "\n",
    "# 共起行列\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('-'*50)\n",
    "\n",
    "# PPMI行列に変換\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "print('-'*50)\n",
    "\n",
    "# truncated SVDで変換\n",
    "print('calculating truncated SVD ...')\n",
    "\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(U)\n",
    "print('-'*50)\n",
    "\n",
    "# 100次元に削減し類似単語ランキング\n",
    "print('calculating most similar ...')\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
