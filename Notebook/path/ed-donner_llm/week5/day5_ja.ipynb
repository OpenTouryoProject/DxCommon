{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## 専門知識労働者\n",
    "- 保険テクノロジー企業である Insurellm の従業員が使用する、専門知識を持つナレッジ ワーカーである質問応答エージェント。エージェントは正確である必要があり、ソリューションは低コストである必要があります。\n",
    "- このプロジェクトでは、RAG（検索拡張生成）を使用して、質問/回答アシスタントが高い精度を確保します。ココ、5つめの実装では、RAGのChatインターフェイスからの試行を幾つか修正し動作を確認します。\n",
    "  - OpenAI → Ollama\n",
    "  - チャンク数を25に増やす\n",
    "  - デバッグ用ハンドラを追加\n",
    "  - EXERCISEで引用付きに変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain、Plotly、およびChromaの輸入\n",
    "\n",
    "# langchain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# VDBサポート (Chroma)\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 価格は重要な要素（と言う建付け）なので、低コストモデルを採用\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# コレはVDBのChromaのDB名\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .envファイルから環境変数をロード\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 123\n",
      "Document types found: {'products', 'employees', 'contracts', 'company'}\n"
     ]
    }
   ],
   "source": [
    "# langChain のローダーを使用してKBのすべてのサブフォルダ内のすべてのドキュメントを読み取りリスト化\n",
    "# 余談：メタデータは、DirectoryLoader および TextLoader によって生成され、そこにカスタムの属性、doc_typeを追加している。\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "# 一部のユーザーに必要な修正を提供してくれた、コース受講生のCGとJon Rに感謝します。\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# それでもうまくいかない場合は、Windowsユーザーの中には次の行のコメントを解除する必要があるかもしれません。\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs]) # add_metadataを使うよう書き換えられている。\n",
    "\n",
    "# テキストを200文字の重複部分を持たせた1000文字ごとのチャンク（かたまり）に分割\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# document も chunk も LangChain の Document クラス\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\") # チャンク・リストの長さ\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\") # doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# 各チャンクに埋込ベクトルを関連付けるVDBに格納\n",
    "# Chroma は SQLite ベースの人気のOSS VDB\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Hugging Faceのフリーのベクトル埋め込みを使用したい場合\n",
    "# 次と、embeddings = OpenAIEmbeddings() を交換します：\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chroma DataStoreがすでに存在するかどうかを確認してください - もしそうなら、コレクションを削除してゼロから始める\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Chroma VectorStoreを作成してください！\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# ベクターを調査\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## ベクトルストアの可視化\n",
    "コードや表示される内容に変更はないので割愛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "# LangChainを使用してすべてをまとめる時間です"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## GradioのChatインターフェイスを使用して、これを紹介します。\n",
    "- LLMとのRAGチャットをプロトタイプする迅速かつ簡単な方法\n",
    "- `Who received the prestigious IIOTY award in 2023?` とでも聞いてみる。\n",
    "- ポイントは、既定のチャンク数を25に増やすと回答改善する理由をデバッグできた点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b55e9abb-e1da-46c5-acba-911868aee329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/tmp/ipykernel_33222/3000209947.py:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"\\ nanswer：\", answer)\n",
      "/tmp/ipykernel_33222/3000209947.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "- **2022**: **Satisfactory**  \n",
      "  Avery focused on rebuilding team dynamics and addressing employee concerns, leading to overall improvement despite a saturated market.  \n",
      "\n",
      "- **2023**: **Exceeds Expectations**  \n",
      "  Market leadership was regained with innovative approaches to personalized insurance solutions. Avery is now recognized in industry publications as a leading voice in Insurance Tech innovation.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2020:**  \n",
      "  - Completed onboarding successfully.  \n",
      "  - Met expectations in delivering project milestones.  \n",
      "  - Received positive feedback from the team leads.\n",
      "\n",
      "- **2021:**  \n",
      "  - Achieved a 95% success rate in project delivery timelines.  \n",
      "  - Awarded \"Rising Star\" at the annual company gala for outstanding contributions.  \n",
      "\n",
      "- **2022:**  \n",
      "  - Exceeded goals by optimizing existing backend code, improving system performance by 25%.  \n",
      "  - Conducted training sessions for junior developers, fostering knowledge sharing.  \n",
      "\n",
      "- **2023:**  \n",
      "  - Led a major overhaul of the API internal architecture, enhancing security protocols.  \n",
      "  - Contributed to the company’s transition to a cloud-based infrastructure.  \n",
      "  - Received an overall performance rating of 4.8/5.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2018**: **3/5** - Adaptable team player but still learning to take initiative.\n",
      "- **2019**: **4/5** - Demonstrated strong problem-solving skills, outstanding contribution on the claims project.\n",
      "- **2020**: **2/5** - Struggled with time management; fell behind on deadlines during a high-traffic release period.\n",
      "- **2021**: **4/5** - Made a significant turnaround with organized work habits and successful project management.\n",
      "- **2022**: **5/5** - Exceptional performance during the \"Innovate\" initiative, showcasing leadership and creativity.\n",
      "- **2023**: **3/5** - Maintaining steady work; expectations for innovation not fully met, leading to discussions about goals.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.5/5  \n",
      "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
      "\n",
      "- **2022:** Rating: 3.0/5  \n",
      "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
      "\n",
      "- **2021:** Rating: 4.0/5  \n",
      "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
      "\n",
      "- **2020:** Rating: 3.5/5  \n",
      "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
      "  *Annual bonus based on successful project completions and performance metrics.*\n",
      "Human: Who received the prestigious IIOTY award in 2023?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\\ nanswer： I don't know.\n"
     ]
    }
   ],
   "source": [
    "# 舞台裏で送られるものを調査\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# OpenAIとの新しいチャットを作成します\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# 代替品 - Ollamaをローカルに使用したい場合は、このラインを除外\n",
    "# llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# チャットの会話メモリを設定します\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# RAGに使用されるベクターストアの抽象化。\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# それをまとめる：GPT 4o-Mini LLM、ベクトルストア、メモリ、デバッグ用ハンドラで会話チェーンをセットアップ\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# 簡単な質問だが、ココでは対象のチャンクが取得できず回答できない。\n",
    "query = \"Who received the prestigious IIOTY award in 2023?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\ nanswer：\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a3607e-526c-4fcc-8aa7-5bc9741ae788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGに使用されるベクターストアの抽象化。 Kは、使用するチャンクの数\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# それをまとめる：GPT 4o-Mini LLM、ベクトルストア、メモリ、デバッグ用ハンドラで会話チェーンをセットアップ\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関数のラッピング - 履歴の記憶はconversation_chainにあるため、Gradioのhistoryは使用されていない。\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gio: http://127.0.0.1:7862/: Operation not supported\n"
     ]
    }
   ],
   "source": [
    "# そしてGradioで：\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644753e7-17f3-4999-a37a-b6aebf1e4579",
   "metadata": {},
   "source": [
    "# 練習問題 - ソース引用を含む回答の生成\n",
    "https://github.com/ed-donner/llm_engineering/blob/main/week5/community-contributions/day5%20-%20generating%20answers%20with%20citations.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961da818-c11b-436b-bc2b-a5ea5e10717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b4745a-0a6c-4544-b78b-c827cfec1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# システム・プロンプト\n",
    "system_message = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Use the following markdown format to answer the question along with the Source used to generate the answer, add inline citation for each sentence & add end of the answer citations:\n",
    "'CEO of Insurellm is Avery Lancaster [[1]](Source Link 1). Who is also a co-founder [[2]](Source Link 2)\n",
    "Citations: (Note: No duplicates allowed in the below list)\n",
    "\n",
    "[1 - Source Title 1](Link 1)\n",
    "[2 - Source Title 2](Link 2)\n",
    "...\n",
    "[n - Source Title n](Link n)'\n",
    " \n",
    "Example answer: \n",
    "'CEO of Insurellm is Avery Lancaster [[1]](knowledge-base\\\\company\\\\about.md). Who is also a co-founder [[2]](knowledge-base\\\\employees\\\\Avery Lancaster.md)\n",
    "Citations:\n",
    "\n",
    "[1 - About Company](knowledge-base\\\\company\\\\about.md)\n",
    "[2 - Avery Lancaster employees](knowledge-base\\\\employees\\\\Avery Lancaster.md)'\n",
    " \n",
    "Important Note: Have unique end of the answer citations. Don't give duplicate citation numbers for the same source link, reuse the same citation number if the same source link is referenced multiple times.\n",
    "\"\"\"\n",
    "\n",
    "x = \"\"\"\n",
    "「あなたは質問回答タスクのアシスタントです。\n",
    "以下のコンテキスト情報を用いて質問に答えてください。\n",
    "答えがわからない場合は、わからないとだけ答えてください。\n",
    "回答は最大3文とし、簡潔にまとめてください。\n",
    "以下のマークダウン形式を用いて、回答の生成に使用した情報源とともに質問に回答し、各文にインライン引用を追加し、回答の末尾に引用を追加してください。\n",
    "「InsurellmのCEOはAvery Lancasterです[[1]](情報源リンク 1)。彼は共同創設者でもあります[[2]](情報源リンク 2)。」\n",
    "引用：（注：以下のリストでは重複は許可されていません）\n",
    "\n",
    "[1 - 情報源タイトル 1](リンク 1)\n",
    "[2 - 情報源タイトル 2](リンク 2)\n",
    "...\n",
    "[n - 情報源タイトル n](リンク n)」\n",
    "\n",
    "回答例：\n",
    "「InsurellmのCEOはAvery Lancasterです」 [[1]](knowledge-base\\\\company\\\\about.md)。共同創業者でもある[[2]](knowledge-base\\\\employees\\\\Avery Lancaster.md)\n",
    "引用：\n",
    "\n",
    "[1 - 会社概要](knowledge-base\\\\company\\\\about.md)\n",
    "[2 - Avery Lancaster 従業員](knowledge-base\\\\employees\\\\Avery Lancaster.md)\n",
    "\n",
    "重要：回答の末尾の引用は必ず一意にしてください。同じソースリンクに重複した引用番号を付けないでください。同じソースリンクが複数回参照されている場合は、同じ引用番号を再利用してください。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659b675e-1304-41fd-8d48-dfebbe585ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャンクにメタデータを追加\n",
    "def generate_user_prompt(message):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "    results = retriever.invoke(message)\n",
    "    doc_chunk_merged = \"\"\n",
    "    for doc_chunk in results:        \n",
    "        source = f\"https://github.com/ed-donner/llm_engineering/tree/main/week5/\" + doc_chunk.metadata.get(\"source\").replace(\"\\\\\",\"/\")\n",
    "        title = doc_chunk.metadata.get(\"doc_type\") + \" -> \" + source.split('\\\\')[-1][:-3]\n",
    "        doc_chunk_merged += f\"Content: {doc_chunk.page_content}\\n Source title: {title}\\n Source link: {source}\\n\\n\"\n",
    "    return f\"Question: {message}\\n {doc_chunk_merged}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ab4096-5d1d-4d43-8efb-cf049c59ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChainでretrieverにカスタム・コンテキスト（メタデータ）を追加する処理は面倒なので、素で処理する場合、以下のようになる。\n",
    "# ・LangChainでretrieverにカスタム・コンテキスト（メタデータ）を追加する処理は調査したがエラーで動作せず\n",
    "# ・LangChainは、簡単な動作のカスタマイズにも知識が必要になり、また、バージョンアップでの破壊的変更が問題だ。\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": generate_user_prompt(message)}]\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True, seed=3, max_tokens=1000)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed147410-0b8b-40f6-8c5e-7cdbab621319",
   "metadata": {},
   "source": [
    "### テストコードで `Please explain what Insurellm is in a couple of sentences` と聞いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ee24e94-8e56-4ab2-820f-013ddaaccf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Insurellm is an insurance tech startup founded by Avery Lancaster in 2015, aiming to innovate the insurance industry with its products. It offers various software solutions, including Markellm, a marketplace connecting consumers with insurance providers, and has expanded to 200 employees and over 300 clients worldwide [[1]](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/about.md) [[2]](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/overview.md). \n",
       "\n",
       "Citations:\n",
       "\n",
       "[1 - About Company](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/about.md)  \n",
       "[2 - Overview of Insurellm](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/overview.md)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the Answer generation - 1\n",
    "user_prompt = \"Please explain what Insurellm is in a couple of sentences\"\n",
    "\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in chat(user_prompt, []):\n",
    "    update_display(Markdown(chunk), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efbd35-7580-4234-902b-9d712785dd2c",
   "metadata": {},
   "source": [
    "### テストコードで `Please explain in short on what products are available in Insurellm` と聞いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1765aa2-8869-4f87-b2fc-e56271216bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Insurellm offers four key software products: Carllm, a portal for auto insurance companies; Homellm, designed for home insurance providers; Rellm, an enterprise platform for the reinsurance sector; and Marketllm, a marketplace connecting consumers with insurance providers [[1]](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/overview.md). Each product leverages advanced technologies to enhance user experience, streamline processes, and provide tailored solutions [[2]](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/products/Rellm.md). These offerings cater to both B2B and B2C customers, aiming to disrupt traditional insurance practices [[3]](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/about.md).\n",
       "\n",
       "Citations:\n",
       "[1 - Overview of Insurellm](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/overview.md)  \n",
       "[2 - Rellm Product Summary](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/products/Rellm.md)  \n",
       "[3 - About Insurellm](https://github.com/ed-donner/llm_engineering/tree/main/week5/knowledge-base/company/about.md)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Testing the Answer generation - 2\n",
    "user_prompt = \"Please explain in short on what products are available in Insurellm\"\n",
    "\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in chat(user_prompt, []):\n",
    "    update_display(Markdown(chunk), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a87d5-4b0f-4e73-9616-86bf55e8bae8",
   "metadata": {},
   "source": [
    "### Gradio の Chatで `Who received the prestigious IIOTY award in 2023?` と聞いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f3dd149-4f0d-47d0-9d8e-f9990015e0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Launch Gradio\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
