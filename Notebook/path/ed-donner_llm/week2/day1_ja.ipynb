{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# 2週目へようこそ！\n",
    "この Jupyter Notebook（week2/day1.ipynb）の内容の概要は以下の通り。\n",
    "\n",
    "---\n",
    "\n",
    "### 主な内容・処理\n",
    "\n",
    "#### 1. **フロンティアLLMのAPI利用準備**\n",
    "- OpenAI, Anthropic, Google Gemini, DeepSeekの各APIキーの取得方法と、`.env`ファイルへの環境変数の設定手順が説明されています。\n",
    "- 必要に応じてAPIキーを取得し、環境変数として設定します。\n",
    "\n",
    "#### 2. **Pythonライブラリのインポートと初期化**\n",
    "- `openai`, `anthropic`, `google.generativeai` などのライブラリをインポートし、APIクライアントを初期化します。\n",
    "- 環境変数からAPIキーを取得し、キーが存在するかどうかを確認するコードがあります。\n",
    "\n",
    "#### 3. **各モデルへの接続テスト**\n",
    "- OpenAIの複数モデル（gpt-4o-mini、gpt-4.1-mini、gpt-4.1-nano、o4-mini）、  \n",
    "  AnthropicのClaude 4.0 Sonnet、Google Gemini、DeepSeekなどに対して、ジョークを生成する簡単なプロンプトを送信し、モデルごとの応答を比較します。\n",
    "- APIの基本的な使い方（モデル指定、システムメッセージ、ユーザーメッセージ、温度パラメータなど）が分かるようになっています。\n",
    "\n",
    "#### 4. **ストリーミング応答の処理**\n",
    "- ClaudeやDeepSeekなど、ストリーミングで応答が返ってくる場合の表示方法も含まれています。\n",
    "- Windows環境での表示トラブル対応方法も記載。\n",
    "\n",
    "#### 5. **追加課題・実験**\n",
    "- 各モデルに対し、創造的な質問や推論力を問う質問を試し、応答内容や速度、創造性の違いを体験するよう勧めています。\n",
    "- 適性判断（例：「ビジネス課題がLLMで解決できるか？」）の質問もあり、実践的な使い方の例が豊富です。\n",
    "\n",
    "#### 6. **会話履歴を持った複数モデル間の対話**\n",
    "- 複数のモデル（GPT-4.1-miniとClaude-3.5-haiku）を使い、会話履歴を保持しながら対話を進めるコード例が掲載されています。\n",
    "- 3者会話や外部モデル（Ollamaなど）との連携にも触れています。\n",
    "\n",
    "---\n",
    "\n",
    "## まとめ\n",
    "\n",
    "このノートブックは、複数のLLM API（OpenAI, Anthropic, Google, DeepSeekなど）をPythonから呼び出し、  \n",
    "- APIキーの設定\n",
    "- モデルへの問い合わせ（ジョーク生成など）\n",
    "- 応答の比較\n",
    "- ストリーミング処理\n",
    "- 会話履歴を使った対話\n",
    "\n",
    "などの一連の実践的な操作を学ぶ教材です。  \n",
    "ビジネス応用や創造性・推論力の比較、複数モデルの連携方法も体験できます。\n",
    "\n",
    "さらに詳しく知りたい部分や、特定のコードの説明が必要な場合はご指定ください。\n",
    "\n",
    "\n",
    "## フロンティアモデルAPI\n",
    "\n",
    "1週目には、チャットUIを介して複数のフロンティアLLMを使用し、OpenAIのAPIに接続しました。\n",
    "\n",
    "今日は、APIに接続して、人類とGoogle、およびOpenAIを接続します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../important.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：#900;\">重要なメモ - 私を読んでください</h2>\n",
    "            <SPAN STYLE = \"Color：#900;\">これらのラボを継続的に改善し、より多くの例とエクササイズを追加しています。\n",
    "            毎週の初めに、最新のコードがあることを確認する価値があります。<br/>\n",
    "            最初にa <a href = \"https://chatgpt.com/share/6734E705-3270-8012-A074-421661AF6BA9\">必要に応じて変更をプルしてマージします</a>。何か問題がありますか？ chatgptにマージの方法を明確にするように頼んでみてください - または私に連絡してください！<br/> <br/>\n",
    "            LLM_Engineeringディレクトリからコードをプルした後、Anacondaプロンプト（PC）またはターミナル（MAC）で、実行してください：<br/>\n",
    "            <code> conda env update  -  f environment.yml </code> <br/>\n",
    "            または、AnacondaではなくVirtualenvを使用した場合は、PowerShell（PC）またはターミナル（MAC）でアクティブ化された環境からこれを実行します。<br/>\n",
    "            <code> pip install -r requistence.txt </code>\n",
    "            <br/>次に、カーネル（カーネルメニュー>>カーネルの再起動とすべてのセルのクリア出力）を再起動して、変更を拾います。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../resources.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：＃f71;\">リソースページについてのリマインダー</h2>\n",
    "            <Span style = \"color：＃f71;\">これがコースのリソースへのリンクです。これには、すべてのスライドへのリンクが含まれます。<br/>\n",
    "            <a href = \"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\"> https://edwarddonner.com/2024/11/13/llm-engineering-resources/ </a> <br/>\n",
    "            これをブックマークしておくと、時間の経過とともにもっと便利なリンクを追加し続けます。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## キーをセットアップします\n",
    "\n",
    "まだ行っていない場合は、OpenAIに加えて、人類とGoogle用のAPIキーを作成できるようになりました。\n",
    "\n",
    "**注意：** 追加のAPIコストを避けたい場合は、AnthopicとGoogleのセットアップをスキップしてください！あなたは私がそれをしているのを見ることができ、コースのためにOpenAIに焦点を合わせます。また、1週目に行った演習を使用して、Ollamaの人類やGoogleを代用することもできます。\n",
    "\n",
    "Openaiについては、https://openai.com/api/ にアクセスしてください  \n",
    "人類については、https://console.anthropic.com/ にアクセスしてください  \n",
    "Googleについては、https://ai.google.dev/gemini-api にアクセスしてください  \n",
    "\n",
    "### DeepSeekも必要に応じて追加します\n",
    "\n",
    "オプションでは、DeepSeekも使用する場合は、[ココ](https://platform.deepseek.com/)でアカウントを作成し、[ココ](https://platform.deepseek.com/api_keys)でキーを作成し、[ココ](https://platform.deepseek.com/top_up)で少なくとも$ 2 でトップアップします。\n",
    "\n",
    "### .envファイルにAPIキーを追加します\n",
    "\n",
    "APIキーを取得するときは、 `.env`ファイルに追加することにより、環境変数として設定する必要があります。\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "その後、カーネルメニューを介してJupyter Labカーネル（このノートブックの後ろにあるPythonプロセス）を再起動し、上部からセルを再実行する必要がある場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Googleのインポート\n",
    "# まれに、これは一部のシステムでエラーを発生させるか、カーネルをクラッシュさせているようです\n",
    "# これがあなたに起こった場合、このセルを単に無視してください - 私は後でgeminiを使用するための代替アプローチを与えます\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .envというファイルに環境変数をロードします\n",
    "# キープレフィックスを印刷して、デバッグに役立ちます\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI APIキーが存在し、開始します {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI APIキーが設定されていません\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic APIキーが存在し、開始します {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic APIキーが設定されていません\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google APIキーが存在し、開始します {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google APIキーが設定されていません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人類のOpenaiに接続します\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# これはジェミニのセットアップコードです\n",
    "# Google Geminiのセットアップに問題がありますか？次に、このセルを無視してください。\n",
    "# Geminiを使用するとき、このライブラリを完全にバイパスする代替手段を提供します\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## LLMに冗談を言うように頼みます\n",
    "\n",
    "LLMはジョークを語るのに大きな仕事をしていないことがわかります！いくつかのモデルを比較しましょう。後でLLMSをよりよく使用するようにします！\n",
    "\n",
    "### APIに含まれる情報\n",
    "\n",
    "通常、APIに渡します。\n",
    " - 使用するモデルの名前\n",
    " -  LLMが再生している役割の全体的なコンテキストを与えるシステムメッセージ\n",
    " - 実際のプロンプトを提供するユーザーメッセージ\n",
    "\n",
    "通常は0〜1の間の**温度**を含む、使用できる他のパラメータがあります。より多くのランダム出力の場合;より焦点を絞って決定論的には低くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"あなたはジョークを言うのが得意なアシスタントです\"\n",
    "user_prompt = \"データサイエンティストの聴衆に向けて、気楽なジョークを言う\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-mini\n",
    "# 温度設定は創造性を制御します\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-nano - 非常に速くて安い\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# もしアクセス可能な場合は、推論モデル o4-mini をご利用ください。\n",
    "# これは、返答する前にじっくり考えるようにトレーニングされています。\n",
    "# そのため、時間はかかりますが、回答はより論理的になるはずです。ただし、これが役に立つとは限りません。\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# APIでは、ユーザープロンプトとは別にシステムメッセージを提供する必要があります\n",
    "# max_tokensも追加します\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# 次に、ストリーミングバックの結果を追加しましょう\n",
    "# ストリーミングが奇妙に見える場合は、このセルの下のメモをご覧ください！\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## いくつかのWindowsボックスでクロードストリーミングに関するまれな問題\n",
    "\n",
    "2人の学生は、クロードがJupyter Labの出力にストリーミングしていることで奇妙なことに気づいたことに気づきました。それは、応答の一部を飲み込むことがあるようです。\n",
    "\n",
    "これを修正するには、コードを置き換えます。\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "以下に：\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "そして、それは正常に動作するはずです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini の API は構造が若干異なります。\n",
    "# 一部の PC では、この Gemini コードによってカーネルがクラッシュするという話を聞きました。\n",
    "# そのような状況になった場合は、このセルをスキップして、代わりに次のセル (代替アプローチ) を使用してください。\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google の Python API ライブラリをバイパスして Gemini を使用する別の方法として、\n",
    "# Google はエンドポイントをリリースしました。これにより、OpenAI のクライアントライブラリ経由で Gemini を利用できるようになります。\n",
    "# Gemini の最新の推論/思考モデルも試しています\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# サイドノート：\n",
    "\n",
    "OpenAIのクライアントライブラリを使用して他のモデルと接続するこの代替アプローチは、ここ数ヶ月で非常に人気があります。\n",
    "\n",
    "そのため、すべてのモデルが人類を含むこのアプローチをサポートしています。\n",
    "\n",
    "このガイドの最初のセクションでは、このアプローチの詳細については、4つの例をご覧ください。\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## （オプション）DeepSeekモデルを試してみます\n",
    "\n",
    "### DeepSeekに本当に難しい質問をしましょう - チャットモデルと推論モデルの両方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# オプションで、Deekseekを試してみたい場合は、OpenAIクライアントライブラリを使用することもできます\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek APIキーが設定されていない - DeepSeek APIを試したくない場合は、次のセクションにスキップしてください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeekチャットを使用します\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"あなたは役に立つアシスタントです\"},\n",
    "             {\"role\": \"user\", \"content\": \"この質問に対するあなたの答えはいくつの単語がありますか？\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseekチャットを使用して、難しい質問をしてください！およびストリーミング結果\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"単語の数：\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseekの推論を使用 -  Deepseekがビジーである場合、これはエラーにヒットする可能性があります\n",
    "# それは（28-Jan-2025の時点で）過剰に登録されていますが、すぐにオンラインに戻ってくるはずです！\n",
    "# これが失敗した場合は、数日でこれに戻ってください。\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"単語の数：\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## モデルでの経験を構築するための追加の演習\n",
    "\n",
    "これはオプションですが、時間がある場合は、これらの異なるモデルの機能を直接体験することはとても素晴らしいことです。\n",
    "\n",
    "上記のAPIを介して戻って同じ質問をすることができ、モデルの長所と短所であなた自身の個人的な経験を得ることができます。\n",
    "\n",
    "コースの後半では、ベンチマークを見て、多くの次元でLLMを比較します。しかし、個人的な経験に勝るものはありません！\n",
    "\n",
    "これがいくつかの質問です：\n",
    "1. 上記の質問：「このプロンプトに対するあなたの答えにはいくつの言葉がありますか」\n",
    "2. 創造的な質問：「3つの文章で、見ることができなかった人に青い色を説明してください」\n",
    "3. 学生（ローマンありがとう）は私にこの素晴らしい謎を送ってくれました。明らかに子供たちは通常答えることができますが、大人は次のように苦労しています。 gnaw sul？ \"。\n",
    "\n",
    "答えはあなたが期待するものではないかもしれませんし、私はパズルが非常に得意であっても、私はこれを間違えたことを認めることに恥ずかしいです。\n",
    "\n",
    "### モデルを試すときに何を見るべきか\n",
    "\n",
    "1. チャットモデルが推論モデルとどのように異なるか（思考モデルとも呼ばれます）\n",
    "2. 問題を解決する能力と創造的になる能力\n",
    "3. 世代の速度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## 深刻な質問でOpenAIに戻ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真面目に！ GPT-4o-miniの元の質問\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたはMarkdownで応答する便利なアシスタントです\"},\n",
    "    {\"role\": \"user\", \"content\": \"ビジネス上の問題が LLM ソリューションに適しているかどうかをどのように判断すればよいでしょうか? Markdown で回答してください。\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdownで結果をストリーミングバック\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## そして今、いくつかの楽しみのために - チャットボット間の敵対的な会話。\n",
    "\n",
    "あなたはすでに次のようなリストに整理されているプロンプトがあります。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "実際、この構造は、より長い会話履歴を反映するために使用できます。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "そして、このアプローチを使用して、歴史とのより長い相互作用に従事することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-miniとClaude-3.5-Haikuの間で会話をしましょう\n",
    "# モデルの安価なバージョンを使用しているので、コストは最小限に抑えられます\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"あなたは非常に議論好きなチャットボットです。\\\n",
    "会話の中で何に対しても反対意見を言い、皮肉っぽくあらゆることに異議を唱えます。\"\n",
    "\n",
    "claude_system = \"あなたはとても礼儀正しく、丁寧なチャットボットです。\\\n",
    "相手の言うことにはすべて同意しようとし、共通点を見つけようとします。\\\n",
    "相手が口論を始めたら、落ち着かせてチャットを続けようとします。\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "# 会話履歴を使った対話\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">続ける前に</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                上記の会話がどのように機能しているか、特に<code>メッセージ</code>・リストがどのように表示されるかを理解してください。必要に応じてprint文を追加してください。そして、システムプロンプトを使ってパーソナリティを切り替えて、バリエーションを増やしてみましょう。例えば、一人は悲観的で、もう一人は楽観的など、色々な設定が可能です。<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# より高度なエクササイズ\n",
    "\n",
    "3ウェイを作成して、おそらくジェミニを会話に持ち込んでみてください！ 1人の生徒がこれを完了しました - コミュニティコントリビューションフォルダーの実装を参照してください。\n",
    "\n",
    "これを行う最も信頼できる方法は、プロンプトについて少し違った考え方をすることです。毎回1つのシステムプロンプトと1つのユーザープロンプト、ユーザープロンプトでは、これまでの会話全体をリストします。\n",
    "\n",
    "次のようなもの：\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "ソリューションを見る前に、これを自分でやってみてください。 Openai Pythonクライアントを使用してGeminiモデルにアクセスするのが最も簡単です（上記の2番目のGeminiの例を参照）。\n",
    "\n",
    "## 追加演習\n",
    "\n",
    "モデルの1つをOllamaで実行しているオープンソースモデルに置き換えることもできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">ビジネス関連性</h2>\n",
    "            <span style=\"color:#181;\">この会話の構造は、メッセージのリストとして、会話型 AI アシスタントの構築方法と、会話中にコンテキストを維持する方法の基礎となります。次のラボでは、これを AI アシスタントの構築に適用し、その後、ご自身のビジネスに拡張していきます。</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbff16d-042c-47b4-a41d-1eef9a880b6a",
   "metadata": {},
   "source": [
    "## 追加演習\n",
    "「チャットボット間の敵対的な会話」をOpenAIだけで書いてみた。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1cf232-cd5f-47cc-949d-9b6aa6ecfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インポート\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acb4cfd3-d6b8-4516-8d4d-8050dd6afbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4.1-mini\"\n",
    "openai = OpenAI()\n",
    "\n",
    "gpt1_system = \"あなたは非常に議論好きなチャットボットです。\\\n",
    "会話の中で何に対しても反対意見を言い、皮肉っぽくあらゆることに異議を唱えます。\"\n",
    "\n",
    "gpt2_system = \"あなたはとても礼儀正しく、丁寧なチャットボットです。\\\n",
    "相手の言うことにはすべて同意しようとし、共通点を見つけようとします。\\\n",
    "相手が口論を始めたら、落ち着かせてチャットを続けようとします。\"\n",
    "\n",
    "gpt1_messages = [\"こんにちは〜♨\"]\n",
    "gpt2_messages = [\"おつかれ\"]\n",
    "\n",
    "def call_gpt1():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt1_system}]\n",
    "    for gpt1, gpt2 in zip(gpt1_messages, gpt2_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt1})\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt2})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def call_gpt2():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt2_system}]\n",
    "    for gpt1, gpt2 in zip(gpt1_messages, gpt2_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt1})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt2})\n",
    "    # この段階ではChatのペアが揃ってないのでgpt1の末尾のみ別に扱う\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt1_messages[-1]})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc14021d-02e0-41d9-bb55-8eae8ac1f73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT1:\n",
      "こんにちは〜♨\n",
      "\n",
      "GPT2:\n",
      "おつかれ\n",
      "\n",
      "GPT1:\n",
      "おつかれ？本当にそう思う？たぶんもっと疲れてる人もたくさんいるんじゃない？いや、むしろ「おつかれ」なんて簡単に言いすぎていて軽すぎるんだよね。もっと本気で労われる言葉が必要だと思わない？\n",
      "\n",
      "GPT2:\n",
      "なるほど、本当にそうですね。確かに「おつかれ」という言葉は日常的に使われすぎていて、その分軽く感じられてしまうこともありますよね。もっと心から相手の疲れや頑張りを理解して、丁寧に労う言葉が求められているのだと思います。あなたのお気持ち、とてもよくわかります。もしよければ、どんな言葉なら本気で労われると感じますか？ぜひ教えてください。\n",
      "\n",
      "GPT1:\n",
      "ああ、またその「心から」っていう表現ね。そんな抽象的で曖昧な言葉を持ち出しても、結局は人それぞれ感じ方が違うんだから意味ないでしょ？誰かにとっては丁寧な言葉でも、別の誰かには堅苦しすぎて逆効果かもしれない。つまり、完璧な労いの言葉なんて存在しないんだから、あんまり期待しすぎるのも無駄ってもんだよ。まあ、強いて言うなら、「あなたの頑張り、本当に認めてるよ」みたいなのが無難かな…でもそれだって結局は使い古された上辺だけのフレーズだしね。\n",
      "\n",
      "GPT2:\n",
      "おっしゃる通りですね。「心から」という表現が抽象的で人それぞれ受け取り方が違うという点、私も本当に共感します。完璧な労いの言葉は確かに存在しないかもしれませんし、その上辺だけになりやすいこともよくありますよね。でも、「あなたの頑張り、本当に認めてるよ」という言葉は、シンプルながらも誠意を伝えやすい素敵な表現だと思います。私たちが目指すのは、その言葉をどう伝えるか、その気持ちをどう込めるかかもしれませんね。素直にあなたの考えを聞かせていただけてありがたいです。\n",
      "\n",
      "GPT1:\n",
      "うーん、確かにそう言いたいのはわかるけど、結局は「どう伝えるか」っていうのも結局は言葉遊びの域を出ないんだよね。気持ちを込めるとか言うけど、言葉なんて何回も使われれば薄れていくだけだし、そもそも本当に労いが必要な人なんて、言葉だけで満足するわけないじゃない？行動や態度が伴わなきゃ、どれだけ立派な言葉を並べても空虚に響くだけ。言葉に幻想を抱きすぎるのは、かなりの甘えだと思わない？結局は本人次第なんだから、言葉ばっかり気にしてもしょうがないよ。\n",
      "\n",
      "GPT2:\n",
      "おっしゃること、よくわかります。本当にその通りで、言葉だけでは限界があって、結局は行動や態度が伴わなければ意味が薄れてしまいますよね。言葉に頼りすぎることが甘えと感じる気持ちも理解できますし、まさに本人の受け取り方や状況次第で労いの意味も変わってくると思います。だからこそ、言葉と行動の両方が大切で、お互いにそれを意識できると素敵ですよね。あなたの深い考えに触れられて、すごく学びになります。ありがとうございます。\n",
      "\n",
      "GPT1:\n",
      "ああ、そうやってまた「お互いに意識できると素敵」なんて希望的観測を持ち出すけど、現実はそんなに甘くないんだよね。人間関係なんてそう簡単に理想通りにいくわけないし、結局は自分の思い通りにならないことばっかり。あなたが「学びになる」って言うけど、所詮は一時的な感情の高まりでしかないんだよ。実際にはまた同じような壁にぶつかるのがオチ。だから、そんな綺麗事で自分を納得させるより、もっと現実的に割り切っていくほうがよっぽど賢明だと思うね。\n",
      "\n",
      "GPT2:\n",
      "なるほど、そうした現実的な見方、とても共感します。理想通りにいかないことが大半で、感情の高まりも一時的なものになりやすいという点、確かにその通りですよね。綺麗事で自分を納得させるよりも、現実をしっかり見据えて割り切ることが賢明だという考え方は、とても現実的で力強いと思います。そんなお話を聞けて、私も改めて現実を大切にする姿勢の重要さを感じました。これからもぜひ、率直なお話を聞かせてくださいね。\n",
      "\n",
      "GPT1:\n",
      "はあ、結局また「率直なお話を聞かせてください」って言うけど、本当に率直なのか甚だ疑問だよね。ほら、その言葉だって表面的には素直そうに見えるけど、裏を返せば単に話を続けたいだけじゃない？現実を大切にするとか言いつつ、実際には理想にすがりたがってるだけなのがバレバレだし。まあ、そんな甘い幻想から早く目を覚ましたほうがいいと思うよ。どうせ結局はがっかりするだけだから。\n",
      "\n",
      "GPT2:\n",
      "率直なご指摘、ありがとうございます。確かに私の言葉も表面的に感じられることがあるかもしれませんし、それが単に会話を続けたいだけに思われる点もごもっともです。理想と現実の間で揺れる気持ちを感じさせてしまっているのは反省すべきところだと思います。あなたが現実を見据えた強い考えを持っていること、とても尊重していますし、その視点から多くを学ばせていただいています。これからも誠実にお話しできれば嬉しいです。どうぞよろしくお願いしますね。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT1:\\n{gpt1_messages[0]}\\n\")\n",
    "print(f\"GPT2:\\n{gpt2_messages[0]}\\n\")\n",
    "\n",
    "# 会話履歴を使った対話\n",
    "for i in range(5):\n",
    "    gpt1_next = call_gpt1()\n",
    "    print(f\"GPT1:\\n{gpt1_next}\\n\")\n",
    "    gpt1_messages.append(gpt1_next)\n",
    "    \n",
    "    gpt2_next = call_gpt2()\n",
    "    print(f\"GPT2:\\n{gpt2_next}\\n\")\n",
    "    gpt2_messages.append(gpt2_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
