{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# 2週目へようこそ！\n",
    "\n",
    "## フロンティアモデルAPI\n",
    "\n",
    "1週目には、チャットUIを介して複数のフロンティアLLMを使用し、OpenaiのAPIに接続しました。\n",
    "\n",
    "今日は、APIに接続して、人類とGoogle、およびOpenaiを接続します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../important.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：#900;\">重要なメモ - 私を読んでください</h2>\n",
    "            <SPAN STYLE = \"Color：#900;\">これらのラボを継続的に改善し、より多くの例とエクササイズを追加しています。\n",
    "            毎週の初めに、最新のコードがあることを確認する価値があります。<br/>\n",
    "            最初にa <a href = \"https://chatgpt.com/share/6734E705-3270-8012-A074-421661AF6BA9\">必要に応じて変更をプルしてマージします</a>。何か問題がありますか？ chatgptにマージの方法を明確にするように頼んでみてください - または私に連絡してください！<br/> <br/>\n",
    "            LLM_Engineeringディレクトリからコードをプルした後、Anacondaプロンプト（PC）またはターミナル（MAC）で、実行してください：<br/>\n",
    "            <code> conda env update  -  f environment.yml </code> <br/>\n",
    "            または、AnacondaではなくVirtualenvを使用した場合は、PowerShell（PC）またはターミナル（MAC）でアクティブ化された環境からこれを実行します。<br/>\n",
    "            <code> pip install -r requistence.txt </code>\n",
    "            <br/>次に、カーネル（カーネルメニュー>>カーネルの再起動とすべてのセルのクリア出力）を再起動して、変更を拾います。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../resources.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：＃f71;\">リソースページについてのリマインダー</h2>\n",
    "            <Span style = \"color：＃f71;\">これがコースのリソースへのリンクです。これには、すべてのスライドへのリンクが含まれます。<br/>\n",
    "            <a href = \"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\"> https://edwarddonner.com/2024/11/13/llm-engineering-resources/ </a> <br/>\n",
    "            これをブックマークしておくと、時間の経過とともにもっと便利なリンクを追加し続けます。\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## キーをセットアップします\n",
    "\n",
    "まだ行っていない場合は、OpenAIに加えて、人類とGoogle用のAPIキーを作成できるようになりました。\n",
    "\n",
    "**注意：** 追加のAPIコストを避けたい場合は、AnthopicとGoogleのセットアップをスキップしてください！あなたは私がそれをしているのを見ることができ、コースのためにOpenaiに焦点を合わせます。また、1週目に行った演習を使用して、Ollamaの人類やGoogleを代用することもできます。\n",
    "\n",
    "Openaiについては、https://openai.com/api/ にアクセスしてください  \n",
    "人類については、https://console.anthropic.com/ にアクセスしてください  \n",
    "Googleについては、https://ai.google.dev/gemini-api にアクセスしてください  \n",
    "\n",
    "### deepseekも必要に応じて追加します\n",
    "\n",
    "オプションでは、DeepSeekも使用する場合は、アカウントXX_MARKDOWN_LINK_XXを作成し、キーXX_MARKDOWN_LINK_XXを作成し、少なくとも$ 2 XX_MARKDOWN_LINK_XXでトップアップします。\n",
    "\n",
    "### .ENVファイルにAPIキーを追加します\n",
    "\n",
    "APIキーを取得するときは、 `.env`ファイルに追加することにより、環境変数として設定する必要があります。\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "その後、カーネルメニューを介してJupyter Labカーネル（このノートブックの後ろにあるPythonプロセス）を再起動し、上部からセルを再実行する必要がある場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸入\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Googleのインポート\n",
    "# まれに、これは一部のシステムでエラーを発生させるか、カーネルをクラッシュさせているようです\n",
    "# これがあなたに起こった場合、このセルを単に無視してください - 私は後でgeminiを使用するための代替アプローチを与えます\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .envというファイルに環境変数をロードします\n",
    "# キープレフィックスを印刷して、デバッグに役立ちます\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Openai APIキーが設定されていません\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"人為的APIキーが設定されていません\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google APIキーが設定されていません\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 人類のOpenaiに接続します\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# これはジェミニのセットアップコードです\n",
    "# Google Geminiのセットアップに問題がありますか？次に、このセルを無視してください。 Geminiを使用するとき、このライブラリを完全にバイパスする代替手段を提供します\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## llmsに冗談を言うように頼みます\n",
    "\n",
    "LLMはジョークを語るのに大きな仕事をしていないことがわかります！いくつかのモデルを比較しましょう。\n",
    "後でLLMSをよりよく使用するようにします！\n",
    "\n",
    "### APIに含まれる情報\n",
    "\n",
    "通常、APIに渡します。\n",
    " - 使用するモデルの名前\n",
    " -  LLMが再生している役割の全体的なコンテキストを与えるシステムメッセージ\n",
    " - 実際のプロンプトを提供するユーザーメッセージ\n",
    "\n",
    "通常は0〜1の間の**温度**を含む、使用できる他のパラメーターがあります。より多くのランダム出力の場合;より焦点を絞って決定論的には低くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4O-MINI\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-mini\n",
    "# 温度設定は創造性を制御します\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-NANO-非常に速くて安い\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# これにアクセスできる場合は、ここに推論モデルo4-miniがあります\n",
    "# これは、返信する前にその応答を考えるように訓練されています\n",
    "# したがって、それは時間がかかりますが、答えはもっと推論されるべきです - これが役立つというわけではありません。\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロード4.0ソネット\n",
    "# APIには、ユーザープロンプトとは別に提供されるシステムメッセージが必要です\n",
    "# また、max_tokensを追加します\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.0ソネット\n",
    "# 次に、ストリーミングバックの結果を追加しましょう\n",
    "# ストリーミングが奇妙に見える場合は、このセルの下のメモをご覧ください！\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## いくつかのWindowsボックスでクロードストリーミングに関するまれな問題\n",
    "\n",
    "2人の学生は、クロードがJupyter Labの出力にストリーミングしていることで奇妙なことに気づいたことに気づきました。それは、応答の一部を飲み込むことがあるようです。\n",
    "\n",
    "これを修正するには、コードを置き換えます。\n",
    "\n",
    "`print（text、end =\" \"、flush = true）`\n",
    "\n",
    "これで：\n",
    "\n",
    "`clean_text = text.replace（\" \\ n \"、\" \"）.replace（\" \\ r \"、\" \"）`  \n",
    "`print（clean_text、end =\" \"、flush = true）`\n",
    "\n",
    "そして、それは正常に動作するはずです！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeminiのAPIの構造はわずかに異なります。\n",
    "# 一部のPCでは、このジェミニコードがカーネルをクラッシュさせると聞いています。\n",
    "# それがあなたに起こった場合は、このセルをスキップして、代わりに次のセルを使用してください - 代替アプローチ。\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogleのPython APIライブラリをバイパスするGeminiを使用する代替方法として、\n",
    "# Googleはエンドポイントをリリースしました。つまり、OpenAIのクライアントライブラリを介してGeminiを使用できます。\n",
    "# また、ジェミニの最新の推論/思考モデルも試しています\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# サイドノート：\n",
    "\n",
    "OpenAIのクライアントライブラリを使用して他のモデルと接続するこの代替アプローチは、ここ数ヶ月で非常に人気があります。\n",
    "\n",
    "そのため、すべてのモデルが人類を含むこのアプローチをサポートしています。\n",
    "\n",
    "このガイドの最初のセクションでは、このアプローチの詳細については、4つの例をご覧ください。\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## （オプション）DeepSeekモデルを試してみます\n",
    "\n",
    "### deepseekに本当に難しい質問をしましょう - チャットモデルと推論モデルの両方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# オプションで、Deekseekを試してみたい場合は、OpenAIクライアントライブラリを使用することもできます\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek APIキーが設定されていない -  DeepSeek APIを試したくない場合は、次のセクションにスキップしてください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeekチャットを使用します\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseekチャットを使用して、難しい質問をしてください！およびストリーミング結果\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"単語の数：\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepseekの推論を使用 -  Deepseekがビジーである場合、これはエラーにヒットする可能性があります\n",
    "# それは（28-Jan-2025の時点で）過剰に登録されていますが、すぐにオンラインに戻ってくるはずです！\n",
    "# これが失敗した場合は、数日でこれに戻ってください。\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"単語の数：\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "##モデルでの経験を構築するための追加の演習\n",
    "\n",
    "これはオプションですが、時間がある場合は、これらの異なるモデルの機能を直接体験することはとても素晴らしいことです。\n",
    "\n",
    "上記のAPIを介して戻って同じ質問をすることができ、モデルの長所と短所であなた自身の個人的な経験を得ることができます。\n",
    "\n",
    "コースの後半では、ベンチマークを見て、多くの次元でLLMを比較します。しかし、個人的な経験に勝るものはありません！\n",
    "\n",
    "これがいくつかの質問です：\n",
    "1。上記の質問：「このプロンプトに対するあなたの答えにはいくつの言葉がありますか」\n",
    "2。創造的な質問：「3つの文章で、見ることができなかった人に青い色を説明してください」\n",
    "3。学生（ローマンありがとう）は私にこの素晴らしい謎を送ってくれました。明らかに子供たちは通常答えることができますが、大人は次のように苦労しています。 gnaw sul？ \"。\n",
    "\n",
    "答えはあなたが期待するものではないかもしれませんし、私はパズルが非常に得意であっても、私はこれを間違えたことを認めることに恥ずかしいです。\n",
    "\n",
    "###モデルを試すときに何を見るべきか\n",
    "\n",
    "1.チャットモデルが推論モデルとどのように異なるか（思考モデルとも呼ばれます）\n",
    "2。問題を解決する能力と創造的になる能力\n",
    "3。世代の速度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## 深刻な質問でOpenaiに戻ります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真面目に！ GPT-4O-MINI元の質問\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# マークダウンで結果をストリーミングバックしてもらいます\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## そして今、いくつかの楽しみのために - チャットボット間の敵対的な会話。\n",
    "\n",
    "あなたはすでに次のようなリストに整理されているプロンプトがあります。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "実際、この構造は、より長い会話履歴を反映するために使用できます。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "そして、このアプローチを使用して、歴史とのより長い相互作用に従事することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4.1-miniとClaude-3.5-Haikuの間で会話をしましょう\n",
    "# モデルの安価なバージョンを使用しているので、コストは最小限に抑えられます\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<テーブルスタイル= \"マージン：0;テキストアライグ：左;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../ fality.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：＃900;\">続行する前</h2>\n",
    "            <Span style = \"color：＃900;\">\n",
    "                上記の会話がどのように機能しているか、特に<code>メッセージ</code>リストがどのように入力されているかを確認してください。必要に応じて印刷ステートメントを追加します。次に、大きなバリエーションについては、システムプロンプトを使用してパーソナリティを切り替えてみてください。おそらく、1つは悲観的であり、1つは楽観的ですか？<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# より高度なエクササイズ\n",
    "\n",
    "3ウェイを作成して、おそらくジェミニを会話に持ち込んでみてください！ 1人の生徒がこれを完了しました - コミュニティコントリビューションフォルダーの実装を参照してください。\n",
    "\n",
    "これを行う最も信頼できる方法は、プロンプトについて少し違った考え方をすることです。毎回1つのシステムプロンプトと1つのユーザープロンプト、ユーザープロンプトでは、これまでの会話全体をリストします。\n",
    "\n",
    "次のようなもの：\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "ソリューションを見る前に、これを自分でやってみてください。 Openai Pythonクライアントを使用してGeminiモデルにアクセスするのが最も簡単です（上記の2番目のGeminiの例を参照）。\n",
    "\n",
    "##追加演習\n",
    "\n",
    "モデルの1つをOllamaで実行しているオープンソースモデルに置き換えることもできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<テーブルスタイル= \"マージン：0;テキストアライグ：左;\">\n",
    "    <tr>\n",
    "        <td style = \"width：150px; height：150px; vertical-align：middle;\">\n",
    "            <img src = \"../ business.jpg\" width = \"150\" height = \"150\" style = \"display：block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style = \"color：＃181;\">ビジネス関連</h2>\n",
    "            <Span style = \"color：＃181;\">メッセージのリストとしての会話のこの構造は、会話型AIアシスタントを構築する方法と、会話中にコンテキストを維持する方法の基本です。これを次のいくつかのラボに適用してAIアシスタントを構築します。その後、これを自分のビジネスに拡張します。</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
