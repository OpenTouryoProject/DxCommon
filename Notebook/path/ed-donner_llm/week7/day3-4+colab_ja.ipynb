{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "# 製品価格を予測します\n",
        "\n",
        "## 3日目：トレーニング！\n",
        "\n",
        "＃重要です私を読んでください!!\n",
        "\n",
        "以下のPIPインストールを実行すると、FSSPECの互換性のないバージョンについて不平を言うPIPからエラーが発生する可能性があります。\n",
        "\n",
        "そのエラーを無視する必要があります！ FSSPECのバージョンは、Huggingfaceが必要とする適切なバージョンです。\n",
        "\n",
        "ChatGptに尋ねると、FSSPECのより最近のバージョンをインストールすることをお勧めします。しかし、それは問題があるでしょう。 Huggingfaceは、ファイルシステムに関するあいまいなエラーでデータセットを後でロードできません。\n",
        "\n",
        "したがって、PIPインストールは以下に表示されているように実行してください。エラーが発生した場合は、逆の方を見てください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# ピップインストール\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# 輸入\n",
        "# イスラム教S.に感謝します。\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# 定数\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "HF_USER = \"ed-donner\" # ここにHFの名前！\n",
        "\n",
        "# データ\n",
        "\n",
        "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
        "# または、アップロードしたものを使用してください\n",
        "# dataset_name = \"ed-donner/pricer-data\"\n",
        "MAX_SEQUENCE_LENGTH = 182\n",
        "\n",
        "# ハブでモデルを保存するために名前を実行します\n",
        "\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# クロラのハイパーパラメーター\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# トレーニング用のハイパーパラメーター\n",
        "\n",
        "EPOCHS = 1 # 必要に応じてより多くのエポックを行うことができますが、1つだけが必要です - おそらくそれ以上はやり過ぎです\n",
        "BATCH_SIZE = 4 # A100ボックスでこれは16までになります\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "# admin config- save_stepsはハブにアップロードする頻度であることに注意してください\n",
        "# 私はこれを5000から2000に変更したので、より頻繁にセーブするようになりました\n",
        "\n",
        "STEPS = 50\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HUB_MODEL_NAME"
      ],
      "metadata": {
        "id": "QyHOj-c4FmkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "＃オプティマイザーの詳細\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizer-choice\n",
        "\n",
        "最も一般的なのは、アダムまたはアダム（体重減衰のあるアダム）です。  \n",
        "アダムは、以前の勾配のローリング平均を保存することにより、良好な収束を達成します。ただし、モデルパラメーターの数の順序の追加メモリフットプリントを追加します。\n"
      ],
      "metadata": {
        "id": "PfvyitbgCMMQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### ログインして、フェイスと重みとバイアスを抱きしめます\n",
        "\n",
        "まだHuggingfaceアカウントをお持ちでない場合は、https：//huggingface.coにアクセスしてサインアップしてトークンを作成します。\n",
        "\n",
        "次に、左のキーアイコンをクリックして、このノートブックの秘密を選択し、トークンとして値を持つ「hf_token」と呼ばれる新しい秘密を追加します。\n",
        "\n",
        "https://wandb.aiでweightsandbiaseについてこれを繰り返し、 `wandb_api_key`と呼ばれる秘密を追加します"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Huggingfaceにログインします\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNOv3cVvJ68"
      },
      "outputs": [],
      "source": [
        "# ウェイトとバイアスにログインします\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# プロジェクトに対して記録するように重みとバイアスを構成します\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvXVoJH8LS6u"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 代わりにトレーニングデータセットを20,000ポイントに削減したい場合は、この行を解除します。\n",
        "# Train = Train.Select（Range（20000））"
      ],
      "metadata": {
        "id": "rJb9IDVjOAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SUsKqA23Gc"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## 次に、トークン剤とモデルをロードします\n",
        "\n",
        "モデルは「量子化」されています - 精度を4ビットに減らしています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lb7M9xn46wx"
      },
      "outputs": [],
      "source": [
        "# 適切な量子化を選択します\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# トークン剤とモデルをロードします\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データコラーター\n",
        "\n",
        "トレーニング中に、製品の説明を予測するためにモデルを訓練しようとしていないことを保証することが重要です。彼らの価格だけ。\n",
        "\n",
        "「価格は$」までのすべてがモデルにコンテキストを提供して次のトークンを予測するためにあるが、学習する必要はないことをトレーナーに伝える必要があります。\n",
        "\n",
        "トレーナーは、「価格は$」の後にトークンを予測するようにモデルに教える必要があります。\n",
        "\n",
        "マスクを設定することでこれを行うための複雑な方法がありますが、幸運なことに、Huggingfaceは私たちのためにこれを大事にするための非常にシンプルなヘルパークラスを提供します。"
      ],
      "metadata": {
        "id": "9BYO0If4uWys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2omVEaPIVJZa"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "response_template = \"Price is $\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# そして今\n",
        "\n",
        "##トレーニング用の構成を設定します\n",
        "\n",
        "2つのオブジェクトを作成する必要があります。\n",
        "\n",
        "Loraのハイパーパラメーターを備えたLoraconfigオブジェクト\n",
        "\n",
        "全体的なトレーニングパラメーターを備えたSFTCONFIG"
      ],
      "metadata": {
        "id": "4DaOeBhyy9eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCwmDmkSATvj"
      },
      "outputs": [],
      "source": [
        "# まず、LORAの構成パラメーターを指定します\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "# 次に、トレーニング用の一般的な構成パラメーターを指定します\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        ")\n",
        "\n",
        "# そして今、監督された微調整トレーナーが微調整を実行します\n",
        "# これらの2セットの構成パラメーターが与えられます\n",
        "# TRLの最新バージョンは、ラベルに関する警告を示しています - この警告を無視してください\n",
        "# しかし、良いトレーニング結果が見られない場合はお知らせください（損失が下がっています）。\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    data_collator=collator\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 次のセルでは、微調整を開始します！\n",
        "\n",
        "これはしばらくの間実行され、すべてのSave_Stepsステップをハブにアップロードします。\n",
        "\n",
        "しばらくすると、Googleはあなたのcolabを止めるかもしれません。無料プランを使用している人にとって、Googleがリソースが少ないときはいつでも発生する可能性があります。有料プランの場合は誰でも、最大24時間を与えることができますが、保証はありません。\n",
        "\n",
        "サーバーが停止した場合は、ここで私のコラブをたどって最後の保存から再開できます。\n",
        "\n",
        "https://colab.research.google.com/drive/1qgtdvias_vwoby4uvi2vwsu0thxy8omo#scrollto = r_o04fkxmmt-\n",
        "\n",
        "このコラブを出力での最終実行で保存したので、例を見ることができます。トリックは、fine_tunedモデルをロードするときに「is_trainable」を設定する必要があることです。\n",
        "\n",
        "とにかく###それを念頭に置いて、これをキックオフしましょう！"
      ],
      "metadata": {
        "id": "ArjP7_OCQOin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 微調整！\n",
        "fine_tuning.train()\n",
        "\n",
        "# 微調整されたモデルを顔に抱きしめます\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "GfvAxnXPvB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32vvrYRVAUNg"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [
        {
          "file_id": "1csEdaECRtjV_1p9zMkaKKjCpYnltlN3M",
          "timestamp": 1754464101229
        },
        {
          "file_id": "1bG7RiOeH5QPq3JQeKT9pKizLvNR-AGl9",
          "timestamp": 1726957579634
        },
        {
          "file_id": "15LkGT3KiWNVYeixJyVxmKCEp-4i0ay3t",
          "timestamp": 1726105199072
        },
        {
          "file_id": "1yuwPchdf-4XC6U19qlyka5VxvdoqbN4F",
          "timestamp": 1726003619500
        },
        {
          "file_id": "1iWviog0DBRNH0eQ8ttPcKAQAnda_XsHe",
          "timestamp": 1725972744690
        },
        {
          "file_id": "1QFH8bL2g2xspETWirmUcThxw801uiRiA",
          "timestamp": 1725923171274
        },
        {
          "file_id": "12vriYAjNGP9erT-dLpJCpv6IFG8vInpX",
          "timestamp": 1725916862962
        },
        {
          "file_id": "1RDjLFskzcODWkdJzO92-8BnXXUDg4IxJ",
          "timestamp": 1725815008044
        },
        {
          "file_id": "1TA_GwdrpWwRZfUw8I9y2fqqwFv9CBU1O",
          "timestamp": 1725200339101
        },
        {
          "file_id": "18UtZAMfXHv2NwlcseEpvqqd0SCHBUeQY",
          "timestamp": 1724340104889
        },
        {
          "file_id": "1BuEhC59gEyqFTD6F9Th2vIcTygFNuz8F",
          "timestamp": 1724060220581
        },
        {
          "file_id": "19E9hoAzWKvn9c9SHqM4Xan_Ph4wNewHS",
          "timestamp": 1724000085127
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}