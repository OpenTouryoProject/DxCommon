{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHsssBgWM_l0"
   },
   "source": [
    "## 製品価格予測\n",
    "\n",
    "（講師は A100 を使用しているが T4 でも実行可能）\n",
    "\n",
    "## 第7週 三日目：トレーニング！\n",
    "\n",
    "# 重要なメモ これを読んでください!\n",
    "\n",
    "以下の pip インストールを実行すると、FSSPECの互換性のないバージョンについて不平を言うPIPからエラーが発生する可能性があります。そのエラーを無視する必要があります！ FSSPECのバージョンは、Hugging Faceが必要とする適切なバージョンです。\n",
    "\n",
    "ChatGptに尋ねると、FSSPECのより最近のバージョンをインストールすることを推奨してきますが、しかし、それは問題があり、Hugging Faceは、ファイルシステムに関するあいまいなエラーでデータセットを後でロードできないので、pip インストールは以下に表示されているように実行してください。エラーが発生した場合は、逆の方を見てください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDyR63OTNUJ6"
   },
   "outputs": [],
   "source": [
    "# pip install\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yikV8pRBer9"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "# Islam S.に感謝します。\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuTX-xonNeOK"
   },
   "outputs": [],
   "source": [
    "# 定数\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "PROJECT_NAME = \"lite-pricer\" # \"pricer\"\n",
    "HF_USER = \"nishi74322014\"\n",
    "\n",
    "# データ\n",
    "DATASET_NAME = f\"{HF_USER}/lite-data\"\n",
    "#DATASET_NAME = \"ed-donner/pricer-data\" # 後で件数を少なくできる。\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# HF Hubにモデルを保存するために名前\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# QLoRAのハイパーパラメタ\n",
    "LORA_R = 32        # r:Rank\n",
    "LORA_ALPHA = 64    # W′ = W + α⋅BA の α\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "LORA_DROPOUT = 0.1 # α⋅BA の部分に適用される。\n",
    "QUANT_4_BIT = True # LORA → QLORA\n",
    "\n",
    "# トレーニング用のハイパーパラメタ\n",
    "\n",
    "# 必要に応じてより多くのエポックを行うことができる。\n",
    "# が、1-2で十分、3以上で過学習になる（ログを見て判断）。\n",
    "EPOCHS = 1\n",
    "\n",
    "# A100ボックスで16まで設定可能\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# GPUメモリが足りないときに「見かけ上のバッチサイズを大きくする」（→ 精度が安定）\n",
    "# 「見かけ上のバッチサイズ」= BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "\n",
    "# 学習率\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 学習率のスケジュール・タイプ\n",
    "LR_SCHEDULER_TYPE = 'cosine' # cosineカーブのように徐々に減少\n",
    "\n",
    "# 学習率ウォームアップ\n",
    "WARMUP_RATIO = 0.03 # ステップ数の 3% をウォームアップに使う\n",
    "\n",
    "# 最適化アルゴリズム [paged_adamw_8bit, paged_adamw_32bit, adafactor]\n",
    "# paged_adamw_32bit：高精度でA100など、paged_adamw_8bit：精度は下がるT4など、adafactor：メモリ使用量は最少、安定性は劣る\n",
    "# LoRAやQLoRAなどの極端な軽量化が必要な時の最終手段。\n",
    "OPTIMIZER = \"paged_adamw_8bit\" # \"paged_adamw_32bit\"\n",
    "\n",
    "# 管理者設定 - SAVE_STEPS はハブへのアップロード頻度です。\n",
    "# より頻繁に保存できるよう、5000 から 2000 に変更しました。\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 2000\n",
    "LOG_TO_WANDB = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyHOj-c4FmkM"
   },
   "outputs": [],
   "source": [
    "# HF Hubにアップロードするモデル名\n",
    "HUB_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfvyitbgCMMQ"
   },
   "source": [
    "# オプティマイザーの詳細\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizers\n",
    "\n",
    "最も一般的なのは、AdamまたはAdamW（重減衰のあるAdam）です。\n",
    "\n",
    "Adamは、以前の勾配のローリング平均を保存することにより、良好な収束を達成します。ただし、モデルパラメタの数の順序の追加メモリフットプリントを追加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JArT3QAQAjx"
   },
   "source": [
    "### Hugging Face と Weights & Biases にログインします\n",
    "\n",
    "まだHugging Faceアカウントをお持ちでない場合は、https://huggingface.co にアクセスしてサインアップしてトークンを作成します。\n",
    "\n",
    "次に、左のキーアイコンをクリックして、このノートブックのシークレットを選択し、トークンとして値を持つ `HF_TOKEN` と呼ばれる新しい秘密を追加します。\n",
    "\n",
    "https://wandb.ai で Weights & Biases についてこれを繰り返し、`WANDB_API_KEY` と呼ばれる秘密を追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyFPZeMcM88v"
   },
   "outputs": [],
   "source": [
    "# Hugging Face Hub にプログラムからログイン\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJNOv3cVvJ68"
   },
   "outputs": [],
   "source": [
    "# Weights & Biasesにログイン\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# プロジェクトに対して記録するように重みとバイアスを構成\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBBvcJ679WjE"
   },
   "source": [
    "# データをロード\n",
    "\n",
    "Hugging Faceにアップロードしたので、取得するのは簡単"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvXVoJH8LS6u"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJb9IDVjOAn9"
   },
   "outputs": [],
   "source": [
    "# トレーニングデータセットのポイント\n",
    "print(len(train))\n",
    "\n",
    "# nishi74322014/lite-data を使用するので削減不要\n",
    "# トレーニングデータセットを20,000ポイントに削減\n",
    "#train = train.select(range(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_SUsKqA23Gc"
   },
   "outputs": [],
   "source": [
    "# wandb.init\n",
    "if LOG_TO_WANDB:\n",
    "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJWQ0a3wZ0Bw"
   },
   "source": [
    "## 次に、トークナイザとモデルをロード\n",
    "\n",
    "モデルを「量子化」し、精度を4ビットに減らす。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lb7M9xn46wx"
   },
   "outputs": [],
   "source": [
    "# 適切な量子化を選択\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_O04fKxMMT-"
   },
   "outputs": [],
   "source": [
    "# トークナイザとモデルをロード\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# （バッチ化などで）長さを揃えるために 「パディングトークンID」 が必要\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BYO0If4uWys"
   },
   "source": [
    "# DataCollator\n",
    "\n",
    "は、データの前処理（バッチ化）を行うクラスで、このトレーニングでは、\n",
    "\n",
    "モデルが商品の説明含め予測するのではなく、価格のみを予測するようにトレーニングすることが重要になる。\n",
    "\n",
    "「Price is $」までの部分は、モデルが次のトークンを予測するためのコンテキストを提供するためのもので、\n",
    "\n",
    "- トレーナーに「`Price is $`までの部分を学習する必要はない」ことを伝える必要がある。\n",
    "- また、トレーナーは、モデルに「`Price is $`の後のトークンを予測するよう」に教える必要がある。\n",
    "\n",
    "マスクを設定することでこれを行うための複雑な方法があるが、幸運なことに、\n",
    "\n",
    "Hugging Faceは私たちのためにこれを大事にするための非常にシンプルなヘルパークラスを提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2omVEaPIVJZa"
   },
   "outputs": [],
   "source": [
    "# 「Price is $」の後ろに続くテキストだけを学習対象にするように指示\n",
    "\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "response_template = \"Price is $\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DaOeBhyy9eS"
   },
   "source": [
    "# そして今\n",
    "\n",
    "## トレーニング用の構成を設定\n",
    "\n",
    "2つのオブジェクトを作成\n",
    "\n",
    "- LoRAのハイパーパラメタを備えた LoraConfig\n",
    "- 全体的なトレーニングパラメタを備えた SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCwmDmkSATvj"
   },
   "outputs": [],
   "source": [
    "# まず、LoraConfigを設定\n",
    "\n",
    "lora_parameters = LoraConfig(\n",
    "    # LoRAスケーリング係数α\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    # LoRA適用部分のドロップアウト率。\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    # LoRA（の低ランク行列）のランク（次元）。\n",
    "    r=LORA_R,\n",
    "    # LoRAでバイアス項をどのように扱うか。\n",
    "    bias=\"none\",\n",
    "    # タスク種類：因果言語モデル（次単語予測）\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # LoRAを適用するモジュール\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# 次に、SFTConfigを設定\n",
    "\n",
    "train_parameters = SFTConfig(\n",
    "    # トレーニング成果物保存ディレクトリ名。\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    # 学習エポック数\n",
    "    num_train_epochs=EPOCHS,\n",
    "    # 学習時のバッチサイズ（デバイスごと）\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    # 評価時のバッチサイズ（デバイスごと）。\n",
    "    per_device_eval_batch_size=1,\n",
    "    # 評価を行わない設定。通常 \"steps\" や \"epoch\" を指定可能。\n",
    "    eval_strategy=\"no\",\n",
    "    # 見かけ上のバッチサイズを大きくする\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    # 使用する最適化アルゴリズム\n",
    "    optim=OPTIMIZER,\n",
    "    # モデルを何ステップごとに保存するか？\n",
    "    save_steps=SAVE_STEPS,\n",
    "    # 保存するチェックポイントの最大数\n",
    "    save_total_limit=10,\n",
    "    # ログを出力するステップ間隔\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    # 学習率\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    # 重み減衰（L2正則化）\n",
    "    weight_decay=0.001,\n",
    "    # 16ビット浮動小数点（半精度）演算を使わない。\n",
    "    fp16=False,\n",
    "    # bfloat16での学習を有効化（T4は非対応）\n",
    "    bf16=False, # True,\n",
    "    # 勾配クリッピングの最大値（勾配爆発の防止）\n",
    "    max_grad_norm=0.3,\n",
    "    # 総ステップ数をエポック数で制御\n",
    "    max_steps=-1,\n",
    "    # ウォームアップステップの割合\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    # バッチを同じ長さの入力にまとめ効率化\n",
    "    group_by_length=True,\n",
    "    # 学習率スケジューラーのタイプ（説明済み\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    # Weights & Biases（W&B）へのログ送信の有効/無効\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    # W&B上などで使われるRUN_NAME\n",
    "    run_name=RUN_NAME,\n",
    "    # モデルが処理する最大トークン長\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    # 入力データのテキストが格納されているフィールド名\n",
    "    dataset_text_field=\"text\",\n",
    "    # 保存タイミングの基準をステップ数に\n",
    "    save_strategy=\"steps\",\n",
    "    # モデルを保存するたびにHubアップロード\n",
    "    hub_strategy=\"every_save\",\n",
    "    # 学習済みモデルをHugging Face Hubにアップロード\n",
    "    push_to_hub=True,\n",
    "    # Hugging Face Hubにアップロードする際のモデル名\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    # H非公開リポジトリとしてアップロード\n",
    "    hub_private_repo=True\n",
    ")\n",
    "\n",
    "# そして今、監督されたファインチューニング・トレーナーがファインチューニングを実行します。\n",
    "# これらの2セットの構成パラメタ（LoraConfig、SFTConfig）を与える。\n",
    "# TRLの最新バージョンは、ラベルに関する警告を示している - この警告を無視してください\n",
    "# 良いトレーニング結果が見られない場合はお知らせください（損失が下がっています）。\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train,\n",
    "    peft_config=lora_parameters,\n",
    "    args=train_parameters,\n",
    "    data_collator=collator\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArjP7_OCQOin"
   },
   "source": [
    "## 次のセルでは、ファインチューニングを開始します！\n",
    "\n",
    "これはしばらくの間実行され、Save_Stepsステップで HF HUB にアップロードします。\n",
    "\n",
    "しばらくすると、GoogleはあなたのColabを止めるかもしれません。\n",
    "\n",
    "無料プランを使用している人にとって、Googleがリソースが少ないときはいつでも発生する可能性があります。\n",
    "\n",
    "有料プランの場合は誰でも、最大24時間を与えることができますが、保証はありません。\n",
    "\n",
    "サーバーが停止した場合は、ここで私のColabをたどって最後の保存から再開できる。\n",
    "\n",
    "https://colab.research.google.com/drive/1qGTDVIas_Vwoby4UVi2vwsU0tHXy8OMO#scrollTo=R_O04fKxMMT-\n",
    "\n",
    "このColabを出力での最終実行で保存したので、例を見ることができます。\n",
    "\n",
    "トリックは、fine_tunedモデルをロードするときに「is_trainable」を設定する必要があることです。\n",
    "\n",
    "### とにかく、それを念頭に置いて、これをキックオフしましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfvAxnXPvB7w"
   },
   "outputs": [],
   "source": [
    "# ファインチューニング！\n",
    "fine_tuning.train()\n",
    "\n",
    "# 結果をHF HUBにプッシュ\n",
    "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")\n",
    "\n",
    "# W&Bに終了を通知\n",
    "if LOG_TO_WANDB:\n",
    "  wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1csEdaECRtjV_1p9zMkaKKjCpYnltlN3M",
     "timestamp": 1754464101229
    },
    {
     "file_id": "1bG7RiOeH5QPq3JQeKT9pKizLvNR-AGl9",
     "timestamp": 1726957579634
    },
    {
     "file_id": "15LkGT3KiWNVYeixJyVxmKCEp-4i0ay3t",
     "timestamp": 1726105199072
    },
    {
     "file_id": "1yuwPchdf-4XC6U19qlyka5VxvdoqbN4F",
     "timestamp": 1726003619500
    },
    {
     "file_id": "1iWviog0DBRNH0eQ8ttPcKAQAnda_XsHe",
     "timestamp": 1725972744690
    },
    {
     "file_id": "1QFH8bL2g2xspETWirmUcThxw801uiRiA",
     "timestamp": 1725923171274
    },
    {
     "file_id": "12vriYAjNGP9erT-dLpJCpv6IFG8vInpX",
     "timestamp": 1725916862962
    },
    {
     "file_id": "1RDjLFskzcODWkdJzO92-8BnXXUDg4IxJ",
     "timestamp": 1725815008044
    },
    {
     "file_id": "1TA_GwdrpWwRZfUw8I9y2fqqwFv9CBU1O",
     "timestamp": 1725200339101
    },
    {
     "file_id": "18UtZAMfXHv2NwlcseEpvqqd0SCHBUeQY",
     "timestamp": 1724340104889
    },
    {
     "file_id": "1BuEhC59gEyqFTD6F9Th2vIcTygFNuz8F",
     "timestamp": 1724060220581
    },
    {
     "file_id": "19E9hoAzWKvn9c9SHqM4Xan_Ph4wNewHS",
     "timestamp": 1724000085127
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
