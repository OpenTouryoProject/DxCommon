{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHsssBgWM_l0"
   },
   "source": [
    "## 製品価格予測\n",
    "\n",
    "（T4 で実行可能）\n",
    "\n",
    "### 第7週 一日目\n",
    "\n",
    "LoRA と QLoRAの紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d14UgrL83gcg"
   },
   "source": [
    "## リマインダー：Colabを使用するための2つの重要なプロチップ：\n",
    "\n",
    "**Pro-tip 1:**\n",
    "\n",
    "すべてのColabの上部には、いくつかのpipインストールがあります。これを実行すると、pipからエラーを受信する場合があります。\n",
    "\n",
    "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
    "\n",
    "これらのpip互換性エラーは安全に無視できます。そして、バージョン番号を変更することでそれらを修正しようとするのは魅力的ですが、実際に実際の問題を導入します！\n",
    "\n",
    "**Pro-tip 2:**\n",
    "\n",
    "Colabを実行している最中に、次のようなエラーが発生する可能性があります。\n",
    "\n",
    "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
    "\n",
    "これは非常に誤解を招くエラーメッセージです。パッケージのバージョンを変更しないでください。\n",
    "\n",
    "これは、GoogleがColabランタイムを切り替えたために実際に起こります。おそらくGoogle Colabが忙しすぎたためです。解決策は次のとおりです。\n",
    "\n",
    "1. カーネルメニュー >> ランタイムを切断して削除\n",
    "2. 新鮮なメニューからColabをリロードし、メニュー >> すべての出力をクリア\n",
    "3. 右上のボタンを使用して新しいT4に接続\n",
    "4. 右上のメニューから「リソースを表示」を選択して、GPUがあることを確認\n",
    "5. Colab内のセルを上から下へ、pipのインストールから始めて再実行\n",
    "\n",
    "そして、すべてがうまくいくはずです - そうでなければ、私に尋ねてください！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDyR63OTNUJ6"
   },
   "outputs": [],
   "source": [
    "# pip install\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
    "!pip install -q datasets requests peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yikV8pRBer9"
   },
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
    "from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuTX-xonNeOK"
   },
   "outputs": [],
   "source": [
    "# 定数\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
    "\n",
    "# QLoRAファインチューニングのためのハイパーパラメタ\n",
    "\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JArT3QAQAjx"
   },
   "source": [
    "### Hugging Faceにログインします\n",
    "\n",
    "まだHugging Faceアカウントをお持ちでない場合は、https://huggingface.co にアクセスしてサインアップしてトークンを作成します。\n",
    "\n",
    "次に、左のキーアイコンをクリックして、このノートブックのシークレットを選択し、トークンとして値を持つ「HF_TOKEN」と呼ばれる新しい秘密を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyFPZeMcM88v"
   },
   "outputs": [],
   "source": [
    "# Hugging Faceにログイン\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDJXqy8171W7"
   },
   "outputs": [],
   "source": [
    "base_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJWQ0a3wZ0Bw"
   },
   "source": [
    "## さまざまな量子化を試します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHqev-NQ-QIw"
   },
   "outputs": [],
   "source": [
    "# 量子化なしでトークナイザとベースモデルをロード\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVFn-3xmDoY8"
   },
   "source": [
    "上記を実行して得られる出力\n",
    "```\n",
    "config.json: 100% ■■■ 826/826 [00:00<00:00, 77.7kB/s]\n",
    "model.safetensors.index.json: 100% ■■■ 23.9k/23.9k [00:00<00:00, 882kB/s]\n",
    "Downloading shards: 100% ■■■  4/4 [20:02<00:00, 265.40s/it]\n",
    "model-00001-of-00004.safetensors: 100% ■■■ 4.98G/4.98G [04:39<00:00, 20.0MB/s]\n",
    "model-00002-of-00004.safetensors: 100% ■■■ 5.00G/5.00G [06:55<00:00, 29.9MB/s]\n",
    "model-00003-of-00004.safetensors: 100% ■■■ 4.92G/4.92G [07:28<00:00, 21.4MB/s]\n",
    "model-00004-of-00004.safetensors: 100% ■■■ 1.17G/1.17G [00:58<00:00, 108MB/s]\n",
    "Loading checkpoint shards: 100% ■■■ 4/4 [00:55<00:00, 27.63s/it]\n",
    "generation_config.json: 100% ■■■ 185/185 [00:00<00:00, 5.54kB/s]\n",
    "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6gJWw3r86KQ"
   },
   "outputs": [],
   "source": [
    "# T4 の GPU RAM は 15GB 、当該モデルが 32.1 GB を占拠 ... T4 ではできないので、\n",
    "# 一部（32.1 - 11.0 = 21.0 GB）のモデル重みを一時的にディスクやCPUに退避させている。\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVf8hf6S88C9"
   },
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dV4-N25Ea3i"
   },
   "source": [
    "上記を実行して得られる出力\n",
    "```\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(128256, 4096)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaAttention(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
    "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
    "          (act_fn): SiLU()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "    (rotary_emb): LlamaRotaryEmbedding()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv-00uzNFPOe"
   },
   "source": [
    "## セッションを再開してください！\n",
    "\n",
    "次のモデルをロードする前に、最後のモデルのキャッシュをクリアするには、ランタイム>>セッションを再起動し、初期セル（インストールとインポート、およびHugging Face Hubログイン）を再度実行する必要があります。\n",
    "\n",
    "これはGPUをクリーンアウトするためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLrvewHg-W0J"
   },
   "outputs": [],
   "source": [
    "# 8ビットを使用してトークナイザとベースモデルをロード\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVmi46TZF7f-"
   },
   "source": [
    "上記を実行して得られる出力（キャッシュを使いダウンロードはしない）\n",
    "```\n",
    "Loading checkpoint shards: 100% ■■■ 4/4 [01:32<00:00, 19.80s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FDztdnv0RCq"
   },
   "outputs": [],
   "source": [
    "# T4 の GPU RAM は 15GB 、当該モデルが 9.1 GB を占拠\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvqOxYfk0RnY"
   },
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTquuoD1He0o"
   },
   "source": [
    "上記を実行して得られる出力\n",
    "```\n",
    "WinMerge で確認すると Linear → Linear8bitLt に変わっていることが確認できる。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb7-0OfSEycl"
   },
   "source": [
    "## セッションを再開してください！\n",
    "\n",
    "次のモデルをロードする前に、最後のモデルのキャッシュをクリアするには、ランタイム>>セッションを再起動し、初期セル（インストールとインポート、およびHugging Face Hubログイン）を再度実行する必要があります。\n",
    "\n",
    "これはGPUをクリーンアウトするためです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iex2Ds7g71W-"
   },
   "outputs": [],
   "source": [
    "# 4ビットを使用してトークナイザとベースモデルをロード\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiBG2jvkIEPE"
   },
   "source": [
    "上記を実行して得られる出力（キャッシュを使いダウンロードはしない）\n",
    "```\n",
    "Loading checkpoint shards: 100% ■■■ 4/4 [01:40<00:00, 22.34s/it]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FfMJ2JbzEr3"
   },
   "outputs": [],
   "source": [
    "# T4 の GPU RAM は 15GB 、当該モデルが 5.59 GB を占拠\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjp1EHH10WTb"
   },
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rl9jrHdIFp7"
   },
   "source": [
    "上記を実行して得られる出力\n",
    "```\n",
    "WinMerge で確認すると Linear → Linear8bitLt → Linear4bit 変わっていることが確認できる。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t45N4iM3K9cC"
   },
   "source": [
    "※ ココでは、セッションを再開しない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSpavkXo1KOI"
   },
   "outputs": [],
   "source": [
    "# ファインチューニング後のトークナイザとベースモデルをロード\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8wtrLJ6LPUZ"
   },
   "source": [
    "上記を実行して得られる出力（キャッシュを使いダウンロードはしない）\n",
    "```\n",
    "adapter_config.json: 100% ■■■ 681/681 [00:00<00:00, 39.0kB/s]\n",
    "adapter_model.safetensors: 100% ■■■ 109M/109M [00:00<00:00, 221MB/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3kXoy3w1oMF"
   },
   "outputs": [],
   "source": [
    "# T4 の GPU RAM は 15GB 、当該モデルが 5.70 GB を占拠\n",
    "# 従って,QLoRAのパラメタは、5.70 - 5.59 = 0.11GB = 110MB と思われる。\n",
    "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2PL1nlM1tM1"
   },
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHa_qUYtMxaJ"
   },
   "source": [
    "上記を実行して得られる出力、lora_q、lora_k、lora_v、lora_o に QLoRAが適用されている。\n",
    "```\n",
    "PeftModelForCausalLM(\n",
    "  (base_model): LoraModel(\n",
    "    (model): LlamaForCausalLM(\n",
    "      (model): LlamaModel(\n",
    "        (embed_tokens): Embedding(128256, 4096)\n",
    "        (layers): ModuleList(\n",
    "          (0-31): 32 x LlamaDecoderLayer(\n",
    "            (self_attn): LlamaAttention(\n",
    "              (q_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (k_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (v_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (o_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "            )\n",
    "            (mlp): LlamaMLP(\n",
    "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
    "              (act_fn): SiLU()\n",
    "            )\n",
    "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "          )\n",
    "        )\n",
    "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
    "        (rotary_emb): LlamaRotaryEmbedding()\n",
    "      )\n",
    "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7h8qWpvYNhEe"
   },
   "source": [
    "以下のセルで計算してみると、凡そ合っている事が解る。\n",
    "```\n",
    "Total number of params: 27,262,976 and size 109.1MB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIaqo-gyBQRh"
   },
   "outputs": [],
   "source": [
    "# 各ターゲットモジュールには、LORA_AとLORA_Bと呼ばれる2つのLORAアダプターマトリックスがあります\n",
    "# これらは、alpha * lora_a * lora_bを追加することでウェイトを適応できるように設計されています\n",
    "\n",
    "# 次のサイズを使用して、重みの数を数えてみる。\n",
    "\n",
    "# 上記のマトリックス寸法を参照してください\n",
    "lora_q_proj = 4096 * 32 + 4096 * 32\n",
    "lora_k_proj = 4096 * 32 + 1024 * 32\n",
    "lora_v_proj = 4096 * 32 + 1024 * 32\n",
    "lora_o_proj = 4096 * 32 + 4096 * 32\n",
    "\n",
    "# 各レイヤーが登場します\n",
    "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
    "\n",
    "# 32のレイヤーがあります\n",
    "params = lora_layer * 32\n",
    "\n",
    "# したがって、MBの合計サイズはです\n",
    "size = (params * 4) / 1_000_000\n",
    "\n",
    "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "15rqdMTJwK76icPBxNoqhI7Ww8UM-Y7ni",
     "timestamp": 1754464190117
    },
    {
     "file_id": "1bG7RiOeH5QPq3JQeKT9pKizLvNR-AGl9",
     "timestamp": 1726925570673
    },
    {
     "file_id": "15LkGT3KiWNVYeixJyVxmKCEp-4i0ay3t",
     "timestamp": 1726105199072
    },
    {
     "file_id": "1yuwPchdf-4XC6U19qlyka5VxvdoqbN4F",
     "timestamp": 1726003619500
    },
    {
     "file_id": "1iWviog0DBRNH0eQ8ttPcKAQAnda_XsHe",
     "timestamp": 1725972744690
    },
    {
     "file_id": "1QFH8bL2g2xspETWirmUcThxw801uiRiA",
     "timestamp": 1725923171274
    },
    {
     "file_id": "12vriYAjNGP9erT-dLpJCpv6IFG8vInpX",
     "timestamp": 1725916862962
    },
    {
     "file_id": "1RDjLFskzcODWkdJzO92-8BnXXUDg4IxJ",
     "timestamp": 1725815008044
    },
    {
     "file_id": "1TA_GwdrpWwRZfUw8I9y2fqqwFv9CBU1O",
     "timestamp": 1725200339101
    },
    {
     "file_id": "18UtZAMfXHv2NwlcseEpvqqd0SCHBUeQY",
     "timestamp": 1724340104889
    },
    {
     "file_id": "1BuEhC59gEyqFTD6F9Th2vIcTygFNuz8F",
     "timestamp": 1724060220581
    },
    {
     "file_id": "19E9hoAzWKvn9c9SHqM4Xan_Ph4wNewHS",
     "timestamp": 1724000085127
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
