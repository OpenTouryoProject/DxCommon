{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "##製品価格を予測します\n",
        "\n",
        "###週7日目\n",
        "\n",
        "LoraとQloraの紹介\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colabを使用するための2つの重要なプロチップのリマインダー：\n",
        "\n",
        "** Pro-Tip 1：**\n",
        "\n",
        "すべてのコラブの上部には、いくつかのPIPインストールがあります。これを実行すると、PIPからエラーを受信する場合があります。\n",
        "\n",
        "> GCSFS 2025.3.2にはFSSPEC == 2025.3.2が必要ですが、FSSPEC 2025.3.0は互換性がありません。\n",
        "\n",
        "これらのPIP互換性エラーは安全に無視できます。そして、バージョン番号を変更することでそれらを修正しようとするのは魅力的ですが、実際に実際の問題を導入します！\n",
        "\n",
        "** Pro-Tip 2：**\n",
        "\n",
        "コラブを実行している最中に、次のようなエラーが発生する可能性があります。\n",
        "\n",
        ">ランタイムエラー：CUDAは必要ですが、BitsandBytesでは利用できません。 [...]のインストールを検討してください\n",
        "\n",
        "これは超模倣エラーメッセージです！パッケージのバージョンを変更してみないでください...\n",
        "\n",
        "これは、GoogleがColabランタイムを切り替えたために実際に起こります。おそらくGoogle Colabが忙しすぎたためです。解決策は次のとおりです。\n",
        "\n",
        "1。ランタイムメニュー>>ランタイムを切断して削除します\n",
        "2。新鮮なメニューからコラブをリロードし、メニュー>>すべての出力をクリアします\n",
        "3.右上のボタンを使用して新しいT4に接続します\n",
        "4.右上のメニューから「リソースを表示」を選択して、GPUがあることを確認してください\n",
        "5.コラブ内のセルを上下から、PIPのインストールから始めて再実行します\n",
        "\n",
        "そして、すべてがうまくいくはずです - そうでなければ、私に尋ねてください！"
      ],
      "metadata": {
        "id": "d14UgrL83gcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# ピップインストール\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# 輸入\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# 定数\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Qlora微調整のためのハイパーパラメーター\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Huggingfaceにログインします\n",
        "\n",
        "まだHuggingfaceアカウントをお持ちでない場合は、https：//huggingface.coにアクセスしてサインアップしてトークンを作成します。\n",
        "\n",
        "次に、左のキーアイコンをクリックして、このノートブックの秘密を選択し、トークンとして値を持つ「hf_token」と呼ばれる新しい秘密を追加します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Huggingfaceにログインします\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "##さまざまな量子化を試します\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 量子化なしでベースモデルをロードします\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ElUPTnB28iNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2VcoS_DY9YSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "j6gJWw3r86KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "sVf8hf6S88C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## セッションを再開してください！\n",
        "\n",
        "次のモデルをロードして最後のモデルのキャッシュをクリアするには、ランタイム>>セッションを再起動し、初期セル（インストールとインポート、およびハグFaceログイン）を再度実行する必要があります。\n",
        "\n",
        "これはGPUをクリーンアウトするためです。"
      ],
      "metadata": {
        "id": "Gv-00uzNFPOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8ビットを使用してベースモデルをロードします\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "7ycR0B4CzUSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CVErveOYFOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "8FDztdnv0RCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "IvqOxYfk0RnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## セッションを再開してください！\n",
        "\n",
        "次のモデルをロードして最後のモデルのキャッシュをクリアするには、ランタイム>>セッションを再起動し、初期セル（インポートとハグFaceログイン）を再度実行する必要があります。\n",
        "\n",
        "これはGPUをクリーンアウトするためです。"
      ],
      "metadata": {
        "id": "Pb7-0OfSEycl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# 4ビットを使用してトークン剤とベースモデルをロードします\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "1FfMJ2JbzEr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "mjp1EHH10WTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ],
      "metadata": {
        "id": "wSpavkXo1KOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "o3kXoy3w1oMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "U2PL1nlM1tM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各ターゲットモジュールには、LORA_AとLORA_Bと呼ばれる2つのLORAアダプターマトリックスがあります\n",
        "# これらは、alpha * lora_a * lora_bを追加することでウェイトを適応できるように設計されています\n",
        "# 次の寸法を使用して、重みの数を数えましょう。\n",
        "\n",
        "# 上記のマトリックス寸法を参照してください\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# 各レイヤーが登場します\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# 32のレイヤーがあります\n",
        "params = lora_layer * 32\n",
        "\n",
        "# したがって、MBの合計サイズはです\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ],
      "metadata": {
        "id": "IIaqo-gyBQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [
        {
          "file_id": "15rqdMTJwK76icPBxNoqhI7Ww8UM-Y7ni",
          "timestamp": 1754464190117
        },
        {
          "file_id": "1bG7RiOeH5QPq3JQeKT9pKizLvNR-AGl9",
          "timestamp": 1726925570673
        },
        {
          "file_id": "15LkGT3KiWNVYeixJyVxmKCEp-4i0ay3t",
          "timestamp": 1726105199072
        },
        {
          "file_id": "1yuwPchdf-4XC6U19qlyka5VxvdoqbN4F",
          "timestamp": 1726003619500
        },
        {
          "file_id": "1iWviog0DBRNH0eQ8ttPcKAQAnda_XsHe",
          "timestamp": 1725972744690
        },
        {
          "file_id": "1QFH8bL2g2xspETWirmUcThxw801uiRiA",
          "timestamp": 1725923171274
        },
        {
          "file_id": "12vriYAjNGP9erT-dLpJCpv6IFG8vInpX",
          "timestamp": 1725916862962
        },
        {
          "file_id": "1RDjLFskzcODWkdJzO92-8BnXXUDg4IxJ",
          "timestamp": 1725815008044
        },
        {
          "file_id": "1TA_GwdrpWwRZfUw8I9y2fqqwFv9CBU1O",
          "timestamp": 1725200339101
        },
        {
          "file_id": "18UtZAMfXHv2NwlcseEpvqqd0SCHBUeQY",
          "timestamp": 1724340104889
        },
        {
          "file_id": "1BuEhC59gEyqFTD6F9Th2vIcTygFNuz8F",
          "timestamp": 1724060220581
        },
        {
          "file_id": "19E9hoAzWKvn9c9SHqM4Xan_Ph4wNewHS",
          "timestamp": 1724000085127
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}